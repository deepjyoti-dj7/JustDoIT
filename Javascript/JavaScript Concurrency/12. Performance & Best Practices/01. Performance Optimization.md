# Performance Optimization

## Table of Contents
- [Understanding Performance](#understanding-performance)
- [Profiling and Measurement](#profiling-and-measurement)
- [Reducing Worker Overhead](#reducing-worker-overhead)
- [Memory Optimization](#memory-optimization)
- [Message Passing Optimization](#message-passing-optimization)
- [SharedArrayBuffer Performance](#sharedarraybuffer-performance)
- [Common Performance Issues](#common-performance-issues)
- [Benchmarking](#benchmarking)

---

## Understanding Performance

### Performance Factors:

**Worker Creation Overhead:**
- Creating workers is expensive (~10-50ms)
- Pool workers for reuse
- Balance worker count with workload

**Message Passing Cost:**
- Structured clone algorithm has overhead
- Large objects are slow to transfer
- Use Transferables when possible

**Synchronization Overhead:**
- Atomic operations have cost
- Locks can cause contention
- Minimize shared state access

```javascript
// ‚ùå Bad: Create worker for each task
for (const item of data) {
  const worker = new Worker('process.js')
  worker.postMessage(item)
}

// ‚úÖ Good: Reuse worker pool
const pool = new WorkerPool('process.js', 4)
for (const item of data) {
  pool.execute(item)
}
```

---

## Profiling and Measurement

### Performance Timing:

```javascript
class PerformanceMonitor {
  constructor() {
    this.metrics = []
  }
  
  async measure(name, fn) {
    const start = performance.now()
    
    try {
      const result = await fn()
      const duration = performance.now() - start
      
      this.metrics.push({ name, duration, success: true })
      
      return result
    } catch (error) {
      const duration = performance.now() - start
      this.metrics.push({ name, duration, success: false })
      throw error
    }
  }
  
  getStats(name) {
    const samples = this.metrics.filter(m => m.name === name)
    
    if (samples.length === 0) return null
    
    const durations = samples.map(s => s.duration)
    const sorted = durations.sort((a, b) => a - b)
    
    return {
      count: samples.length,
      min: sorted[0],
      max: sorted[sorted.length - 1],
      avg: durations.reduce((a, b) => a + b, 0) / samples.length,
      median: sorted[Math.floor(sorted.length / 2)],
      p95: sorted[Math.floor(sorted.length * 0.95)],
      p99: sorted[Math.floor(sorted.length * 0.99)]
    }
  }
  
  report() {
    const names = [...new Set(this.metrics.map(m => m.name))]
    
    names.forEach(name => {
      const stats = this.getStats(name)
      console.log(`${name}:`, stats)
    })
  }
}

// Usage
const monitor = new PerformanceMonitor()

await monitor.measure('worker-task', async () => {
  return workerPool.execute(data)
})

monitor.report()
```

### Chrome DevTools Profiling:

```javascript
// Add marks and measures
performance.mark('worker-start')

const result = await workerPool.execute(data)

performance.mark('worker-end')
performance.measure('worker-execution', 'worker-start', 'worker-end')

// View in Chrome DevTools Performance tab
```

---

## Reducing Worker Overhead

### Worker Pool Best Practices:

```javascript
class OptimizedWorkerPool {
  constructor(workerScript, options = {}) {
    const {
      minWorkers = 2,
      maxWorkers = navigator.hardwareConcurrency || 4,
      idleTimeout = 30000,
      warmup = true
    } = options
    
    this.minWorkers = minWorkers
    this.maxWorkers = maxWorkers
    this.idleTimeout = idleTimeout
    this.workers = []
    this.taskQueue = []
    
    // Pre-create minimum workers
    for (let i = 0; i < minWorkers; i++) {
      this.createWorker()
    }
    
    // Optionally warmup workers
    if (warmup) {
      this.warmup()
    }
    
    // Auto-scale workers
    this.startAutoScaling()
  }
  
  createWorker() {
    const worker = new Worker(this.workerScript)
    const workerInfo = {
      worker,
      busy: false,
      lastUsed: Date.now(),
      tasksCompleted: 0
    }
    
    worker.onmessage = (e) => {
      workerInfo.busy = false
      workerInfo.lastUsed = Date.now()
      workerInfo.tasksCompleted++
      
      if (workerInfo.currentResolve) {
        workerInfo.currentResolve(e.data)
      }
      
      this.processQueue()
    }
    
    this.workers.push(workerInfo)
    return workerInfo
  }
  
  async warmup() {
    // Execute dummy task on each worker to initialize
    await Promise.all(
      this.workers.map(w => this.executeOnWorker(w, { warmup: true }))
    )
  }
  
  startAutoScaling() {
    setInterval(() => {
      const utilization = this.workers.filter(w => w.busy).length / this.workers.length
      
      // Scale up if high utilization and pending tasks
      if (utilization > 0.8 && this.taskQueue.length > 0 && this.workers.length < this.maxWorkers) {
        this.createWorker()
      }
      
      // Scale down if low utilization
      if (utilization < 0.2 && this.workers.length > this.minWorkers) {
        this.removeIdleWorker()
      }
    }, 5000)
  }
  
  removeIdleWorker() {
    const now = Date.now()
    const idleWorker = this.workers.find(w =>
      !w.busy && (now - w.lastUsed) > this.idleTimeout
    )
    
    if (idleWorker) {
      idleWorker.worker.terminate()
      this.workers = this.workers.filter(w => w !== idleWorker)
    }
  }
  
  async execute(data) {
    const availableWorker = this.workers.find(w => !w.busy)
    
    if (availableWorker) {
      return this.executeOnWorker(availableWorker, data)
    }
    
    // Queue task
    return new Promise((resolve, reject) => {
      this.taskQueue.push({ data, resolve, reject })
    })
  }
  
  executeOnWorker(workerInfo, data) {
    return new Promise((resolve) => {
      workerInfo.busy = true
      workerInfo.currentResolve = resolve
      workerInfo.worker.postMessage(data)
    })
  }
  
  processQueue() {
    if (this.taskQueue.length === 0) return
    
    const availableWorker = this.workers.find(w => !w.busy)
    if (!availableWorker) return
    
    const task = this.taskQueue.shift()
    
    this.executeOnWorker(availableWorker, task.data)
      .then(task.resolve)
      .catch(task.reject)
  }
}
```

---

## Memory Optimization

### Avoiding Memory Leaks:

```javascript
// ‚ùå Bad: Memory leak - worker never terminated
function processData(data) {
  const worker = new Worker('process.js')
  worker.postMessage(data)
  // Worker never terminated!
}

// ‚úÖ Good: Terminate worker
function processData(data) {
  const worker = new Worker('process.js')
  
  worker.onmessage = (e) => {
    console.log(e.data)
    worker.terminate()  // Clean up
  }
  
  worker.postMessage(data)
}

// ‚úÖ Better: Use worker pool
const pool = new WorkerPool('process.js', 4)
pool.execute(data)  // Pool manages lifecycle
```

### Efficient Data Handling:

```javascript
// ‚ùå Bad: Copying large arrays
const largeArray = new Float64Array(1000000)
worker.postMessage({ array: largeArray })  // Copies 8MB!

// ‚úÖ Good: Transfer ownership
const largeArray = new Float64Array(1000000)
worker.postMessage({ array: largeArray }, [largeArray.buffer])  // Zero-copy transfer

// ‚úÖ Good: Use SharedArrayBuffer
const sharedBuffer = new SharedArrayBuffer(8000000)
const sharedArray = new Float64Array(sharedBuffer)
worker.postMessage({ buffer: sharedBuffer })  // Shared, not copied
```

---

## Message Passing Optimization

### Batch Messages:

```javascript
// ‚ùå Bad: Send many small messages
for (let i = 0; i < 1000; i++) {
  worker.postMessage({ item: i })
}

// ‚úÖ Good: Batch items
const batch = []
for (let i = 0; i < 1000; i++) {
  batch.push({ item: i })
}
worker.postMessage({ batch })
```

### Minimize Message Size:

```javascript
// ‚ùå Bad: Send entire object
worker.postMessage({
  user: fullUserObject,  // 100KB of data
  action: 'process'
})

// ‚úÖ Good: Send only needed data
worker.postMessage({
  userId: user.id,
  userName: user.name,
  action: 'process'
})
```

### Use Transferable Objects:

```javascript
// Transferable types:
// - ArrayBuffer
// - MessagePort
// - ImageBitmap
// - OffscreenCanvas

// ‚úÖ Transfer ArrayBuffer
const buffer = new ArrayBuffer(1024 * 1024)  // 1MB
worker.postMessage({ data: buffer }, [buffer])

// ‚úÖ Transfer ImageBitmap
const bitmap = await createImageBitmap(imageBlob)
worker.postMessage({ image: bitmap }, [bitmap])
```

---

## SharedArrayBuffer Performance

### Optimal Access Patterns:

```javascript
// ‚ùå Bad: Random access causes cache misses
for (let i = 0; i < array.length; i++) {
  const randomIndex = Math.floor(Math.random() * array.length)
  process(array[randomIndex])
}

// ‚úÖ Good: Sequential access optimizes cache
for (let i = 0; i < array.length; i++) {
  process(array[i])
}
```

### Minimize Contention:

```javascript
// ‚ùå Bad: All workers access same location
for (let i = 0; i < workers.length; i++) {
  workers[i].postMessage({
    buffer: sharedBuffer,
    index: 0  // All write to index 0 - contention!
  })
}

// ‚úÖ Good: Each worker has own section
const chunkSize = sharedArray.length / workers.length
for (let i = 0; i < workers.length; i++) {
  workers[i].postMessage({
    buffer: sharedBuffer,
    start: i * chunkSize,
    end: (i + 1) * chunkSize
  })
}
```

---

## Common Performance Issues

### Issue 1: Too Many Workers

```javascript
// ‚ùå Problem
const workerCount = 100  // CPU only has 8 cores!

// ‚úÖ Solution
const workerCount = navigator.hardwareConcurrency || 4
```

### Issue 2: Small Tasks

```javascript
// ‚ùå Problem: Overhead > computation time
for (const item of data) {
  await worker.execute(item)  // 50ms overhead, 1ms computation
}

// ‚úÖ Solution: Batch small tasks
const batchSize = 100
for (let i = 0; i < data.length; i += batchSize) {
  const batch = data.slice(i, i + batchSize)
  await worker.execute(batch)  // 50ms overhead, 100ms computation
}
```

### Issue 3: Blocking Main Thread

```javascript
// ‚ùå Problem
worker.onmessage = (e) => {
  const result = expensiveProcessing(e.data)  // Blocks!
  updateUI(result)
}

// ‚úÖ Solution
worker.onmessage = (e) => {
  requestIdleCallback(() => {
    const result = expensiveProcessing(e.data)
    updateUI(result)
  })
}
```

---

## Benchmarking

### Comparison Framework:

```javascript
class Benchmark {
  async compare(implementations, data, iterations = 100) {
    const results = {}
    
    for (const [name, impl] of Object.entries(implementations)) {
      const durations = []
      
      for (let i = 0; i < iterations; i++) {
        const start = performance.now()
        await impl(data)
        durations.push(performance.now() - start)
      }
      
      results[name] = {
        avg: durations.reduce((a, b) => a + b) / durations.length,
        min: Math.min(...durations),
        max: Math.max(...durations)
      }
    }
    
    return results
  }
  
  printResults(results) {
    console.table(results)
    
    const fastest = Object.entries(results)
      .sort((a, b) => a[1].avg - b[1].avg)[0]
    
    console.log(`\nüèÜ Fastest: ${fastest[0]} (${fastest[1].avg.toFixed(2)}ms avg)`)
  }
}

// Usage
const benchmark = new Benchmark()

const data = new Array(10000).fill(0).map((_, i) => i)

const results = await benchmark.compare({
  'Sequential': async (data) => {
    return data.map(x => x * 2)
  },
  
  'Parallel (4 workers)': async (data) => {
    return parallelMap(data, x => x * 2, 4)
  },
  
  'SharedArrayBuffer': async (data) => {
    return sharedArrayProcess(data)
  }
}, data)

benchmark.printResults(results)
```

---

## Summary

### Performance Checklist:

‚úÖ **Worker Management:**
- Use worker pools
- Reuse workers
- Auto-scale based on load
- Terminate idle workers

‚úÖ **Message Passing:**
- Batch small messages
- Use Transferables
- Minimize message size
- Avoid unnecessary copying

‚úÖ **Memory:**
- Terminate workers
- Use SharedArrayBuffer for large data
- Clear references
- Monitor memory usage

‚úÖ **Workload:**
- Batch small tasks
- Match worker count to cores
- Profile before optimizing
- Benchmark alternatives

### Key Metrics:
- Worker creation time
- Message passing overhead
- Task execution time
- Memory consumption
- CPU utilization

### Tools:
- Chrome DevTools Performance
- Performance API
- Memory profiler
- Custom benchmarks

### Related Topics:
- Worker pools
- SharedArrayBuffer
- Transferable objects
- Memory management
