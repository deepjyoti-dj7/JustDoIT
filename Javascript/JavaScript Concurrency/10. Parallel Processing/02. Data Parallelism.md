# Data Parallelism

## Table of Contents
- [What is Data Parallelism?](#what-is-data-parallelism)
- [Array Processing](#array-processing)
- [Parallel Map/Reduce](#parallel-mapreduce)
- [Data Partitioning](#data-partitioning)
- [SharedArrayBuffer Techniques](#sharedarraybuffer-techniques)
- [Performance Considerations](#performance-considerations)
- [Real-World Examples](#real-world-examples)

---

## What is Data Parallelism?

**Data parallelism** applies the same operation to multiple data elements simultaneously across different workers.

**Characteristics:**
- Same operation on different data
- Data is partitioned across workers
- Results are combined
- Focus on processing large datasets

```javascript
// Sequential
const results = data.map(item => process(item))

// Parallel
const results = await parallelMap(data, process)
```

---

## Array Processing

### Parallel Map:

```javascript
async function parallelMap(array, fn, workerCount = 4) {
  const chunkSize = Math.ceil(array.length / workerCount)
  const chunks = []
  
  for (let i = 0; i < workerCount; i++) {
    const start = i * chunkSize
    const end = Math.min(start + chunkSize, array.length)
    chunks.push(array.slice(start, end))
  }
  
  const workers = chunks.map((chunk, i) => {
    return new Promise((resolve) => {
      const worker = new Worker('map-worker.js')
      worker.onmessage = (e) => {
        resolve(e.data)
        worker.terminate()
      }
      worker.postMessage({ chunk, fn: fn.toString() })
    })
  })
  
  const results = await Promise.all(workers)
  return results.flat()
}

// Usage
const data = Array.from({ length: 10000 }, (_, i) => i)
const squared = await parallelMap(data, x => x * x)
```

### Parallel Filter:

```javascript
async function parallelFilter(array, predicate, workerCount = 4) {
  const chunkSize = Math.ceil(array.length / workerCount)
  const workers = []
  
  for (let i = 0; i < workerCount; i++) {
    const start = i * chunkSize
    const end = Math.min(start + chunkSize, array.length)
    const chunk = array.slice(start, end)
    
    workers.push(new Promise((resolve) => {
      const worker = new Worker('filter-worker.js')
      worker.onmessage = (e) => {
        resolve(e.data)
        worker.terminate()
      }
      worker.postMessage({ chunk, predicate: predicate.toString() })
    }))
  }
  
  const results = await Promise.all(workers)
  return results.flat()
}

// filter-worker.js
self.onmessage = (e) => {
  const { chunk, predicate } = e.data
  const predicateFn = eval(`(${predicate})`)
  const filtered = chunk.filter(predicateFn)
  self.postMessage(filtered)
}
```

---

## Parallel Map/Reduce

### Map-Reduce Implementation:

```javascript
class MapReduce {
  constructor(workerCount = 4) {
    this.workerCount = workerCount
  }
  
  async execute(data, mapFn, reduceFn, initialValue) {
    // Map phase
    const mapResults = await this.parallelMap(data, mapFn)
    
    // Reduce phase
    return this.parallelReduce(mapResults, reduceFn, initialValue)
  }
  
  async parallelMap(data, mapFn) {
    const chunkSize = Math.ceil(data.length / this.workerCount)
    const workers = []
    
    for (let i = 0; i < this.workerCount; i++) {
      const start = i * chunkSize
      const end = Math.min(start + chunkSize, data.length)
      const chunk = data.slice(start, end)
      
      workers.push(this.runMapWorker(chunk, mapFn))
    }
    
    const results = await Promise.all(workers)
    return results.flat()
  }
  
  async parallelReduce(data, reduceFn, initialValue) {
    if (data.length === 0) return initialValue
    if (data.length === 1) return data[0]
    
    const chunkSize = Math.ceil(data.length / this.workerCount)
    const workers = []
    
    for (let i = 0; i < this.workerCount; i++) {
      const start = i * chunkSize
      const end = Math.min(start + chunkSize, data.length)
      const chunk = data.slice(start, end)
      
      if (chunk.length > 0) {
        workers.push(this.runReduceWorker(chunk, reduceFn, initialValue))
      }
    }
    
    const partialResults = await Promise.all(workers)
    
    // Final reduce
    return partialResults.reduce(reduceFn, initialValue)
  }
  
  runMapWorker(chunk, mapFn) {
    return new Promise((resolve) => {
      const worker = new Worker('map-worker.js')
      worker.onmessage = (e) => {
        resolve(e.data)
        worker.terminate()
      }
      worker.postMessage({ chunk, fn: mapFn.toString() })
    })
  }
  
  runReduceWorker(chunk, reduceFn, initialValue) {
    return new Promise((resolve) => {
      const worker = new Worker('reduce-worker.js')
      worker.onmessage = (e) => {
        resolve(e.data)
        worker.terminate()
      }
      worker.postMessage({ chunk, fn: reduceFn.toString(), initialValue })
    })
  }
}

// Usage: Word count
const mapReduce = new MapReduce(4)

const documents = ['hello world', 'world of data', 'hello data']

const wordCount = await mapReduce.execute(
  documents,
  (doc) => doc.split(' ').map(word => ({ word, count: 1 })),
  (acc, item) => {
    acc[item.word] = (acc[item.word] || 0) + item.count
    return acc
  },
  {}
)
```

---

## Data Partitioning

### Range Partitioning:

```javascript
function partitionByRange(data, numPartitions) {
  const partitionSize = Math.ceil(data.length / numPartitions)
  const partitions = []
  
  for (let i = 0; i < numPartitions; i++) {
    const start = i * partitionSize
    const end = Math.min(start + partitionSize, data.length)
    partitions.push(data.slice(start, end))
  }
  
  return partitions
}
```

### Hash Partitioning:

```javascript
function partitionByHash(data, numPartitions, keyFn) {
  const partitions = Array.from({ length: numPartitions }, () => [])
  
  data.forEach(item => {
    const key = keyFn(item)
    const hash = simpleHash(key)
    const partition = hash % numPartitions
    partitions[partition].push(item)
  })
  
  return partitions
}

function simpleHash(str) {
  let hash = 0
  for (let i = 0; i < str.length; i++) {
    hash = ((hash << 5) - hash) + str.charCodeAt(i)
    hash |= 0
  }
  return Math.abs(hash)
}

// Usage
const users = [
  { id: 1, name: 'Alice' },
  { id: 2, name: 'Bob' },
  // ...
]

const partitions = partitionByHash(users, 4, user => user.id.toString())
```

---

## SharedArrayBuffer Techniques

### Parallel Sum:

```javascript
// Main thread
const data = new Float64Array(10000)
for (let i = 0; i < data.length; i++) {
  data[i] = Math.random()
}

const sharedBuffer = new SharedArrayBuffer(data.byteLength)
const sharedArray = new Float64Array(sharedBuffer)
sharedArray.set(data)

const resultBuffer = new SharedArrayBuffer(32)
const results = new Float64Array(resultBuffer)

const workerCount = 4
const chunkSize = data.length / workerCount

const workers = []
for (let i = 0; i < workerCount; i++) {
  const worker = new Worker('sum-worker.js')
  worker.postMessage({
    dataBuffer: sharedBuffer,
    resultBuffer,
    start: i * chunkSize,
    end: (i + 1) * chunkSize,
    resultIndex: i
  })
  workers.push(worker)
}

await Promise.all(workers.map(w => 
  new Promise(resolve => w.onmessage = () => resolve())
))

const totalSum = results.reduce((a, b) => a + b, 0)

// sum-worker.js
self.onmessage = (e) => {
  const { dataBuffer, resultBuffer, start, end, resultIndex } = e.data
  
  const data = new Float64Array(dataBuffer)
  const results = new Float64Array(resultBuffer)
  
  let sum = 0
  for (let i = start; i < end; i++) {
    sum += data[i]
  }
  
  results[resultIndex] = sum
  self.postMessage('done')
}
```

### Parallel Sort:

```javascript
async function parallelSort(array) {
  const workerCount = 4
  const chunkSize = Math.ceil(array.length / workerCount)
  
  // Phase 1: Sort chunks in parallel
  const sortedChunks = await Promise.all(
    Array.from({ length: workerCount }, (_, i) => {
      const start = i * chunkSize
      const end = Math.min(start + chunkSize, array.length)
      const chunk = array.slice(start, end)
      
      return new Promise((resolve) => {
        const worker = new Worker('sort-worker.js')
        worker.onmessage = (e) => {
          resolve(e.data)
          worker.terminate()
        }
        worker.postMessage(chunk)
      })
    })
  )
  
  // Phase 2: Merge sorted chunks
  return mergeSortedArrays(sortedChunks)
}

function mergeSortedArrays(arrays) {
  while (arrays.length > 1) {
    const merged = []
    
    for (let i = 0; i < arrays.length; i += 2) {
      if (i + 1 < arrays.length) {
        merged.push(merge(arrays[i], arrays[i + 1]))
      } else {
        merged.push(arrays[i])
      }
    }
    
    arrays = merged
  }
  
  return arrays[0]
}

function merge(arr1, arr2) {
  const result = []
  let i = 0, j = 0
  
  while (i < arr1.length && j < arr2.length) {
    if (arr1[i] <= arr2[j]) {
      result.push(arr1[i++])
    } else {
      result.push(arr2[j++])
    }
  }
  
  return result.concat(arr1.slice(i)).concat(arr2.slice(j))
}
```

---

## Performance Considerations

### Optimal Worker Count:

```javascript
function getOptimalWorkerCount(dataSize, overhead = 1000) {
  const maxWorkers = navigator.hardwareConcurrency || 4
  const minWorkPerWorker = overhead
  const optimalWorkers = Math.min(
    maxWorkers,
    Math.floor(dataSize / minWorkPerWorker)
  )
  return Math.max(1, optimalWorkers)
}

// Usage
const data = new Array(100000)
const workerCount = getOptimalWorkerCount(data.length)
```

### Minimizing Overhead:

```javascript
// ❌ Too much overhead
for (const item of data) {
  await processInWorker(item)  // Creates worker for each item!
}

// ✅ Batch processing
const batchSize = 1000
for (let i = 0; i < data.length; i += batchSize) {
  const batch = data.slice(i, i + batchSize)
  await processInWorker(batch)
}
```

---

## Real-World Examples

### 1. Image Processing:

```javascript
async function processImages(images) {
  const workerCount = 4
  const chunkSize = Math.ceil(images.length / workerCount)
  
  const workers = []
  for (let i = 0; i < workerCount; i++) {
    const chunk = images.slice(i * chunkSize, (i + 1) * chunkSize)
    
    workers.push(new Promise((resolve) => {
      const worker = new Worker('image-processor.js')
      worker.onmessage = (e) => {
        resolve(e.data)
        worker.terminate()
      }
      worker.postMessage({ images: chunk })
    }))
  }
  
  const results = await Promise.all(workers)
  return results.flat()
}
```

### 2. Matrix Multiplication:

```javascript
async function parallelMatrixMultiply(A, B) {
  const rows = A.length
  const cols = B[0].length
  const workerCount = 4
  const rowsPerWorker = Math.ceil(rows / workerCount)
  
  const workers = []
  for (let i = 0; i < workerCount; i++) {
    const startRow = i * rowsPerWorker
    const endRow = Math.min(startRow + rowsPerWorker, rows)
    
    workers.push(new Promise((resolve) => {
      const worker = new Worker('matrix-worker.js')
      worker.onmessage = (e) => {
        resolve(e.data)
        worker.terminate()
      }
      worker.postMessage({ A, B, startRow, endRow })
    }))
  }
  
  const partialResults = await Promise.all(workers)
  return partialResults.flat()
}
```

### 3. Data Aggregation:

```javascript
async function aggregateData(records) {
  const mapReduce = new MapReduce(4)
  
  return await mapReduce.execute(
    records,
    // Map: Extract relevant fields
    (record) => ({ category: record.category, value: record.value }),
    // Reduce: Sum by category
    (acc, item) => {
      acc[item.category] = (acc[item.category] || 0) + item.value
      return acc
    },
    {}
  )
}
```

---

## Summary

### Key Concepts:
- Same operation on different data elements
- Data partitioning strategies
- Parallel map/reduce patterns
- SharedArrayBuffer for zero-copy

### When to Use:
- ✅ Large datasets
- ✅ Independent computations
- ✅ Array operations
- ✅ Batch processing
- ❌ Small datasets
- ❌ Sequential dependencies

### Best Practices:
- Choose optimal worker count
- Minimize worker overhead
- Use appropriate partitioning
- Batch operations when possible
- Combine results efficiently

### Related Topics:
- Task parallelism
- Map-Reduce
- SharedArrayBuffer
- Performance optimization
