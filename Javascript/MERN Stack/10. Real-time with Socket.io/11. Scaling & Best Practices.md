# 11. Scaling & Best Practices

## Introduction

Scaling Socket.io applications requires handling multiple server instances, load balancing, and efficient resource management. This guide covers horizontal scaling, Redis adapter, clustering, performance optimization, and production best practices.

---

## Scaling Challenges

### Single Server Limitations

```
Single Server
  ├─ Limited Connections (~10,000 concurrent)
  ├─ Single Point of Failure
  ├─ CPU/Memory Constraints
  └─ No High Availability
```

### Multi-Server Requirements

```
Multiple Servers
  ├─ Load Balancing (distribute connections)
  ├─ Message Synchronization (share events)
  ├─ Session Persistence (sticky sessions)
  └─ State Management (shared storage)
```

---

## Horizontal Scaling with Redis Adapter

### Why Redis Adapter?

When you have multiple Socket.io servers, events emitted on one server won't reach clients connected to other servers. Redis adapter synchronizes events across all server instances.

```
Without Redis:
  Server 1 ──┬─► Client A
             └─► Client B
  Server 2 ──┬─► Client C  (won't receive events from Server 1)
             └─► Client D

With Redis:
  Server 1 ──┬─► Client A
             └─► Client B
      ↓ (via Redis)
  Server 2 ──┬─► Client C  (receives events from Server 1)
             └─► Client D
```

### Installation

```bash
npm install @socket.io/redis-adapter redis
```

### Basic Setup

```javascript
// server.js
const express = require('express');
const http = require('http');
const { Server } = require('socket.io');
const { createAdapter } = require('@socket.io/redis-adapter');
const { createClient } = require('redis');

const app = express();
const server = http.createServer(app);

const io = new Server(server, {
  cors: { origin: '*' }
});

// Create Redis clients
const pubClient = createClient({ url: 'redis://localhost:6379' });
const subClient = pubClient.duplicate();

Promise.all([pubClient.connect(), subClient.connect()]).then(() => {
  // Set up Redis adapter
  io.adapter(createAdapter(pubClient, subClient));
  
  console.log('Redis adapter configured');
});

io.on('connection', (socket) => {
  console.log('Client connected:', socket.id);
  
  socket.on('message', (data) => {
    // This will be broadcast to all servers
    io.emit('message', data);
  });
});

const PORT = process.env.PORT || 5000;
server.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

### Multiple Server Instances

```javascript
// server1.js (PORT=5001)
const PORT = 5001;
server.listen(PORT, () => {
  console.log(`Server 1 on port ${PORT}`);
});

// server2.js (PORT=5002)
const PORT = 5002;
server.listen(PORT, () => {
  console.log(`Server 2 on port ${PORT}`);
});

// server3.js (PORT=5003)
const PORT = 5003;
server.listen(PORT, () => {
  console.log(`Server 3 on port ${PORT}`);
});
```

---

## Load Balancing

### Nginx Configuration (Sticky Sessions)

```nginx
# nginx.conf
upstream socket_servers {
    ip_hash;  # Sticky sessions based on IP
    server localhost:5001;
    server localhost:5002;
    server localhost:5003;
}

server {
    listen 80;
    
    location / {
        proxy_pass http://socket_servers;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeouts
        proxy_connect_timeout 7d;
        proxy_send_timeout 7d;
        proxy_read_timeout 7d;
    }
}
```

### HAProxy Configuration

```haproxy
# haproxy.cfg
global
    maxconn 4096

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend http-in
    bind *:80
    default_backend socket_servers

backend socket_servers
    balance source  # Sticky sessions
    option http-server-close
    option forwardfor
    
    server server1 localhost:5001 check
    server server2 localhost:5002 check
    server server3 localhost:5003 check
```

---

## Node.js Clustering

### Cluster Module

```javascript
// cluster.js
const cluster = require('cluster');
const os = require('os');
const { setupMaster, setupWorker } = require('@socket.io/sticky');
const { createAdapter } = require('@socket.io/cluster-adapter');

const numCPUs = os.cpus().length;

if (cluster.isMaster) {
  console.log(`Master ${process.pid} is running`);
  
  // Create workers
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }
  
  cluster.on('exit', (worker, code, signal) => {
    console.log(`Worker ${worker.process.pid} died`);
    cluster.fork(); // Restart worker
  });
  
  // Setup sticky sessions
  setupMaster(3000, {
    loadBalancingMethod: 'least-connection'
  });
} else {
  const express = require('express');
  const http = require('http');
  const { Server } = require('socket.io');
  
  const app = express();
  const server = http.createServer(app);
  const io = new Server(server);
  
  // Use cluster adapter
  io.adapter(createAdapter());
  
  // Setup worker
  setupWorker(io);
  
  io.on('connection', (socket) => {
    console.log(`Client connected to worker ${process.pid}`);
    
    socket.on('message', (data) => {
      io.emit('message', data);
    });
  });
  
  console.log(`Worker ${process.pid} started`);
}
```

---

## Performance Optimization

### 1. Connection Optimization

```javascript
const io = new Server(server, {
  // Reduce ping interval
  pingInterval: 10000,
  pingTimeout: 5000,
  
  // Limit message size
  maxHttpBufferSize: 1e6, // 1MB
  
  // Use only WebSocket (no polling fallback)
  transports: ['websocket'],
  
  // Compression
  perMessageDeflate: {
    threshold: 1024 // Compress messages > 1KB
  }
});
```

### 2. Event Debouncing/Throttling

```javascript
// Client-side debouncing
let debounceTimer;
function sendTypingIndicator() {
  clearTimeout(debounceTimer);
  
  socket.emit('typing');
  
  debounceTimer = setTimeout(() => {
    socket.emit('stopTyping');
  }, 2000);
}

// Server-side throttling
const throttle = require('lodash/throttle');

socket.on('cursorMove', throttle((position) => {
  socket.broadcast.emit('cursorMoved', position);
}, 100)); // Max once per 100ms
```

### 3. Room Management

```javascript
// Clean up empty rooms
function cleanupEmptyRooms() {
  const rooms = io.sockets.adapter.rooms;
  
  rooms.forEach((sockets, roomId) => {
    // Skip socket IDs (they're also rooms)
    if (io.sockets.sockets.has(roomId)) return;
    
    if (sockets.size === 0) {
      rooms.delete(roomId);
    }
  });
}

setInterval(cleanupEmptyRooms, 60000); // Every minute
```

### 4. Message Batching

```javascript
// Server-side batching
const messageBatch = [];
let batchTimer;

socket.on('message', (message) => {
  messageBatch.push(message);
  
  if (!batchTimer) {
    batchTimer = setTimeout(() => {
      // Send batch
      io.emit('messageBatch', messageBatch);
      messageBatch.length = 0;
      batchTimer = null;
    }, 100);
  }
});
```

---

## Monitoring & Logging

### Connection Metrics

```javascript
const express = require('express');
const app = express();

// Metrics endpoint
app.get('/metrics', (req, res) => {
  const metrics = {
    connections: io.engine.clientsCount,
    rooms: io.sockets.adapter.rooms.size,
    timestamp: new Date()
  };
  
  res.json(metrics);
});

// Log connections
io.on('connection', (socket) => {
  console.log(`[${new Date().toISOString()}] Connection: ${socket.id}`);
  console.log(`Total connections: ${io.engine.clientsCount}`);
  
  socket.on('disconnect', () => {
    console.log(`[${new Date().toISOString()}] Disconnect: ${socket.id}`);
    console.log(`Total connections: ${io.engine.clientsCount}`);
  });
});
```

### Error Monitoring

```javascript
// Server errors
io.engine.on('connection_error', (err) => {
  console.error('Connection error:', {
    message: err.message,
    code: err.code,
    context: err.context,
    timestamp: new Date()
  });
});

// Socket errors
socket.on('error', (error) => {
  console.error('Socket error:', {
    socketId: socket.id,
    error: error.message,
    timestamp: new Date()
  });
});
```

---

## Security Best Practices

### 1. Authentication & Authorization

```javascript
const jwt = require('jsonwebtoken');

io.use((socket, next) => {
  const token = socket.handshake.auth.token;
  
  if (!token) {
    return next(new Error('Authentication required'));
  }
  
  try {
    const decoded = jwt.verify(token, process.env.JWT_SECRET);
    socket.userId = decoded.userId;
    next();
  } catch (err) {
    next(new Error('Invalid token'));
  }
});
```

### 2. Rate Limiting

```javascript
const rateLimit = new Map();

socket.on('message', (data) => {
  const userId = socket.userId;
  const now = Date.now();
  
  if (!rateLimit.has(userId)) {
    rateLimit.set(userId, { count: 0, resetTime: now + 60000 });
  }
  
  const userLimit = rateLimit.get(userId);
  
  if (now > userLimit.resetTime) {
    userLimit.count = 0;
    userLimit.resetTime = now + 60000;
  }
  
  if (userLimit.count >= 100) {
    socket.emit('error', 'Rate limit exceeded');
    return;
  }
  
  userLimit.count++;
  
  // Process message
  io.emit('message', data);
});
```

### 3. Input Validation & Sanitization

```javascript
const validator = require('validator');

socket.on('message', (data) => {
  // Validate input
  if (!data || typeof data !== 'string') {
    return socket.emit('error', 'Invalid message');
  }
  
  // Sanitize
  const sanitized = validator.escape(data);
  
  // Length check
  if (sanitized.length > 500) {
    return socket.emit('error', 'Message too long');
  }
  
  io.emit('message', {
    from: socket.userId,
    text: sanitized,
    timestamp: new Date()
  });
});
```

### 4. CORS Configuration

```javascript
const io = new Server(server, {
  cors: {
    origin: process.env.CLIENT_URL || 'http://localhost:3000',
    methods: ['GET', 'POST'],
    credentials: true,
    allowedHeaders: ['Content-Type', 'Authorization']
  }
});
```

---

## Production Checklist

### Server Configuration

```javascript
const io = new Server(server, {
  // CORS
  cors: {
    origin: process.env.CLIENT_URL,
    credentials: true
  },
  
  // Timeouts
  pingTimeout: 60000,
  pingInterval: 25000,
  
  // Performance
  transports: ['websocket'],
  maxHttpBufferSize: 1e6,
  perMessageDeflate: true,
  
  // Adapter for scaling
  adapter: redisAdapter
});
```

### Environment Variables

```bash
# .env
NODE_ENV=production
PORT=5000
CLIENT_URL=https://yourdomain.com
REDIS_URL=redis://localhost:6379
JWT_SECRET=your-secret-key
```

### PM2 Configuration

```javascript
// ecosystem.config.js
module.exports = {
  apps: [{
    name: 'socket-server',
    script: './server.js',
    instances: 4, // Number of instances
    exec_mode: 'cluster',
    env: {
      NODE_ENV: 'production',
      PORT: 5000
    },
    max_memory_restart: '1G',
    error_file: './logs/err.log',
    out_file: './logs/out.log',
    log_date_format: 'YYYY-MM-DD HH:mm:ss'
  }]
};
```

```bash
# Start with PM2
pm2 start ecosystem.config.js
pm2 logs
pm2 monit
```

---

## Interview Questions

**Q1: How to scale Socket.io horizontally?**
- Use **Redis adapter** to sync events across servers
- Configure **sticky sessions** in load balancer
- Use **cluster module** for multi-core scaling

**Q2: Why use sticky sessions?**
- WebSocket requires consistent server connection
- Without sticky sessions, handshake and upgrade may hit different servers
- `ip_hash` in Nginx or `balance source` in HAProxy

**Q3: What is Redis adapter for?**
- Synchronizes events across multiple Socket.io servers
- Broadcasts from one server reach clients on other servers
- Required for horizontal scaling

**Q4: How to optimize Socket.io performance?**
- Use **WebSocket only** (no polling)
- **Compress** large messages
- **Debounce/throttle** frequent events
- **Batch** messages when possible
- Clean up empty rooms

**Q5: Security best practices for Socket.io?**
- **Authenticate** with JWT
- **Rate limit** events per user
- **Validate and sanitize** all inputs
- **Configure CORS** properly
- Use **HTTPS/WSS** in production

---

## Best Practices Summary

### Architecture
1. Use **Redis adapter** for multi-server deployments
2. Configure **sticky sessions** in load balancer
3. Use **clustering** for multi-core utilization
4. Implement **health checks** and monitoring

### Performance
5. Use **WebSocket only** transport
6. **Compress** messages over 1KB
7. **Debounce/throttle** high-frequency events
8. **Batch** messages when appropriate
9. Clean up **empty rooms** periodically
10. Limit **message size** to prevent abuse

### Security
11. **Authenticate** all connections
12. **Rate limit** per user/IP
13. **Validate and sanitize** inputs
14. Configure **CORS** restrictively
15. Use **HTTPS/WSS** in production
16. **Log** security events

### Monitoring
17. Track **connection metrics**
18. Monitor **memory usage**
19. Log **errors** and exceptions
20. Set up **alerts** for anomalies

### Code Quality
21. Clean up **event listeners** properly
22. Handle **disconnections** gracefully
23. Implement **reconnection** logic
24. Use **namespaces** to organize features
25. Document **events** and data structures

---

## Summary

- **Horizontal Scaling**: Redis adapter + sticky sessions
- **Redis Adapter**: Sync events across servers
- **Sticky Sessions**: Nginx `ip_hash`, HAProxy `balance source`
- **Clustering**: Node.js cluster module for multi-core
- **Load Balancing**: Nginx, HAProxy with WebSocket support
- **Performance**: WebSocket-only, compression, debouncing
- **Security**: JWT auth, rate limiting, input validation
- **Monitoring**: Track connections, errors, memory
- **Production**: PM2, environment variables, health checks
- **Best Practice**: Scale horizontally, optimize, secure, monitor
