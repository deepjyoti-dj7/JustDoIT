# 11. Streams & Buffers

## What are Streams?

**Streams** process data piece by piece (chunks) instead of loading everything into memory at once.

**Benefits:**
- Handle large files efficiently
- Start processing before all data arrives
- Lower memory usage

```
Traditional: [Load entire file] → [Process] → [Output]
Stream:      [Chunk 1] → [Process] → [Output]
             [Chunk 2] → [Process] → [Output]
             [Chunk 3] → [Process] → [Output]
```

---

## Types of Streams

| Type | Description | Example |
|------|-------------|---------|
| **Readable** | Read data from source | Reading a file |
| **Writable** | Write data to destination | Writing to file |
| **Duplex** | Both read and write | Network socket |
| **Transform** | Modify data while reading/writing | Compression |

---

## Readable Streams

### Reading a File

```javascript
const fs = require('fs');

// Create readable stream
const readStream = fs.createReadStream('large-file.txt', {
  encoding: 'utf8',
  highWaterMark: 64 * 1024  // 64KB chunks (default 16KB)
});

// Listen for data
readStream.on('data', (chunk) => {
  console.log('Received chunk:', chunk.length, 'bytes');
});

// Listen for end
readStream.on('end', () => {
  console.log('Finished reading file');
});

// Listen for errors
readStream.on('error', (err) => {
  console.error('Error:', err);
});
```

---

## Writable Streams

### Writing to a File

```javascript
const fs = require('fs');

// Create writable stream
const writeStream = fs.createWriteStream('output.txt');

// Write data
writeStream.write('Line 1\n');
writeStream.write('Line 2\n');
writeStream.write('Line 3\n');

// Close stream
writeStream.end('Final line\n');

// Listen for finish
writeStream.on('finish', () => {
  console.log('Writing completed');
});

writeStream.on('error', (err) => {
  console.error('Error:', err);
});
```

---

## Piping Streams

**Pipe** connects readable stream to writable stream:

```javascript
const fs = require('fs');

const readStream = fs.createReadStream('input.txt');
const writeStream = fs.createWriteStream('output.txt');

// Pipe read to write
readStream.pipe(writeStream);

writeStream.on('finish', () => {
  console.log('File copied!');
});
```

### Chaining Pipes

```javascript
const fs = require('fs');
const zlib = require('zlib');

// Read → Compress → Write
fs.createReadStream('file.txt')
  .pipe(zlib.createGzip())
  .pipe(fs.createWriteStream('file.txt.gz'));

console.log('Compressing file...');
```

---

## Transform Streams

Modify data while streaming:

```javascript
const { Transform } = require('stream');

// Create uppercase transformer
const upperCaseTransform = new Transform({
  transform(chunk, encoding, callback) {
    this.push(chunk.toString().toUpperCase());
    callback();
  }
});

// Use transformer
process.stdin
  .pipe(upperCaseTransform)
  .pipe(process.stdout);

// Type something → it outputs in UPPERCASE
```

---

## Practical Examples

### Example 1: Copy Large File

```javascript
const fs = require('fs');

function copyFile(source, destination) {
  const readStream = fs.createReadStream(source);
  const writeStream = fs.createWriteStream(destination);
  
  readStream.pipe(writeStream);
  
  writeStream.on('finish', () => {
    console.log('Copy completed!');
  });
  
  readStream.on('error', (err) => {
    console.error('Read error:', err);
  });
  
  writeStream.on('error', (err) => {
    console.error('Write error:', err);
  });
}

copyFile('large-video.mp4', 'backup-video.mp4');
```

### Example 2: HTTP File Download

```javascript
const http = require('http');
const fs = require('fs');

http.createServer((req, res) => {
  // Stream file to client
  const readStream = fs.createReadStream('video.mp4');
  
  res.writeHead(200, { 'Content-Type': 'video/mp4' });
  
  readStream.pipe(res);
  
}).listen(3000);

console.log('Server running on port 3000');
```

### Example 3: Line-by-Line Reading

```javascript
const fs = require('fs');
const readline = require('readline');

const readStream = fs.createReadStream('data.txt');

const rl = readline.createInterface({
  input: readStream,
  crlfDelay: Infinity
});

rl.on('line', (line) => {
  console.log('Line:', line);
});

rl.on('close', () => {
  console.log('Finished reading file');
});
```

---

## What are Buffers?

**Buffer** is a temporary storage for binary data (like an array of bytes).

```javascript
// Create buffer from string
const buf1 = Buffer.from('Hello');
console.log(buf1);  // <Buffer 48 65 6c 6c 6f>

// Create empty buffer (10 bytes)
const buf2 = Buffer.alloc(10);

// Create buffer with values
const buf3 = Buffer.from([72, 101, 108, 108, 111]);
console.log(buf3.toString());  // 'Hello'
```

---

## Working with Buffers

### Creating Buffers

```javascript
// From string
const buf1 = Buffer.from('Hello World');

// From array
const buf2 = Buffer.from([72, 101, 108, 108, 111]);

// Allocate empty buffer
const buf3 = Buffer.alloc(10);  // 10 bytes, filled with 0

// Allocate unsafe (faster, but may contain old data)
const buf4 = Buffer.allocUnsafe(10);
```

### Reading Buffers

```javascript
const buf = Buffer.from('Hello');

// To string
console.log(buf.toString());       // 'Hello'
console.log(buf.toString('hex'));  // '48656c6c6f'
console.log(buf.toString('base64')); // 'SGVsbG8='

// Access individual bytes
console.log(buf[0]);  // 72 (ASCII for 'H')
console.log(buf[1]);  // 101 (ASCII for 'e')

// Length
console.log(buf.length);  // 5
```

### Writing to Buffers

```javascript
const buf = Buffer.alloc(10);

// Write string
buf.write('Hello');
console.log(buf.toString());  // 'Hello'

// Write at specific position
buf.write('World', 5);
console.log(buf.toString());  // 'HelloWorld'
```

### Comparing Buffers

```javascript
const buf1 = Buffer.from('ABC');
const buf2 = Buffer.from('ABC');
const buf3 = Buffer.from('ABD');

console.log(buf1.equals(buf2));  // true
console.log(buf1.equals(buf3));  // false

console.log(buf1.compare(buf3));  // -1 (buf1 < buf3)
```

### Concatenating Buffers

```javascript
const buf1 = Buffer.from('Hello ');
const buf2 = Buffer.from('World');

const result = Buffer.concat([buf1, buf2]);
console.log(result.toString());  // 'Hello World'
```

### Slicing Buffers

```javascript
const buf = Buffer.from('Hello World');

const slice = buf.slice(0, 5);
console.log(slice.toString());  // 'Hello'

// Note: slice creates a view, not a copy
slice[0] = 72;
console.log(buf.toString());  // Original also changed!
```

---

## Encoding Types

```javascript
const text = 'Hello';

// UTF-8 (default)
const buf1 = Buffer.from(text, 'utf8');

// ASCII
const buf2 = Buffer.from(text, 'ascii');

// Base64
const buf3 = Buffer.from(text, 'base64');

// Hex
const buf4 = Buffer.from(text, 'hex');

// To Base64
console.log(buf1.toString('base64'));  // 'SGVsbG8='
```

---

## Stream Events

### Readable Stream Events

```javascript
const readStream = fs.createReadStream('file.txt');

readStream.on('data', (chunk) => {
  // New chunk available
});

readStream.on('end', () => {
  // No more data
});

readStream.on('error', (err) => {
  // Error occurred
});

readStream.on('close', () => {
  // Stream closed
});
```

### Writable Stream Events

```javascript
const writeStream = fs.createWriteStream('file.txt');

writeStream.on('finish', () => {
  // All data written
});

writeStream.on('error', (err) => {
  // Error occurred
});

writeStream.on('close', () => {
  // Stream closed
});
```

---

## Stream vs Traditional

### ❌ Without Streams (Loads entire file)

```javascript
const fs = require('fs');

fs.readFile('large-file.txt', (err, data) => {
  if (err) throw err;
  console.log(data);
  // Problem: Entire file loaded into memory!
});
```

### ✅ With Streams (Memory efficient)

```javascript
const fs = require('fs');

const readStream = fs.createReadStream('large-file.txt');

readStream.on('data', (chunk) => {
  console.log(chunk);
  // Processes one chunk at a time
});
```

---

## Backpressure

When writable stream can't keep up with readable stream:

```javascript
const fs = require('fs');

const readStream = fs.createReadStream('input.txt');
const writeStream = fs.createWriteStream('output.txt');

readStream.on('data', (chunk) => {
  const canWrite = writeStream.write(chunk);
  
  if (!canWrite) {
    // Pause reading until drain
    readStream.pause();
  }
});

writeStream.on('drain', () => {
  // Resume reading
  readStream.resume();
});

// Or just use pipe (handles backpressure automatically)
readStream.pipe(writeStream);
```

---

## Best Practices

✅ **Use streams for large files**: Don't load everything into memory  
✅ **Use pipe()**: Handles backpressure automatically  
✅ **Handle errors**: Listen to 'error' event  
✅ **Close streams**: Use `stream.end()` or `stream.destroy()`  
✅ **Use async iterators**: Modern alternative to events  
❌ **Don't ignore backpressure**: Can cause memory issues  

---

## Modern Async Iterator

```javascript
const fs = require('fs');

async function readFile() {
  const stream = fs.createReadStream('file.txt', { encoding: 'utf8' });
  
  for await (const chunk of stream) {
    console.log(chunk);
  }
  
  console.log('Done!');
}

readFile();
```

---

## Summary

**Streams:**
- Process data in chunks (memory efficient)
- Four types: Readable, Writable, Duplex, Transform
- Use `pipe()` to connect streams
- Listen to events: `data`, `end`, `error`, `finish`

**Buffers:**
- Temporary storage for binary data
- Create with `Buffer.from()` or `Buffer.alloc()`
- Convert with `toString()` or `toJSON()`
- Support multiple encodings (utf8, hex, base64)

**Next:** Learn about the HTTP module and creating servers!

