# 03. Cache Invalidation Strategies

## The Challenge

> "There are only two hard things in Computer Science: cache invalidation and naming things." — Phil Karlton

**Problem:** How to keep cache consistent with database?

**Options:**
1. **TTL-based**: Let cache expire automatically
2. **Manual invalidation**: Delete/update on changes
3. **Event-driven**: Invalidate based on events
4. **Write-through**: Update cache on writes

---

## TTL-Based Invalidation

**Strategy:** Set expiration time on all cached data.

### Basic Approach

```python
import redis
import json

redis_client = redis.Redis()

def cache_user(user_id, data, ttl=3600):
    key = f"user:{user_id}"
    redis_client.setex(key, ttl, json.dumps(data))
```

### Adaptive TTL

Set TTL based on data characteristics:

```python
def get_ttl(data_type):
    ttl_config = {
        'hot_data': 60,        # 1 minute (trending posts)
        'warm_data': 3600,     # 1 hour (user profiles)
        'cold_data': 86400,    # 1 day (static content)
        'config': 604800       # 1 week (app config)
    }
    return ttl_config.get(data_type, 3600)

def cache_data(key, data, data_type):
    ttl = get_ttl(data_type)
    redis_client.setex(key, ttl, json.dumps(data))
```

### Advantages

✅ **Simple**: No complex invalidation logic
✅ **Automatic**: No manual intervention
✅ **Memory efficient**: Old data auto-removed

### Disadvantages

❌ **Stale data**: Cache outdated until expiration
❌ **Cache stampede**: Multiple requests on expiration

---

## Manual Invalidation

**Strategy:** Explicitly delete/update cache on data changes.

### Delete on Update

```python
def update_user(user_id, data):
    # 1. Update database
    db.update(f"UPDATE users SET ... WHERE id={user_id}")
    
    # 2. Invalidate cache
    redis_client.delete(f"user:{user_id}")
    
    # Next read will refresh cache from DB
```

### Update on Write

```python
def update_user(user_id, data):
    # 1. Update database
    db.update(f"UPDATE users SET ... WHERE id={user_id}")
    
    # 2. Update cache immediately
    redis_client.setex(
        f"user:{user_id}",
        3600,
        json.dumps(data)
    )
```

### Pattern-Based Invalidation

Invalidate multiple related keys:

```python
def invalidate_user_cache(user_id):
    # Get all keys for this user
    pattern = f"user:{user_id}:*"
    
    cursor = 0
    while True:
        cursor, keys = redis_client.scan(
            cursor,
            match=pattern,
            count=100
        )
        
        if keys:
            redis_client.delete(*keys)
        
        if cursor == 0:
            break
```

### Advantages

✅ **Immediate consistency**: Cache updated instantly
✅ **Control**: Precise invalidation logic

### Disadvantages

❌ **Complexity**: More code to maintain
❌ **Coupling**: App logic tied to cache
❌ **Error-prone**: Easy to miss invalidation

---

## Event-Driven Invalidation

**Strategy:** Use message queue/events to trigger invalidation.

### Using Message Queue

```python
import redis
import json
from kafka import KafkaConsumer

def invalidation_worker():
    consumer = KafkaConsumer(
        'cache-invalidation',
        bootstrap_servers=['localhost:9092']
    )
    
    redis_client = redis.Redis()
    
    for message in consumer:
        event = json.loads(message.value)
        
        if event['type'] == 'user_updated':
            redis_client.delete(f"user:{event['user_id']}")
        
        elif event['type'] == 'post_deleted':
            redis_client.delete(f"post:{event['post_id']}")
```

**Producer (on data change):**

```python
from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers=['localhost:9092'])

def update_user(user_id, data):
    # 1. Update database
    db.update(f"UPDATE users SET ... WHERE id={user_id}")
    
    # 2. Publish invalidation event
    event = {
        'type': 'user_updated',
        'user_id': user_id
    }
    producer.send('cache-invalidation', json.dumps(event).encode())
```

### Using Redis Pub/Sub

```python
# Publisher
def update_user(user_id, data):
    db.update(...)
    
    # Publish invalidation message
    redis_client.publish(
        'cache:invalidate',
        json.dumps({'key': f'user:{user_id}'})
    )

# Subscriber
def invalidation_listener():
    pubsub = redis_client.pubsub()
    pubsub.subscribe('cache:invalidate')
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            redis_client.delete(data['key'])
```

### Advantages

✅ **Decoupled**: Invalidation logic separate
✅ **Scalable**: Multiple cache servers
✅ **Reliable**: Message queue ensures delivery

### Disadvantages

❌ **Complexity**: Requires message queue infrastructure
❌ **Latency**: Slight delay in invalidation

---

## Cache Stampede Prevention

**Problem:** Many requests hit database when cache expires.

```
Cache expires → 100 requests → 100 DB queries → DB overload!
```

### Solution 1: Locking

**Only first request queries DB, others wait.**

```python
import redis
import time

def get_user_with_lock(user_id):
    key = f"user:{user_id}"
    lock_key = f"lock:{key}"
    
    # Try cache
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    
    # Try to acquire lock
    lock_acquired = redis_client.set(
        lock_key,
        '1',
        nx=True,  # Only set if not exists
        ex=10     # Lock expires in 10s
    )
    
    if lock_acquired:
        try:
            # This request queries DB
            user = db.query(f"SELECT * FROM users WHERE id={user_id}")
            redis_client.setex(key, 3600, json.dumps(user))
            return user
        finally:
            # Release lock
            redis_client.delete(lock_key)
    else:
        # Another request is loading data
        # Wait and retry
        time.sleep(0.1)
        return get_user_with_lock(user_id)
```

### Solution 2: Probabilistic Early Expiration

**Refresh cache before expiration randomly.**

```python
import random
import time

def get_user_with_early_refresh(user_id):
    key = f"user:{user_id}"
    
    cached = redis_client.get(key)
    if cached:
        # Get TTL
        ttl = redis_client.ttl(key)
        
        # Probabilistically refresh early
        # Higher probability as expiration approaches
        if ttl > 0:
            refresh_probability = 1 - (ttl / 3600)  # Original TTL: 3600
            
            if random.random() < refresh_probability:
                # Refresh cache asynchronously
                threading.Thread(
                    target=refresh_user_cache,
                    args=(user_id,)
                ).start()
        
        return json.loads(cached)
    
    # Cache miss - query DB
    user = db.query(f"SELECT * FROM users WHERE id={user_id}")
    redis_client.setex(key, 3600, json.dumps(user))
    return user
```

### Solution 3: Soft vs Hard Expiration

**Keep expired data, refresh in background.**

```python
def get_user_soft_expiry(user_id):
    key = f"user:{user_id}"
    meta_key = f"meta:{key}"
    
    cached = redis_client.get(key)
    meta = redis_client.get(meta_key)
    
    if cached and meta:
        expiry = int(meta)
        current_time = int(time.time())
        
        # Soft expiration check
        if current_time > expiry:
            # Data expired but still usable
            # Refresh asynchronously
            threading.Thread(
                target=refresh_user_cache,
                args=(user_id,)
            ).start()
        
        return json.loads(cached)
    
    # Hard miss - query DB
    user = db.query(f"SELECT * FROM users WHERE id={user_id}")
    
    # Store with soft expiry metadata
    redis_client.setex(key, 7200, json.dumps(user))  # Hard: 2 hours
    redis_client.setex(meta_key, 7200, str(int(time.time()) + 3600))  # Soft: 1 hour
    
    return user
```

---

## Cache Warming

**Strategy:** Pre-populate cache before traffic hits.

### On Application Startup

```python
def warm_cache():
    """Pre-load frequently accessed data"""
    
    # Top users
    top_users = db.query("SELECT * FROM users WHERE is_popular=true")
    for user in top_users:
        redis_client.setex(
            f"user:{user['id']}",
            3600,
            json.dumps(user)
        )
    
    # Trending posts
    trending = db.query("SELECT * FROM posts WHERE trending=true")
    for post in trending:
        redis_client.setex(
            f"post:{post['id']}",
            600,
            json.dumps(post)
        )
    
    print("Cache warmed successfully")

# Run on startup
if __name__ == "__main__":
    warm_cache()
    start_application()
```

### Scheduled Warming

```python
from apscheduler.schedulers.background import BackgroundScheduler

def warm_trending_cache():
    """Refresh trending content every 5 minutes"""
    trending = db.query("SELECT * FROM posts WHERE trending=true")
    for post in trending:
        redis_client.setex(f"post:{post['id']}", 600, json.dumps(post))

scheduler = BackgroundScheduler()
scheduler.add_job(warm_trending_cache, 'interval', minutes=5)
scheduler.start()
```

---

## Cache Invalidation Best Practices

### 1. Always Set TTL

```python
# ✅ Good: TTL as safety net
redis_client.setex(key, 3600, data)

# ❌ Bad: No TTL (stale data risk)
redis_client.set(key, data)
```

### 2. Use Versioning

```python
# Include version in key
def cache_key(entity, id, version="v1"):
    return f"{version}:{entity}:{id}"

redis_client.setex(cache_key("user", 1001, "v2"), 3600, data)

# Change version to invalidate all old cache
```

### 3. Combine Strategies

```python
def update_user(user_id, data):
    # 1. Update DB
    db.update(...)
    
    # 2. Update cache (write-through)
    redis_client.setex(f"user:{user_id}", 3600, json.dumps(data))
    
    # 3. Publish event (for other services)
    pubsub.publish('user:updated', {'user_id': user_id})
    
    # 4. TTL as safety net (already set in setex)
```

### 4. Monitor Invalidation

```python
def invalidate_with_metrics(key):
    redis_client.delete(key)
    metrics.increment('cache.invalidations')
```

### 5. Handle Partial Failures

```python
def update_user(user_id, data):
    # 1. Update DB (critical)
    db.update(...)
    
    # 2. Invalidate cache (non-critical)
    try:
        redis_client.delete(f"user:{user_id}")
    except redis.RedisError:
        # Log error but don't fail request
        logger.error(f"Failed to invalidate cache for user {user_id}")
        # TTL will eventually clean up
```

---

## Summary

**Invalidation Strategies:**
1. **TTL-based**: Automatic expiration (simple, may be stale)
2. **Manual**: Explicit delete/update (immediate, complex)
3. **Event-driven**: Message queue (decoupled, scalable)
4. **Write-through**: Update on write (always fresh)

**Cache Stampede Prevention:**
- Locking (first request loads, others wait)
- Probabilistic early expiration
- Soft vs hard expiration

**Cache Warming:**
- Pre-populate on startup
- Scheduled refresh for hot data

**Best Practices:**
- Always set TTL as safety net
- Use versioning for cache keys
- Combine multiple strategies
- Monitor invalidations
- Handle failures gracefully

---

## Interview Questions & Answers

### Q1: What is cache invalidation?
**A:** Process of removing or updating stale data from cache when source data changes.

**Strategies:**
- TTL-based (automatic expiration)
- Manual (delete/update on change)
- Event-driven (message queue)
- Write-through (update on write)

### Q2: What is cache stampede?
**A:** When cache expires, many requests simultaneously hit database:
```
Cache expires → 1000 requests → 1000 DB queries → DB crash!
```

**Solutions:**
- Locking (only first request queries DB)
- Probabilistic early refresh
- Soft expiration (serve stale, refresh background)

### Q3: How do you prevent cache stampede with locking?
**A:** Use distributed lock:
```python
lock_acquired = redis.set(
    f"lock:{key}",
    '1',
    nx=True,  # Only if not exists
    ex=10     # Lock expires in 10s
)

if lock_acquired:
    # This request queries DB
    data = db.query(...)
    redis.setex(key, 3600, data)
else:
    # Wait for other request to load
    time.sleep(0.1)
    retry()
```

### Q4: What's the difference between delete and update cache invalidation?
**A:**

**Delete (lazy):**
```python
db.update(...)
redis.delete(key)  # Next read refreshes
```

**Update (eager):**
```python
db.update(...)
redis.set(key, new_data)  # Immediate update
```

Update is faster for next read but adds write overhead.

### Q5: Why should you always set TTL?
**A:** Safety net for:
1. **Stale data**: Auto-cleanup if invalidation missed
2. **Memory**: Prevents cache bloat
3. **Failures**: If invalidation fails, TTL eventually cleans up

```python
# ✅ Always set TTL
redis.setex(key, 3600, data)
```

### Q6: What is cache warming?
**A:** Pre-populating cache before traffic:
```python
def warm_cache():
    # Pre-load frequently accessed data
    popular_users = db.query("SELECT * FROM users WHERE popular=true")
    for user in popular_users:
        redis.setex(f"user:{user.id}", 3600, user)
```

Prevents cold start cache misses.

### Q7: How do you invalidate multiple related keys?
**A:** Use pattern matching with SCAN:
```python
cursor = 0
while True:
    cursor, keys = redis.scan(
        cursor,
        match=f"user:{user_id}:*",
        count=100
    )
    if keys:
        redis.delete(*keys)
    if cursor == 0:
        break
```

### Q8: What is soft vs hard expiration?
**A:**
- **Hard expiration**: Data deleted, cache miss on access
- **Soft expiration**: Data kept but marked stale, refresh in background

```python
# Soft: TTL expired but data still usable
if is_expired(meta) and cached:
    async_refresh(key)  # Background refresh
    return cached       # Serve stale
```

### Q9: Should you invalidate cache if DB update fails?
**A:** **No!** Only invalidate after successful DB update:
```python
# ✅ Good
db.update(...)  # May throw exception
redis.delete(key)  # Only if update succeeds

# ❌ Bad
redis.delete(key)
db.update(...)  # If this fails, cache is inconsistent
```

### Q10: What is probabilistic early expiration?
**A:** Randomly refresh cache before expiration:
```python
ttl = redis.ttl(key)
refresh_probability = 1 - (ttl / original_ttl)

if random.random() < refresh_probability:
    async_refresh(key)  # Refresh early
```

Prevents stampede by distributing refreshes.

### Q11: How do you handle cache invalidation failures?
**A:** Don't fail the request:
```python
try:
    db.update(...)
    redis.delete(key)
except RedisError:
    # Log but don't fail
    logger.error("Cache invalidation failed")
    # TTL will eventually clean up
```

### Q12: What is event-driven invalidation?
**A:** Use message queue to decouple invalidation:
```python
# On update
db.update(...)
kafka.send('cache-invalidate', {'key': 'user:1001'})

# Invalidation worker
for msg in kafka.consume('cache-invalidate'):
    redis.delete(msg['key'])
```

Benefits: Decoupled, scalable, reliable.

### Q13: Should you cache data without TTL?
**A:** **Rarely.** Only for:
- Application config (changes require restart)
- Static reference data (never changes)

**Always set TTL** for user-generated content:
```python
redis.setex('user:1001', 3600, data)  # ✅
```

### Q14: What is cache versioning?
**A:** Include version in cache key:
```python
# v1 cache
redis.set('v1:user:1001', data)

# Deploy new version
redis.set('v2:user:1001', data)

# Old cache auto-expires (TTL)
```

Invalidates all cache on version change.

### Q15: How often should you refresh cache?
**A:** Depends on data characteristics:

**Hot data** (trending): 1-5 minutes
**Warm data** (user profiles): 1 hour
**Cold data** (static content): 1 day
**Config**: 1 week or manual invalidation

```python
ttl_config = {
    'trending': 300,
    'users': 3600,
    'content': 86400,
    'config': 604800
}
```

**Next:** Distributed Caching Patterns!