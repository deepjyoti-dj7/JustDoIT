# 04. Distributed Caching Patterns

## Why Distributed Caching?

**Single cache server limitations:**
- Limited memory capacity
- Single point of failure (SPOF)
- Network bottleneck
- Can't handle high traffic

**Distributed caching solves:**
- **Scalability**: More memory, higher throughput
- **Availability**: No single point of failure
- **Performance**: Request distribution

---

## Cache Distribution Strategies

### 1. Client-Side Partitioning

Client decides which cache server to use.

```
┌──────────────┐
│ Application  │
│  (Client)    │
└─────┬──┬──────┘
      │   │
   ┌──┼───┼──┐
   │   │    │
   ▼   ▼    ▼
┌──────────────┐
│ Redis Node 1 │
└──────────────┘
┌──────────────┐
│ Redis Node 2 │
└──────────────┘
┌──────────────┐
│ Redis Node 3 │
└──────────────┘
```

### 2. Proxy-Based Partitioning

Proxy layer handles distribution.

```
┌──────────────┐
│ Application  │
└──────┬───────┘
       │
       ▼
┌──────────────┐
│ Cache Proxy  │ (Twemproxy, Envoy)
└────┬──┬──────┘
     │   │
  ┌──┼───┼──┐
  │   │    │
  ▼   ▼    ▼
Node1 Node2 Node3
```

### 3. Redis Cluster

Redis handles distribution automatically.

```
┌──────────────┐
│ Application  │
└──────┬───────┘
       │
       ▼
┌────────────────────────────┐
│      Redis Cluster        │
│                            │
│ Node1  Node2  Node3       │
│ (0-5460) (5461-10922)     │
│          (10923-16383)    │
└────────────────────────────┘
```

---

## Consistent Hashing

**Problem:** Simple modulo hashing breaks on node changes.

### Modulo Hashing (Naive)

```python
def get_node(key, num_nodes):
    return hash(key) % num_nodes

# 3 nodes: 0, 1, 2
get_node("user:1001", 3)  # → Node 1
get_node("user:1002", 3)  # → Node 0
```

**Problem:** Add/remove node → most keys rehash!

```python
# Add node (now 4 nodes)
get_node("user:1001", 4)  # → Node 1 (changed!)
get_node("user:1002", 4)  # → Node 2 (changed!)
# 75% of keys move to different nodes!
```

### Consistent Hashing Solution

**Concept:** Hash ring (0 to 2^32-1)

```
         Node 1
           •
      ╭─────╮
     │       │
    •         •  ← Keys
Node 3     Node 2
     │       │
      ╰─────╯
```

**Key placement:** Clockwise to next node.

#### Implementation

```python
import hashlib
import bisect

class ConsistentHash:
    def __init__(self, nodes=None, virtual_nodes=150):
        self.virtual_nodes = virtual_nodes
        self.ring = {}  # hash → node
        self.sorted_keys = []
        
        if nodes:
            for node in nodes:
                self.add_node(node)
    
    def _hash(self, key):
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def add_node(self, node):
        """Add node with virtual nodes"""
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            h = self._hash(virtual_key)
            self.ring[h] = node
            bisect.insort(self.sorted_keys, h)
    
    def remove_node(self, node):
        """Remove node and its virtual nodes"""
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            h = self._hash(virtual_key)
            del self.ring[h]
            self.sorted_keys.remove(h)
    
    def get_node(self, key):
        """Get node for key"""
        if not self.ring:
            return None
        
        h = self._hash(key)
        
        # Find next node on ring (clockwise)
        idx = bisect.bisect_right(self.sorted_keys, h)
        if idx == len(self.sorted_keys):
            idx = 0
        
        return self.ring[self.sorted_keys[idx]]

# Usage
ch = ConsistentHash(['redis1:6379', 'redis2:6379', 'redis3:6379'])

print(ch.get_node('user:1001'))  # redis2:6379
print(ch.get_node('user:1002'))  # redis1:6379

# Add node - only ~25% keys move!
ch.add_node('redis4:6379')
print(ch.get_node('user:1001'))  # Still redis2:6379 (most likely)
```

### Virtual Nodes

**Problem:** Uneven distribution with few nodes.

**Solution:** Each physical node has multiple virtual nodes on ring.

```python
# redis1 gets virtual nodes:
# redis1:0, redis1:1, ..., redis1:149

# Better distribution!
```

---

## Redis Cluster

Built-in distributed Redis solution.

### Architecture

**16384 hash slots** distributed across nodes:

```
Node 1: Slots 0-5460
Node 2: Slots 5461-10922
Node 3: Slots 10923-16383
```

### Setup Redis Cluster

```bash
# Create 6 Redis instances (3 masters, 3 replicas)
for port in 7000 7001 7002 7003 7004 7005; do
    mkdir -p cluster/$port
    cat > cluster/$port/redis.conf <<EOF
port $port
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
EOF
    redis-server cluster/$port/redis.conf &
done

# Create cluster
redis-cli --cluster create \
  127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 \
  127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \
  --cluster-replicas 1
```

### Using Redis Cluster

#### Python

```python
from rediscluster import RedisCluster

startup_nodes = [
    {"host": "127.0.0.1", "port": "7000"},
    {"host": "127.0.0.1", "port": "7001"},
    {"host": "127.0.0.1", "port": "7002"}
]

rc = RedisCluster(
    startup_nodes=startup_nodes,
    decode_responses=True,
    skip_full_coverage_check=True
)

# Use like normal Redis
rc.set('user:1001', 'John')
print(rc.get('user:1001'))
```

#### Node.js

```javascript
const Redis = require('ioredis');

const cluster = new Redis.Cluster([
  { host: '127.0.0.1', port: 7000 },
  { host: '127.0.0.1', port: 7001 },
  { host: '127.0.0.1', port: 7002 }
]);

cluster.set('user:1001', 'John');
cluster.get('user:1001', (err, result) => {
  console.log(result);
});
```

### Hash Tags

Force related keys to same slot:

```bash
# Without hash tags - different slots
SET user:1001:profile "..."
SET user:1001:settings "..."
# May be on different nodes - can't use multi-key operations

# With hash tags - same slot
SET user:{1001}:profile "..."
SET user:{1001}:settings "..."
# {1001} is hashed - both on same node

# Multi-key operations work
MGET user:{1001}:profile user:{1001}:settings
```

---

## Multi-Level Caching

Multiple cache layers for optimal performance.

### L1 (Application) + L2 (Redis)

```python
from cachetools import TTLCache
import redis

class MultiLevelCache:
    def __init__(self):
        # L1: In-memory cache (fast, small)
        self.l1 = TTLCache(maxsize=1000, ttl=60)
        
        # L2: Redis (slower, larger)
        self.l2 = redis.Redis()
    
    def get(self, key):
        # Try L1
        if key in self.l1:
            print("L1 hit")
            return self.l1[key]
        
        # Try L2
        value = self.l2.get(key)
        if value:
            print("L2 hit")
            # Promote to L1
            self.l1[key] = value
            return value
        
        # Cache miss
        print("Cache miss")
        return None
    
    def set(self, key, value, ttl=3600):
        # Write to both levels
        self.l1[key] = value
        self.l2.setex(key, ttl, value)

cache = MultiLevelCache()

# First access: L2 hit
value = cache.get('user:1001')

# Second access: L1 hit (faster!)
value = cache.get('user:1001')
```

### Browser + CDN + Redis

```
┌───────────────────┐
│ L1: Browser Cache │ (1 second)
└────────┬──────────┘
         │
         ▼
┌───────────────────┐
│   L2: CDN Cache   │ (1 minute)
└────────┬──────────┘
         │
         ▼
┌───────────────────┐
│  L3: Redis Cache │ (1 hour)
└────────┬──────────┘
         │
         ▼
┌───────────────────┐
│   L4: Database   │
└───────────────────┘
```

---

## Cache Replication

Multiple cache replicas for high availability.

### Redis Sentinel

Automatic failover for Redis:

```bash
# sentinel.conf
sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 10000

# Start sentinel
redis-sentinel sentinel.conf
```

#### Using Sentinel in Python

```python
from redis.sentinel import Sentinel

sentinel = Sentinel([
    ('localhost', 26379),
    ('localhost', 26380),
    ('localhost', 26381)
], socket_timeout=0.1)

# Get master (for writes)
master = sentinel.master_for('mymaster', socket_timeout=0.1)
master.set('key', 'value')

# Get slave (for reads)
slave = sentinel.slave_for('mymaster', socket_timeout=0.1)
value = slave.get('key')
```

---

## Best Practices

### 1. Use Consistent Hashing

```python
# ✅ Good: Consistent hashing
ch = ConsistentHash(nodes)
node = ch.get_node(key)

# ❌ Bad: Modulo hashing (breaks on scale)
node = hash(key) % len(nodes)
```

### 2. Set Appropriate TTL per Level

```python
# L1 (app cache): Short TTL
l1_cache.set(key, value, ttl=60)

# L2 (Redis): Medium TTL
redis.setex(key, 3600, value)

# L3 (DB): Permanent
db.insert(key, value)
```

### 3. Use Hash Tags for Related Keys

```bash
# ✅ Good: Related keys on same node
SET user:{1001}:profile "..."
SET user:{1001}:settings "..."
MGET user:{1001}:profile user:{1001}:settings  # Works!

# ❌ Bad: May be on different nodes
SET user:1001:profile "..."
SET user:1001:settings "..."
MGET user:1001:profile user:1001:settings  # May fail in cluster
```

### 4. Monitor Cache Distribution

```python
def check_distribution():
    node_counts = {}
    for key in all_keys:
        node = ch.get_node(key)
        node_counts[node] = node_counts.get(node, 0) + 1
    
    # Check for imbalance
    avg = sum(node_counts.values()) / len(node_counts)
    for node, count in node_counts.items():
        if count > avg * 1.2 or count < avg * 0.8:
            alert(f"Unbalanced distribution on {node}")
```

### 5. Handle Cluster Resharding

```python
try:
    result = redis_cluster.get(key)
except redis.exceptions.ClusterError as e:
    # Handle MOVED/ASK redirections
    if 'MOVED' in str(e) or 'ASK' in str(e):
        # Cluster topology changed
        redis_cluster.connection_pool.nodes.reset()
        result = redis_cluster.get(key)
```

---

## Summary

**Distribution Strategies:**
1. **Client-side**: App handles partitioning (simple)
2. **Proxy-based**: Proxy layer handles routing (decoupled)
3. **Redis Cluster**: Built-in distribution (production-ready)

**Consistent Hashing:**
- Hash ring (0 to 2^32-1)
- Virtual nodes for balanced distribution
- Minimal rehashing on node changes

**Multi-Level Caching:**
- L1: App cache (small, fast, short TTL)
- L2: Redis (medium, distributed, medium TTL)
- L3: Database (large, slow, permanent)

**Redis Cluster:**
- 16384 hash slots
- Automatic sharding
- Built-in replication
- Use hash tags for related keys

**Best Practices:**
- Use consistent hashing
- Set appropriate TTL per level
- Monitor cache distribution
- Use hash tags for multi-key operations
- Handle cluster resharding gracefully

---

## Interview Questions & Answers

### Q1: What is distributed caching?
**A:** Caching across multiple cache servers for:
- **Scalability**: More memory/throughput
- **Availability**: No single point of failure
- **Performance**: Distribute load

**Strategies:** Client-side, proxy-based, Redis Cluster.

### Q2: What's wrong with modulo hashing?
**A:** When nodes change, most keys rehash:
```python
# 3 nodes
hash(key) % 3  # Node 1

# Add node (4 nodes)
hash(key) % 4  # Node 0 (different!)
# 75% of keys move!
```
Causes cache invalidation.

### Q3: What is consistent hashing?
**A:** Hash ring where keys/nodes mapped to ring positions. Key goes to next node clockwise.

**Benefits:**
- Add/remove node: Only ~1/N keys move (N = # nodes)
- Minimal disruption
- Balanced distribution with virtual nodes

### Q4: What are virtual nodes in consistent hashing?
**A:** Multiple positions on ring for each physical node:
```python
# redis1 has virtual nodes:
# redis1:0, redis1:1, ..., redis1:149
```

**Why?** Better key distribution, especially with few nodes.

### Q5: How does Redis Cluster work?
**A:** 
- **16384 hash slots** distributed across nodes
- Each key mapped to slot: `CRC16(key) % 16384`
- Automatic sharding and replication

**Example:**
```
Node 1: Slots 0-5460
Node 2: Slots 5461-10922
Node 3: Slots 10923-16383
```

### Q6: What are hash tags in Redis Cluster?
**A:** Force related keys to same slot:
```bash
# {1001} is hashed - both on same node
SET user:{1001}:profile "..."
SET user:{1001}:settings "..."

# Multi-key operations work
MGET user:{1001}:profile user:{1001}:settings
```

### Q7: What is multi-level caching?
**A:** Multiple cache layers:
```
L1: App cache (60s TTL) → Fast, small
L2: Redis (1h TTL) → Medium, distributed
L3: Database → Slow, permanent
```

Hit L1 → fastest. Miss L1 → try L2 → promote to L1.

### Q8: Why use multi-level caching?
**A:**
- **Performance**: L1 faster than L2, L2 faster than DB
- **Efficiency**: L1 reduces L2 load
- **Cost**: L1 (free) reduces L2 (paid) usage

**Example:** Browser (1s) → CDN (1m) → Redis (1h) → DB

### Q9: How do you implement consistent hashing?
**A:**
```python
class ConsistentHash:
    def __init__(self, nodes, virtual_nodes=150):
        self.ring = {}  # hash → node
        self.sorted_keys = []
        for node in nodes:
            for i in range(virtual_nodes):
                h = hash(f"{node}:{i}")
                self.ring[h] = node
                self.sorted_keys.append(h)
        self.sorted_keys.sort()
    
    def get_node(self, key):
        h = hash(key)
        # Find next node on ring (clockwise)
        idx = bisect_right(self.sorted_keys, h)
        if idx == len(self.sorted_keys):
            idx = 0
        return self.ring[self.sorted_keys[idx]]
```

### Q10: What is cache sharding?
**A:** Splitting cache data across multiple servers:

**Horizontal sharding:** Each server holds subset of keys
```
Node 1: user:1-1000
Node 2: user:1001-2000
Node 3: user:2001-3000
```

**Hash-based:** Key hashed to determine node.

### Q11: How does Redis Sentinel work?
**A:** Monitors Redis master/replicas:
- Detects master failure
- Promotes replica to master (automatic failover)
- Updates clients with new master

```python
sentinel = Sentinel([('localhost', 26379)])
master = sentinel.master_for('mymaster')  # Auto-connects to current master
```

### Q12: What's the difference between Redis Cluster and Sentinel?
**A:**

**Sentinel:**
- Master-replica replication
- Automatic failover
- **No sharding** (all data on master)

**Cluster:**
- Automatic **sharding** (16384 slots)
- Built-in replication
- Horizontal scaling

Use **Sentinel** for HA, **Cluster** for scale.

### Q13: How many virtual nodes should you use?
**A:** Typically **100-200** per physical node:
- Too few (<50): Uneven distribution
- Too many (>500): Slower lookups

Default: **150** is good balance.

### Q14: Can you use MGET in Redis Cluster?
**A:** Only if all keys on **same slot**:
```bash
# ✅ Works: Same slot (hash tag)
MGET user:{1001}:name user:{1001}:email

# ❌ Fails: Different slots
MGET user:1001:name user:1002:name
# (error) CROSSSLOT Keys in request don't hash to the same slot
```

### Q15: How do you handle node failures in distributed cache?
**A:**

**Consistent hashing:** Keys on failed node redistributed to next node.

**Redis Cluster:** Automatic failover to replica.

**Application code:**
```python
try:
    value = cache.get(key)
except CacheError:
    # Fallback to database
    value = db.query(...)
```

Always have DB fallback!

**End of Caching Patterns & Strategies!**