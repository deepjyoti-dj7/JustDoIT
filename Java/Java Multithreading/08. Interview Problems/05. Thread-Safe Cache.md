# ðŸ’¾ Thread-Safe Cache

## ðŸ“– What is a Thread-Safe Cache?

A concurrent data structure that stores key-value pairs with thread-safe read/write operations, eviction policies, and TTL support.

**Key Features:**
- Thread-safe operations
- Eviction policies (LRU, LFU, TTL)
- Concurrent access
- Cache misses handling

---

## ðŸ’» Example 1: Simple Thread-Safe Cache

```java
import java.util.concurrent.ConcurrentHashMap;

class SimpleCache<K, V> {
    private final ConcurrentHashMap<K, V> cache = new ConcurrentHashMap<>();
    
    public V get(K key) {
        V value = cache.get(key);
        System.out.println(Thread.currentThread().getName() + " GET " + key + 
                          " = " + (value != null ? value : "MISS"));
        return value;
    }
    
    public void put(K key, V value) {
        cache.put(key, value);
        System.out.println(Thread.currentThread().getName() + " PUT " + key + 
                          " = " + value);
    }
    
    public void remove(K key) {
        cache.remove(key);
        System.out.println(Thread.currentThread().getName() + " REMOVE " + key);
    }
    
    public int size() {
        return cache.size();
    }
    
    public static void main(String[] args) {
        SimpleCache<String, Integer> cache = new SimpleCache<>();
        
        // Multiple threads accessing cache
        for (int i = 1; i <= 5; i++) {
            final int threadId = i;
            new Thread(() -> {
                cache.put("key" + threadId, threadId * 100);
                cache.get("key" + threadId);
                cache.get("key" + (threadId % 3 + 1));
            }, "Thread-" + i).start();
        }
    }
}
```

---

## ðŸ’» Example 2: LRU Cache

```java
import java.util.*;
import java.util.concurrent.locks.*;

class LRUCache<K, V> {
    private final int capacity;
    private final Map<K, Node<K, V>> cache = new HashMap<>();
    private final Node<K, V> head = new Node<>(null, null);
    private final Node<K, V> tail = new Node<>(null, null);
    private final ReadWriteLock lock = new ReentrantReadWriteLock();
    
    static class Node<K, V> {
        K key;
        V value;
        Node<K, V> prev, next;
        
        Node(K key, V value) {
            this.key = key;
            this.value = value;
        }
    }
    
    public LRUCache(int capacity) {
        this.capacity = capacity;
        head.next = tail;
        tail.prev = head;
    }
    
    public V get(K key) {
        lock.writeLock().lock();
        try {
            Node<K, V> node = cache.get(key);
            if (node == null) {
                System.out.println(Thread.currentThread().getName() + 
                                  " GET " + key + " = MISS");
                return null;
            }
            
            // Move to front (most recently used)
            removeNode(node);
            addToFront(node);
            
            System.out.println(Thread.currentThread().getName() + 
                              " GET " + key + " = " + node.value + " [HIT]");
            return node.value;
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public void put(K key, V value) {
        lock.writeLock().lock();
        try {
            Node<K, V> node = cache.get(key);
            
            if (node != null) {
                // Update existing
                node.value = value;
                removeNode(node);
                addToFront(node);
            } else {
                // Add new
                if (cache.size() >= capacity) {
                    // Evict LRU
                    Node<K, V> lru = tail.prev;
                    removeNode(lru);
                    cache.remove(lru.key);
                    System.out.println("  [EVICTED] " + lru.key);
                }
                
                node = new Node<>(key, value);
                cache.put(key, node);
                addToFront(node);
            }
            
            System.out.println(Thread.currentThread().getName() + 
                              " PUT " + key + " = " + value);
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    private void addToFront(Node<K, V> node) {
        node.next = head.next;
        node.prev = head;
        head.next.prev = node;
        head.next = node;
    }
    
    private void removeNode(Node<K, V> node) {
        node.prev.next = node.next;
        node.next.prev = node.prev;
    }
    
    public void printCache() {
        lock.readLock().lock();
        try {
            System.out.print("Cache: [");
            Node<K, V> curr = head.next;
            while (curr != tail) {
                System.out.print(curr.key + "=" + curr.value);
                curr = curr.next;
                if (curr != tail) System.out.print(", ");
            }
            System.out.println("]");
        } finally {
            lock.readLock().unlock();
        }
    }
    
    public static void main(String[] args) throws InterruptedException {
        LRUCache<String, Integer> cache = new LRUCache<>(3);
        
        // Multiple threads accessing cache
        Runnable task1 = () -> {
            cache.put("A", 1);
            cache.put("B", 2);
            cache.get("A");
        };
        
        Runnable task2 = () -> {
            cache.put("C", 3);
            cache.put("D", 4);
            cache.get("B");
        };
        
        Thread t1 = new Thread(task1, "Thread-1");
        Thread t2 = new Thread(task2, "Thread-2");
        
        t1.start();
        Thread.sleep(500);
        t2.start();
        
        t1.join();
        t2.join();
        
        cache.printCache();
    }
}
```

---

## ðŸ’» Example 3: Cache with TTL (Time-To-Live)

```java
import java.util.concurrent.*;

class TTLCache<K, V> {
    private final ConcurrentHashMap<K, CacheEntry<V>> cache = new ConcurrentHashMap<>();
    private final long defaultTTL;
    
    static class CacheEntry<V> {
        final V value;
        final long expiryTime;
        
        CacheEntry(V value, long ttl) {
            this.value = value;
            this.expiryTime = System.currentTimeMillis() + ttl;
        }
        
        boolean isExpired() {
            return System.currentTimeMillis() > expiryTime;
        }
    }
    
    public TTLCache(long defaultTTL) {
        this.defaultTTL = defaultTTL;
        startCleanupTask();
    }
    
    public V get(K key) {
        CacheEntry<V> entry = cache.get(key);
        
        if (entry == null) {
            System.out.println(Thread.currentThread().getName() + 
                              " GET " + key + " = MISS");
            return null;
        }
        
        if (entry.isExpired()) {
            cache.remove(key);
            System.out.println(Thread.currentThread().getName() + 
                              " GET " + key + " = EXPIRED");
            return null;
        }
        
        System.out.println(Thread.currentThread().getName() + 
                          " GET " + key + " = " + entry.value);
        return entry.value;
    }
    
    public void put(K key, V value) {
        put(key, value, defaultTTL);
    }
    
    public void put(K key, V value, long ttl) {
        CacheEntry<V> entry = new CacheEntry<>(value, ttl);
        cache.put(key, entry);
        System.out.println(Thread.currentThread().getName() + 
                          " PUT " + key + " = " + value + " (TTL: " + ttl + "ms)");
    }
    
    private void startCleanupTask() {
        ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor();
        executor.scheduleAtFixedRate(() -> {
            int removed = 0;
            for (K key : cache.keySet()) {
                CacheEntry<V> entry = cache.get(key);
                if (entry != null && entry.isExpired()) {
                    cache.remove(key);
                    removed++;
                }
            }
            if (removed > 0) {
                System.out.println("[CLEANUP] Removed " + removed + " expired entries");
            }
        }, 1, 1, TimeUnit.SECONDS);
    }
    
    public static void main(String[] args) throws InterruptedException {
        TTLCache<String, String> cache = new TTLCache<>(3000); // 3 second default TTL
        
        cache.put("key1", "value1", 2000); // 2 seconds
        cache.put("key2", "value2", 5000); // 5 seconds
        
        Thread.sleep(1000);
        cache.get("key1"); // Should exist
        
        Thread.sleep(1500);
        cache.get("key1"); // Should be expired
        cache.get("key2"); // Should still exist
        
        Thread.sleep(4000);
        cache.get("key2"); // Should be expired
    }
}
```

---

## ðŸ’» Example 4: Cache with Loading

```java
import java.util.concurrent.*;
import java.util.function.Function;

class LoadingCache<K, V> {
    private final ConcurrentHashMap<K, CompletableFuture<V>> cache = 
        new ConcurrentHashMap<>();
    private final Function<K, V> loader;
    
    public LoadingCache(Function<K, V> loader) {
        this.loader = loader;
    }
    
    public V get(K key) {
        CompletableFuture<V> future = cache.computeIfAbsent(key, k -> 
            CompletableFuture.supplyAsync(() -> {
                System.out.println(Thread.currentThread().getName() + 
                                  " LOADING " + key);
                try {
                    Thread.sleep(2000); // Simulate expensive load
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
                V value = loader.apply(k);
                System.out.println(Thread.currentThread().getName() + 
                                  " LOADED " + key + " = " + value);
                return value;
            })
        );
        
        try {
            V value = future.get();
            System.out.println(Thread.currentThread().getName() + 
                              " GET " + key + " = " + value);
            return value;
        } catch (Exception e) {
            cache.remove(key);
            throw new RuntimeException(e);
        }
    }
    
    public void invalidate(K key) {
        cache.remove(key);
        System.out.println("INVALIDATED " + key);
    }
    
    public static void main(String[] args) throws InterruptedException {
        LoadingCache<Integer, String> cache = new LoadingCache<>(
            id -> "User-" + id
        );
        
        // Multiple threads request same key concurrently
        for (int i = 1; i <= 5; i++) {
            new Thread(() -> {
                cache.get(1); // All threads request same key
            }, "Thread-" + i).start();
        }
        
        Thread.sleep(3000);
        
        // New request (cached)
        new Thread(() -> cache.get(1), "Thread-6").start();
    }
}
```

---

## ðŸ’» Example 5: Write-Through Cache

```java
import java.util.concurrent.*;
import java.util.function.*;

class WriteThroughCache<K, V> {
    private final ConcurrentHashMap<K, V> cache = new ConcurrentHashMap<>();
    private final BiConsumer<K, V> writeToDatabase;
    private final Function<K, V> readFromDatabase;
    
    public WriteThroughCache(Function<K, V> readFromDatabase, 
                            BiConsumer<K, V> writeToDatabase) {
        this.readFromDatabase = readFromDatabase;
        this.writeToDatabase = writeToDatabase;
    }
    
    public V get(K key) {
        V value = cache.get(key);
        
        if (value == null) {
            System.out.println(Thread.currentThread().getName() + 
                              " Cache MISS, reading from DB");
            value = readFromDatabase.apply(key);
            if (value != null) {
                cache.put(key, value);
            }
        } else {
            System.out.println(Thread.currentThread().getName() + 
                              " Cache HIT");
        }
        
        System.out.println(Thread.currentThread().getName() + 
                          " GET " + key + " = " + value);
        return value;
    }
    
    public void put(K key, V value) {
        System.out.println(Thread.currentThread().getName() + 
                          " Writing to DB: " + key + " = " + value);
        writeToDatabase.accept(key, value); // Write to DB first
        cache.put(key, value); // Then update cache
        System.out.println(Thread.currentThread().getName() + 
                          " Cache updated");
    }
    
    public static void main(String[] args) throws InterruptedException {
        // Simulate database
        ConcurrentHashMap<Integer, String> database = new ConcurrentHashMap<>();
        database.put(1, "Initial-1");
        database.put(2, "Initial-2");
        
        WriteThroughCache<Integer, String> cache = new WriteThroughCache<>(
            key -> {
                try {
                    Thread.sleep(1000); // Simulate DB read delay
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                return database.get(key);
            },
            (key, value) -> {
                try {
                    Thread.sleep(500); // Simulate DB write delay
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                database.put(key, value);
            }
        );
        
        // Concurrent reads
        for (int i = 1; i <= 3; i++) {
            new Thread(() -> cache.get(1), "Reader-" + i).start();
        }
        
        Thread.sleep(2000);
        
        // Write
        new Thread(() -> cache.put(1, "Updated-1"), "Writer").start();
        
        Thread.sleep(1000);
        
        // Read updated value
        new Thread(() -> cache.get(1), "Reader-4").start();
    }
}
```

---

## ðŸ’» Example 6: Multi-Level Cache

```java
import java.util.concurrent.*;

class MultiLevelCache<K, V> {
    private final ConcurrentHashMap<K, V> l1Cache; // Fast, small
    private final ConcurrentHashMap<K, V> l2Cache; // Slower, larger
    private final int l1MaxSize;
    
    public MultiLevelCache(int l1MaxSize) {
        this.l1MaxSize = l1MaxSize;
        this.l1Cache = new ConcurrentHashMap<>();
        this.l2Cache = new ConcurrentHashMap<>();
    }
    
    public V get(K key) {
        // Check L1 first
        V value = l1Cache.get(key);
        if (value != null) {
            System.out.println(Thread.currentThread().getName() + 
                              " L1 HIT: " + key);
            return value;
        }
        
        // Check L2
        value = l2Cache.get(key);
        if (value != null) {
            System.out.println(Thread.currentThread().getName() + 
                              " L2 HIT: " + key);
            // Promote to L1
            if (l1Cache.size() < l1MaxSize) {
                l1Cache.put(key, value);
            }
            return value;
        }
        
        System.out.println(Thread.currentThread().getName() + " MISS: " + key);
        return null;
    }
    
    public void put(K key, V value) {
        // Put in L1
        if (l1Cache.size() >= l1MaxSize) {
            // Evict random entry to L2
            K evictKey = l1Cache.keys().nextElement();
            V evictValue = l1Cache.remove(evictKey);
            l2Cache.put(evictKey, evictValue);
            System.out.println("  [EVICTED to L2] " + evictKey);
        }
        
        l1Cache.put(key, value);
        System.out.println(Thread.currentThread().getName() + 
                          " PUT " + key + " in L1");
    }
    
    public void printStats() {
        System.out.println("L1 size: " + l1Cache.size() + ", L2 size: " + l2Cache.size());
    }
    
    public static void main(String[] args) {
        MultiLevelCache<String, Integer> cache = new MultiLevelCache<>(3);
        
        // Fill cache
        for (int i = 1; i <= 5; i++) {
            cache.put("key" + i, i * 100);
        }
        
        cache.printStats();
        
        // Access patterns
        cache.get("key1"); // L2 hit, promoted to L1
        cache.get("key1"); // L1 hit
        cache.get("key5"); // L1 hit
        
        cache.printStats();
    }
}
```

---

## ðŸ“Š Cache Patterns Comparison

| Pattern | Use Case | Complexity | Consistency |
|---------|----------|------------|-------------|
| **Simple** | Basic caching | Low | Eventual |
| **LRU** | Limited memory | Medium | Eventual |
| **TTL** | Expiring data | Medium | Time-based |
| **Write-Through** | Strong consistency | Medium | Strong |
| **Write-Behind** | High performance | High | Eventual |
| **Multi-Level** | Hierarchical storage | High | Eventual |

---

## ðŸ’¡ Best Practices

1. âœ… Use ConcurrentHashMap for simple cases
2. âœ… Implement eviction policy for bounded caches
3. âœ… Consider TTL for time-sensitive data
4. âœ… Use CompletableFuture to prevent cache stampede
5. âœ… Monitor hit/miss ratios
6. âœ… Implement proper invalidation strategy

---

## ðŸŽ¯ Interview Questions

1. **How to implement thread-safe LRU cache?**
2. **What is cache stampede? How to prevent it?**
3. **Write-through vs write-back cache?**
4. **How to handle cache expiration?**
5. **What is multi-level caching?**

---

## ðŸ“š Related Topics

- [ConcurrentHashMap](../../05.%20Concurrent%20Collections/01.%20ConcurrentHashMap.md)
- [CompletableFuture](../../04.%20Executor%20Framework/04.%20CompletableFuture.md)
