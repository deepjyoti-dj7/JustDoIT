# Caching Concepts - Complete Guide

---

## 1. Introduction to Caching

Caching is a technique to store frequently accessed data in a fast-access storage layer (cache) to improve application performance, reduce latency, and minimize load on backend systems like databases.

### Why Caching?
- **Performance**: Faster data retrieval (microseconds vs milliseconds)
- **Scalability**: Reduces load on databases and external services
- **Cost**: Lower infrastructure costs by reducing database queries
- **Availability**: Serves data even when backend is temporarily unavailable
- **User Experience**: Faster response times lead to better UX

---

## 2. How Caching Works

```
┌─────────┐
│ Client  │
└────┬────┘
     │ Request
     ▼
┌─────────────┐
│ Application │
└──────┬──────┘
       │
       ▼
┌──────────────┐    Cache Hit
│    Cache     │────────────────► Return data
└──────┬───────┘
       │ Cache Miss
       ▼
┌──────────────┐
│   Database   │
└──────┬───────┘
       │
       ▼
   Store in Cache
```

**Cache Hit**: Data found in cache (fast)
**Cache Miss**: Data not in cache, fetch from source (slow)

---

## 3. Cache Hit Ratio

```
Cache Hit Ratio = (Cache Hits) / (Cache Hits + Cache Misses) × 100%
```

**Example:**
- 900 cache hits, 100 cache misses
- Hit Ratio = 900 / (900 + 100) = 90%

**Target**: 80-95% hit ratio for optimal performance

---

## 4. Types of Caching

### 4.1. In-Memory Caching
- Data stored in application memory (RAM)
- **Fastest** access
- **Examples**: ConcurrentHashMap, Caffeine, Guava Cache
- **Use Case**: Single-instance applications, local cache

### 4.2. Distributed Caching
- Cache shared across multiple application instances
- **Examples**: Redis, Memcached, Hazelcast
- **Use Case**: Microservices, horizontally scaled applications

### 4.3. Database Caching
- Built-in caching in databases
- **Examples**: Query cache, result set cache
- **Use Case**: Automatic, transparent to application

### 4.4. Web/HTTP Caching
- Browser and CDN caching
- **Examples**: Browser cache, CloudFlare, Akamai
- **Use Case**: Static content (images, CSS, JS)

### 4.5. Application-Level Caching
- Custom caching logic in application code
- **Examples**: Spring Cache abstraction
- **Use Case**: Business logic, method results

---

## 5. Cache Levels

### Multi-Level Caching Architecture

```
L1 (Local):    Application Memory (fastest, smallest)
                      ↓
L2 (Distributed): Redis/Memcached (fast, larger)
                      ↓
L3 (Database):   Database query cache
                      ↓
Source:          Database/API (slowest, largest)
```

---

## 6. Cache Eviction Policies

### 6.1. LRU (Least Recently Used)
- Evicts least recently accessed items
- **Best for**: General-purpose caching
- **Use Case**: Most common pattern

### 6.2. LFU (Least Frequently Used)
- Evicts least frequently accessed items
- **Best for**: Long-term patterns
- **Use Case**: Static content, reference data

### 6.3. FIFO (First In, First Out)
- Evicts oldest items first
- **Best for**: Simple queue-like behavior
- **Use Case**: Time-series data

### 6.4. TTL (Time To Live)
- Items expire after set time
- **Best for**: Time-sensitive data
- **Use Case**: Session data, temporary tokens

### 6.5. LRU + TTL (Hybrid)
- Combines LRU with expiration
- **Best for**: Most scenarios
- **Use Case**: Production applications

---

## 7. Cache Invalidation Strategies

### 7.1. Time-Based Invalidation (TTL)
```java
// Cache expires after 5 minutes
@Cacheable(value = "products", key = "#id")
@CacheEvict(allEntries = true, beforeInvocation = false)
public Product getProduct(Long id) {
    return productRepository.findById(id);
}
```

### 7.2. Event-Based Invalidation
```java
@CacheEvict(value = "products", key = "#product.id")
public Product updateProduct(Product product) {
    return productRepository.save(product);
}
```

### 7.3. Manual Invalidation
```java
@Autowired
private CacheManager cacheManager;

public void clearCache() {
    cacheManager.getCache("products").clear();
}
```

### 7.4. Write-Through
- Update both cache and database simultaneously
- **Pros**: Cache always consistent
- **Cons**: Slower writes

### 7.5. Write-Behind (Write-Back)
- Update cache first, database asynchronously
- **Pros**: Faster writes
- **Cons**: Risk of data loss

---

## 8. Cache Patterns

### 8.1. Cache-Aside (Lazy Loading)
```java
public Product getProduct(Long id) {
    // Check cache first
    Product product = cache.get(id);
    
    if (product == null) {
        // Cache miss - load from database
        product = database.find(id);
        
        // Store in cache
        cache.put(id, product);
    }
    
    return product;
}
```

**Pros**: Only cache what's needed
**Cons**: Cache misses are expensive

### 8.2. Read-Through
```java
// Cache automatically loads from database on miss
@Cacheable(value = "products", key = "#id")
public Product getProduct(Long id) {
    return productRepository.findById(id);
}
```

**Pros**: Transparent to application
**Cons**: All reads go through cache

### 8.3. Write-Through
```java
@CachePut(value = "products", key = "#product.id")
public Product updateProduct(Product product) {
    return productRepository.save(product);
}
```

**Pros**: Cache always up-to-date
**Cons**: Write latency

### 8.4. Write-Behind
```java
// Update cache immediately, database asynchronously
public void updateProduct(Product product) {
    cache.put(product.getId(), product);
    asyncQueue.add(() -> database.save(product));
}
```

**Pros**: Fast writes
**Cons**: Complex, risk of data loss

### 8.5. Refresh-Ahead
```java
// Proactively refresh before expiration
@Scheduled(fixedRate = 240000) // Every 4 minutes
public void refreshCache() {
    List<Product> products = productRepository.findPopular();
    products.forEach(p -> cache.put(p.getId(), p));
}
```

**Pros**: Reduced latency for frequently accessed data
**Cons**: Wastes resources on unused data

---

## 9. Cache Key Design

### 9.1. Simple Key
```java
@Cacheable(value = "products", key = "#id")
public Product getProduct(Long id) { }
```

### 9.2. Composite Key
```java
@Cacheable(value = "products", key = "#customerId + '_' + #productId")
public Product getCustomerProduct(Long customerId, Long productId) { }
```

### 9.3. SpEL Expressions
```java
@Cacheable(value = "products", key = "#product.category + '_' + #product.id")
public Product getProduct(Product product) { }
```

### 9.4. Custom Key Generator
```java
@Bean
public KeyGenerator customKeyGenerator() {
    return (target, method, params) -> {
        return method.getName() + "_" + Arrays.toString(params);
    };
}

@Cacheable(value = "products", keyGenerator = "customKeyGenerator")
public Product getProduct(Long id) { }
```

---

## 10. Cache Consistency

### 10.1. Strong Consistency
- Cache and database always in sync
- **Implementation**: Write-through, synchronous updates
- **Use Case**: Financial data, inventory

### 10.2. Eventual Consistency
- Cache may be temporarily stale
- **Implementation**: TTL, async updates
- **Use Case**: Product catalogs, user profiles

### 10.3. Cache Stampede (Thundering Herd)
**Problem**: Multiple requests fetch same data on cache miss

**Solution 1 - Locking:**
```java
public Product getProduct(Long id) {
    synchronized(getLock(id)) {
        Product product = cache.get(id);
        if (product == null) {
            product = database.find(id);
            cache.put(id, product);
        }
        return product;
    }
}
```

**Solution 2 - Probabilistic Early Expiration:**
```java
// Refresh cache slightly before actual expiration
```

---

## 11. Cache Warming

### 11.1. Preloading Cache on Startup
```java
@Component
public class CacheWarmer implements ApplicationRunner {
    
    @Autowired
    private ProductService productService;
    
    @Override
    public void run(ApplicationArguments args) {
        log.info("Warming cache...");
        List<Product> popularProducts = productRepository.findTop100();
        popularProducts.forEach(p -> productService.getProduct(p.getId()));
        log.info("Cache warmed with {} products", popularProducts.size());
    }
}
```

### 11.2. Scheduled Cache Refresh
```java
@Scheduled(cron = "0 0 2 * * ?") // Daily at 2 AM
public void refreshCache() {
    cacheManager.getCacheNames().forEach(cacheName -> {
        cacheManager.getCache(cacheName).clear();
    });
    warmCache();
}
```

---

## 12. Cache Metrics

### Key Metrics to Monitor
- **Hit Ratio**: (Hits / Total Requests) × 100%
- **Miss Ratio**: (Misses / Total Requests) × 100%
- **Eviction Rate**: Items removed per second
- **Memory Usage**: Cache size in bytes/MB
- **Latency**: Cache access time (p50, p95, p99)
- **Throughput**: Requests per second

---

## 13. Common Caching Challenges

### 13.1. Cache Penetration
**Problem**: Requests for non-existent data always miss cache

**Solution**: Cache null/empty results
```java
@Cacheable(value = "products", key = "#id", unless = "#result == null")
public Product getProduct(Long id) {
    return productRepository.findById(id).orElse(null);
}
```

### 13.2. Cache Avalanche
**Problem**: Many cache entries expire simultaneously

**Solution**: Randomize TTL
```java
int ttl = 300 + random.nextInt(60); // 5-6 minutes
```

### 13.3. Hot Key Problem
**Problem**: One key receives too many requests

**Solution**: Local caching, replica caches
```java
@Cacheable(value = "hotProducts", key = "#id")
public Product getHotProduct(Long id) { }
```

---

## 14. When to Use Caching

### ✅ Good Use Cases
- Frequently accessed data (read-heavy)
- Expensive computations
- External API calls
- Database queries
- Session data
- Static content
- Reference data (countries, categories)

### ❌ Avoid Caching
- Rarely accessed data
- Frequently changing data
- Large objects (>1MB)
- User-specific sensitive data (without encryption)
- Real-time data requirements

---

## 15. Best Practices

1. **Set appropriate TTL** - balance freshness and performance
2. **Monitor cache hit ratio** - aim for 80%+
3. **Handle cache misses gracefully** - don't overload database
4. **Use distributed cache** for scaled applications
5. **Implement cache warming** for critical data
6. **Avoid caching everything** - be selective
7. **Design proper cache keys** - avoid collisions
8. **Plan invalidation strategy** - keep data consistent
9. **Monitor memory usage** - prevent OOM
10. **Test cache failures** - ensure graceful degradation

---

## 16. Caching Anti-Patterns

### ❌ Over-Caching
- Caching too much data
- Wastes memory
- Reduces hit ratio

### ❌ Under-Caching
- Not caching enough
- Missing optimization opportunities

### ❌ No Eviction Policy
- Cache grows indefinitely
- Out of memory errors

### ❌ Ignoring Invalidation
- Serving stale data
- Data inconsistency

### ❌ Caching Mutable Objects
- Modifying cached objects
- Unpredictable behavior

---

## 17. Interview Questions

**Q1: What is caching?**
**A:** Storing frequently accessed data in fast-access storage to improve performance and reduce backend load.

**Q2: Cache hit vs cache miss?**
**A:** Hit: Data found in cache (fast). Miss: Data not in cache, fetch from source (slow).

**Q3: What is cache hit ratio?**
**A:** Percentage of requests served from cache: Hits / (Hits + Misses) × 100%.

**Q4: LRU vs LFU?**
**A:** LRU: Evicts least recently used. LFU: Evicts least frequently used.

**Q5: What is TTL?**
**A:** Time-To-Live - duration before cache entry expires and is removed.

**Q6: Cache-aside vs read-through?**
**A:** Cache-aside: App checks cache, loads on miss. Read-through: Cache auto-loads from source.

**Q7: Write-through vs write-behind?**
**A:** Write-through: Update cache and DB synchronously. Write-behind: Update cache first, DB asynchronously.

**Q8: What is cache warming?**
**A:** Preloading cache with frequently accessed data before receiving requests.

**Q9: What is cache stampede?**
**A:** Multiple requests simultaneously fetch same data on cache miss, overloading backend.

**Q10: How to prevent cache stampede?**
**A:** Use locking, probabilistic early expiration, or cache warming.

**Q11: What is cache penetration?**
**A:** Requests for non-existent data always bypass cache, hitting database.

**Q12: What is cache avalanche?**
**A:** Many cache entries expire simultaneously, causing sudden database load.

**Q13: What is distributed caching?**
**A:** Cache shared across multiple application instances (e.g., Redis, Memcached).

**Q14: When to use distributed cache?**
**A:** Microservices, horizontally scaled applications, shared session storage.

**Q15: What is cache eviction?**
**A:** Removing entries from cache to free space, based on eviction policy.

**Q16: Strong vs eventual consistency?**
**A:** Strong: Cache and DB always in sync. Eventual: May be temporarily stale.

**Q17: What is hot key problem?**
**A:** Single cache key receiving too many requests, becoming bottleneck.

**Q18: How to handle cache failures?**
**A:** Circuit breaker, fallback to database, graceful degradation.

**Q19: Should you cache everything?**
**A:** No. Cache frequently accessed, expensive-to-compute, or slow-to-fetch data.

**Q20: What is cache key design?**
**A:** Creating unique identifiers for cached data to avoid collisions and enable efficient lookup.

---

## 18. Summary

Caching is essential for high-performance applications. Understanding cache patterns, eviction policies, invalidation strategies, and common pitfalls ensures effective caching implementation. Monitor metrics, plan for failures, and design appropriate cache keys for optimal results.

---

**Next:** Cache Annotations →
