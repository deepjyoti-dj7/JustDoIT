# System Design Questions

---

## Question 1: Design URL Shortener Service

**Requirements:**
- Shorten long URLs to unique short codes
- Redirect short URLs to original
- Track click statistics
- Handle 100M URLs
- 10K requests/second

**High-Level Design:**

```
┌─────────┐      ┌─────────────┐      ┌──────────┐
│ Client  │─────▶│ API Gateway │─────▶│ Service  │
└─────────┘      └─────────────┘      └──────────┘
                                            │
                        ┌───────────────────┼───────────────┐
                        ▼                   ▼               ▼
                   ┌─────────┐        ┌─────────┐    ┌─────────┐
                   │  Cache  │        │Database │    │Analytics│
                   │ (Redis) │        │(MySQL)  │    │(Kafka)  │
                   └─────────┘        └─────────┘    └─────────┘
```

**API Design:**

```java
// Entities
@Entity
@Table(name = "urls", indexes = {
    @Index(name = "idx_short_code", columnList = "shortCode", unique = true),
    @Index(name = "idx_user_id", columnList = "userId")
})
public class Url {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false, length = 2048)
    private String originalUrl;
    
    @Column(nullable = false, unique = true, length = 10)
    private String shortCode;
    
    private Long userId;
    
    @Column(nullable = false)
    private LocalDateTime createdAt;
    
    private LocalDateTime expiresAt;
    
    private Long clickCount = 0L;
    
    // Getters and setters
}

// Service
@Service
public class UrlShortenerService {
    
    @Autowired
    private UrlRepository urlRepository;
    
    @Autowired
    private RedisTemplate<String, String> redisTemplate;
    
    @Autowired
    private KafkaTemplate<String, ClickEvent> kafkaTemplate;
    
    private static final String BASE62 = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";
    private static final int SHORT_CODE_LENGTH = 7;
    
    @Transactional
    public String shortenUrl(String originalUrl, Long userId) {
        // Check if URL already shortened
        Optional<Url> existing = urlRepository.findByOriginalUrlAndUserId(originalUrl, userId);
        if (existing.isPresent()) {
            return existing.get().getShortCode();
        }
        
        // Generate unique short code
        String shortCode = generateShortCode();
        while (urlRepository.existsByShortCode(shortCode)) {
            shortCode = generateShortCode();
        }
        
        // Save to database
        Url url = new Url();
        url.setOriginalUrl(originalUrl);
        url.setShortCode(shortCode);
        url.setUserId(userId);
        url.setCreatedAt(LocalDateTime.now());
        url.setExpiresAt(LocalDateTime.now().plusYears(1));
        urlRepository.save(url);
        
        // Cache the mapping
        redisTemplate.opsForValue().set(
            "url:" + shortCode, 
            originalUrl, 
            365, 
            TimeUnit.DAYS
        );
        
        return shortCode;
    }
    
    public String getOriginalUrl(String shortCode) {
        // Try cache first
        String cached = redisTemplate.opsForValue().get("url:" + shortCode);
        if (cached != null) {
            recordClick(shortCode);
            return cached;
        }
        
        // Fetch from database
        Url url = urlRepository.findByShortCode(shortCode)
                .orElseThrow(() -> new ResourceNotFoundException("URL not found"));
        
        if (url.getExpiresAt() != null && url.getExpiresAt().isBefore(LocalDateTime.now())) {
            throw new ExpiredUrlException("URL has expired");
        }
        
        // Cache for future requests
        redisTemplate.opsForValue().set(
            "url:" + shortCode, 
            url.getOriginalUrl(), 
            365, 
            TimeUnit.DAYS
        );
        
        recordClick(shortCode);
        return url.getOriginalUrl();
    }
    
    private void recordClick(String shortCode) {
        // Async click tracking
        ClickEvent event = new ClickEvent(shortCode, LocalDateTime.now());
        kafkaTemplate.send("url-clicks", event);
    }
    
    private String generateShortCode() {
        // Using Base62 encoding of timestamp + random
        long timestamp = System.currentTimeMillis();
        int random = ThreadLocalRandom.current().nextInt(1000000);
        long combined = timestamp * 1000000 + random;
        
        return toBase62(combined).substring(0, SHORT_CODE_LENGTH);
    }
    
    private String toBase62(long num) {
        StringBuilder sb = new StringBuilder();
        while (num > 0) {
            sb.append(BASE62.charAt((int) (num % 62)));
            num /= 62;
        }
        return sb.reverse().toString();
    }
}

// Controller
@RestController
@RequestMapping("/api")
public class UrlController {
    
    @Autowired
    private UrlShortenerService service;
    
    @PostMapping("/shorten")
    public ResponseEntity<ShortenResponse> shortenUrl(
            @Valid @RequestBody ShortenRequest request,
            @AuthenticationPrincipal UserDetails user) {
        Long userId = ((CustomUserDetails) user).getId();
        String shortCode = service.shortenUrl(request.getUrl(), userId);
        
        String shortUrl = "https://short.ly/" + shortCode;
        return ResponseEntity.ok(new ShortenResponse(shortUrl, shortCode));
    }
    
    @GetMapping("/{shortCode}")
    public ResponseEntity<Void> redirect(@PathVariable String shortCode) {
        String originalUrl = service.getOriginalUrl(shortCode);
        return ResponseEntity.status(HttpStatus.FOUND)
                .location(URI.create(originalUrl))
                .build();
    }
    
    @GetMapping("/stats/{shortCode}")
    public ResponseEntity<UrlStats> getStats(@PathVariable String shortCode) {
        UrlStats stats = service.getStats(shortCode);
        return ResponseEntity.ok(stats);
    }
}

// Analytics Consumer
@Service
public class ClickAnalyticsConsumer {
    
    @Autowired
    private UrlRepository urlRepository;
    
    private Map<String, AtomicLong> clickBuffer = new ConcurrentHashMap<>();
    
    @KafkaListener(topics = "url-clicks", groupId = "analytics-group")
    public void handleClick(ClickEvent event) {
        clickBuffer.computeIfAbsent(event.getShortCode(), k -> new AtomicLong(0))
                .incrementAndGet();
    }
    
    @Scheduled(fixedRate = 60000)  // Every minute
    @Transactional
    public void flushClickCounts() {
        clickBuffer.forEach((shortCode, count) -> {
            urlRepository.incrementClickCount(shortCode, count.get());
        });
        clickBuffer.clear();
    }
}
```

**Database Schema:**
```sql
CREATE TABLE urls (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    original_url VARCHAR(2048) NOT NULL,
    short_code VARCHAR(10) NOT NULL UNIQUE,
    user_id BIGINT,
    created_at TIMESTAMP NOT NULL,
    expires_at TIMESTAMP,
    click_count BIGINT DEFAULT 0,
    INDEX idx_short_code (short_code),
    INDEX idx_user_id (user_id)
);

CREATE TABLE url_analytics (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    short_code VARCHAR(10) NOT NULL,
    clicked_at TIMESTAMP NOT NULL,
    ip_address VARCHAR(45),
    user_agent TEXT,
    referrer VARCHAR(2048),
    country VARCHAR(2),
    INDEX idx_short_code (short_code),
    INDEX idx_clicked_at (clicked_at)
);
```

**Scaling Considerations:**
1. **Caching:** Redis for hot URLs (80/20 rule)
2. **Database:** Sharding by hash(shortCode)
3. **Rate Limiting:** Per user/IP
4. **CDN:** Distribute redirect service globally
5. **Analytics:** Kafka + separate analytics DB

---

## Question 2: Design E-Commerce Platform

**Requirements:**
- User management
- Product catalog with search
- Shopping cart
- Order processing
- Payment integration
- Inventory management

**Microservices Architecture:**

```
┌──────────────┐
│ API Gateway  │
└──────┬───────┘
       │
       ├─────────┬──────────┬──────────┬──────────┐
       ▼         ▼          ▼          ▼          ▼
  ┌─────────┐ ┌─────────┐ ┌────────┐ ┌─────────┐ ┌──────────┐
  │  User   │ │ Product │ │ Order  │ │ Payment │ │Inventory │
  │ Service │ │ Service │ │Service │ │ Service │ │ Service  │
  └─────────┘ └─────────┘ └────────┘ └─────────┘ └──────────┘
       │         │          │          │          │
       ▼         ▼          ▼          ▼          ▼
  ┌─────────┐ ┌─────────┐ ┌────────┐ ┌─────────┐ ┌──────────┐
  │User DB  │ │Product  │ │Order   │ │Payment  │ │Inventory │
  │         │ │ DB +    │ │  DB    │ │  DB     │ │   DB     │
  │         │ │Elastic  │ │        │ │         │ │          │
  └─────────┘ └─────────┘ └────────┘ └─────────┘ └──────────┘
```

**Order Service (Saga Pattern):**

```java
@Service
public class OrderService {
    
    @Autowired
    private OrderRepository orderRepository;
    
    @Autowired
    private KafkaTemplate<String, OrderEvent> kafkaTemplate;
    
    @Transactional
    public Order createOrder(CreateOrderRequest request) {
        // Create order in PENDING state
        Order order = new Order();
        order.setUserId(request.getUserId());
        order.setItems(request.getItems());
        order.setTotal(calculateTotal(request.getItems()));
        order.setStatus(OrderStatus.PENDING);
        order = orderRepository.save(order);
        
        // Start saga - publish event
        OrderCreatedEvent event = new OrderCreatedEvent(
            order.getId(),
            order.getUserId(),
            order.getItems(),
            order.getTotal()
        );
        kafkaTemplate.send("order-created", event);
        
        return order;
    }
    
    // Listen for inventory reserved
    @KafkaListener(topics = "inventory-reserved")
    @Transactional
    public void handleInventoryReserved(InventoryReservedEvent event) {
        Order order = orderRepository.findById(event.getOrderId()).orElseThrow();
        order.setStatus(OrderStatus.INVENTORY_RESERVED);
        orderRepository.save(order);
        
        // Next step: process payment
        PaymentRequest paymentRequest = new PaymentRequest(
            event.getOrderId(),
            order.getUserId(),
            order.getTotal()
        );
        kafkaTemplate.send("process-payment", paymentRequest);
    }
    
    // Listen for payment completed
    @KafkaListener(topics = "payment-completed")
    @Transactional
    public void handlePaymentCompleted(PaymentCompletedEvent event) {
        Order order = orderRepository.findById(event.getOrderId()).orElseThrow();
        order.setStatus(OrderStatus.CONFIRMED);
        order.setPaymentId(event.getPaymentId());
        orderRepository.save(order);
        
        // Notify user
        kafkaTemplate.send("order-confirmed", new OrderConfirmedEvent(order.getId()));
    }
    
    // Compensating transaction
    @KafkaListener(topics = "payment-failed")
    @Transactional
    public void handlePaymentFailed(PaymentFailedEvent event) {
        Order order = orderRepository.findById(event.getOrderId()).orElseThrow();
        order.setStatus(OrderStatus.CANCELLED);
        orderRepository.save(order);
        
        // Release inventory
        kafkaTemplate.send("release-inventory", new ReleaseInventoryEvent(
            event.getOrderId(),
            order.getItems()
        ));
    }
}

// Inventory Service
@Service
public class InventoryService {
    
    @Autowired
    private InventoryRepository inventoryRepository;
    
    @KafkaListener(topics = "order-created")
    @Transactional
    public void reserveInventory(OrderCreatedEvent event) {
        try {
            for (OrderItem item : event.getItems()) {
                Inventory inventory = inventoryRepository
                    .findByProductIdWithLock(item.getProductId())
                    .orElseThrow();
                
                if (inventory.getAvailable() < item.getQuantity()) {
                    throw new InsufficientInventoryException();
                }
                
                inventory.setAvailable(inventory.getAvailable() - item.getQuantity());
                inventory.setReserved(inventory.getReserved() + item.getQuantity());
                inventoryRepository.save(inventory);
            }
            
            // Success
            kafkaTemplate.send("inventory-reserved", 
                new InventoryReservedEvent(event.getOrderId()));
            
        } catch (Exception e) {
            // Failure - trigger compensation
            kafkaTemplate.send("inventory-reservation-failed",
                new InventoryReservationFailedEvent(event.getOrderId()));
        }
    }
    
    @KafkaListener(topics = "release-inventory")
    @Transactional
    public void releaseInventory(ReleaseInventoryEvent event) {
        for (OrderItem item : event.getItems()) {
            Inventory inventory = inventoryRepository
                .findByProductId(item.getProductId())
                .orElseThrow();
            
            inventory.setAvailable(inventory.getAvailable() + item.getQuantity());
            inventory.setReserved(inventory.getReserved() - item.getQuantity());
            inventoryRepository.save(inventory);
        }
    }
}
```

**Product Search (Elasticsearch):**

```java
@Document(indexName = "products")
public class ProductDocument {
    @Id
    private String id;
    
    @Field(type = FieldType.Text, analyzer = "standard")
    private String name;
    
    @Field(type = FieldType.Text)
    private String description;
    
    @Field(type = FieldType.Keyword)
    private String category;
    
    @Field(type = FieldType.Double)
    private Double price;
    
    @Field(type = FieldType.Integer)
    private Integer rating;
    
    // Getters and setters
}

@Service
public class ProductSearchService {
    
    @Autowired
    private ElasticsearchRestTemplate elasticsearchTemplate;
    
    public Page<ProductDocument> search(String query, Pageable pageable) {
        Criteria criteria = new Criteria("name").contains(query)
            .or("description").contains(query);
        
        Query searchQuery = new CriteriaQuery(criteria).setPageable(pageable);
        
        SearchHits<ProductDocument> searchHits = 
            elasticsearchTemplate.search(searchQuery, ProductDocument.class);
        
        List<ProductDocument> products = searchHits.stream()
            .map(SearchHit::getContent)
            .collect(Collectors.toList());
        
        return new PageImpl<>(products, pageable, searchHits.getTotalHits());
    }
    
    public Page<ProductDocument> searchWithFilters(SearchRequest request, Pageable pageable) {
        BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();
        
        // Text search
        if (request.getQuery() != null) {
            boolQuery.must(QueryBuilders.multiMatchQuery(
                request.getQuery(), "name", "description"
            ));
        }
        
        // Category filter
        if (request.getCategory() != null) {
            boolQuery.filter(QueryBuilders.termQuery("category", request.getCategory()));
        }
        
        // Price range
        if (request.getMinPrice() != null || request.getMaxPrice() != null) {
            RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery("price");
            if (request.getMinPrice() != null) {
                rangeQuery.gte(request.getMinPrice());
            }
            if (request.getMaxPrice() != null) {
                rangeQuery.lte(request.getMaxPrice());
            }
            boolQuery.filter(rangeQuery);
        }
        
        NativeSearchQuery searchQuery = new NativeSearchQueryBuilder()
            .withQuery(boolQuery)
            .withPageable(pageable)
            .build();
        
        SearchHits<ProductDocument> searchHits = 
            elasticsearchTemplate.search(searchQuery, ProductDocument.class);
        
        List<ProductDocument> products = searchHits.stream()
            .map(SearchHit::getContent)
            .collect(Collectors.toList());
        
        return new PageImpl<>(products, pageable, searchHits.getTotalHits());
    }
}
```

---

## Question 3: Design Real-Time Notification System

**Requirements:**
- Push notifications to mobile apps
- Email notifications
- SMS notifications
- In-app notifications
- User preferences
- Delivery tracking

**Architecture:**

```
┌─────────────┐
│  Services   │ (Order, Payment, etc.)
└──────┬──────┘
       │ publish events
       ▼
┌──────────────┐
│    Kafka     │
└──────┬───────┘
       │ consume
       ▼
┌──────────────────┐
│ Notification     │
│ Service          │
└────┬──┬──┬───┬───┘
     │  │  │   │
     ▼  ▼  ▼   ▼
  ┌───┬───┬───┬────┐
  │FCM│Email│SMS│WebSocket│
  └───┴───┴───┴────┘
```

**Implementation:**

```java
// Notification Service
@Service
public class NotificationService {
    
    @Autowired
    private UserPreferenceRepository preferenceRepository;
    
    @Autowired
    private NotificationRepository notificationRepository;
    
    @Autowired
    private List<NotificationChannel> channels;
    
    @KafkaListener(topics = "order-confirmed")
    public void handleOrderConfirmed(OrderConfirmedEvent event) {
        NotificationRequest request = NotificationRequest.builder()
            .userId(event.getUserId())
            .type(NotificationType.ORDER_CONFIRMED)
            .title("Order Confirmed")
            .message("Your order #" + event.getOrderId() + " has been confirmed")
            .data(Map.of("orderId", event.getOrderId()))
            .build();
        
        sendNotification(request);
    }
    
    @Async
    public void sendNotification(NotificationRequest request) {
        // Get user preferences
        UserPreference pref = preferenceRepository
            .findByUserId(request.getUserId())
            .orElse(UserPreference.getDefault());
        
        // Save notification
        Notification notification = new Notification();
        notification.setUserId(request.getUserId());
        notification.setType(request.getType());
        notification.setTitle(request.getTitle());
        notification.setMessage(request.getMessage());
        notification.setCreatedAt(LocalDateTime.now());
        notification.setRead(false);
        notification = notificationRepository.save(notification);
        
        // Send through enabled channels
        for (NotificationChannel channel : channels) {
            if (isChannelEnabled(pref, channel.getType(), request.getType())) {
                try {
                    channel.send(request);
                    recordDelivery(notification.getId(), channel.getType(), true);
                } catch (Exception e) {
                    log.error("Failed to send via " + channel.getType(), e);
                    recordDelivery(notification.getId(), channel.getType(), false);
                }
            }
        }
    }
    
    private boolean isChannelEnabled(UserPreference pref, ChannelType channel, 
                                     NotificationType type) {
        return switch (channel) {
            case PUSH -> pref.isPushEnabled() && pref.getPushTypes().contains(type);
            case EMAIL -> pref.isEmailEnabled() && pref.getEmailTypes().contains(type);
            case SMS -> pref.isSmsEnabled() && pref.getSmsTypes().contains(type);
            case IN_APP -> true;  // Always enabled
        };
    }
}

// FCM Channel
@Component
public class FcmNotificationChannel implements NotificationChannel {
    
    @Autowired
    private FirebaseMessaging firebaseMessaging;
    
    @Autowired
    private DeviceTokenRepository tokenRepository;
    
    @Override
    public ChannelType getType() {
        return ChannelType.PUSH;
    }
    
    @Override
    public void send(NotificationRequest request) {
        List<String> tokens = tokenRepository.findByUserId(request.getUserId());
        
        for (String token : tokens) {
            Message message = Message.builder()
                .setToken(token)
                .setNotification(com.google.firebase.messaging.Notification.builder()
                    .setTitle(request.getTitle())
                    .setBody(request.getMessage())
                    .build())
                .putAllData(request.getData())
                .build();
            
            try {
                String response = firebaseMessaging.send(message);
                log.info("FCM sent: {}", response);
            } catch (FirebaseMessagingException e) {
                if (e.getErrorCode().equals("INVALID_REGISTRATION_TOKEN")) {
                    tokenRepository.deleteByToken(token);
                }
                throw new NotificationException("FCM send failed", e);
            }
        }
    }
}

// WebSocket Channel (In-App)
@Component
public class WebSocketNotificationChannel implements NotificationChannel {
    
    @Autowired
    private SimpMessagingTemplate messagingTemplate;
    
    @Override
    public ChannelType getType() {
        return ChannelType.IN_APP;
    }
    
    @Override
    public void send(NotificationRequest request) {
        messagingTemplate.convertAndSendToUser(
            request.getUserId().toString(),
            "/queue/notifications",
            request
        );
    }
}

// WebSocket Configuration
@Configuration
@EnableWebSocketMessageBroker
public class WebSocketConfig implements WebSocketMessageBrokerConfigurer {
    
    @Override
    public void configureMessageBroker(MessageBrokerRegistry config) {
        config.enableSimpleBroker("/topic", "/queue");
        config.setApplicationDestinationPrefixes("/app");
        config.setUserDestinationPrefix("/user");
    }
    
    @Override
    public void registerStompEndpoints(StompEndpointRegistry registry) {
        registry.addEndpoint("/ws")
                .setAllowedOrigins("*")
                .withSockJS();
    }
}
```

---

## Question 4: Design Rate Limiter

**Requirements:**
- Limit API calls per user
- Different limits for different tiers
- Distributed system
- Low latency

**Algorithms:**

**1. Token Bucket (Implemented):**
```java
@Service
public class RateLimiterService {
    
    @Autowired
    private RedisTemplate<String, String> redisTemplate;
    
    public boolean allowRequest(String userId, RateLimitTier tier) {
        String key = "rate_limit:" + userId;
        long now = System.currentTimeMillis();
        
        // Get bucket state
        String bucketJson = redisTemplate.opsForValue().get(key);
        TokenBucket bucket;
        
        if (bucketJson == null) {
            bucket = new TokenBucket(tier.getCapacity(), tier.getRefillRate());
        } else {
            bucket = deserialize(bucketJson);
        }
        
        // Refill tokens
        bucket.refill(now);
        
        // Try to consume
        if (bucket.tryConsume()) {
            redisTemplate.opsForValue().set(key, serialize(bucket), 1, TimeUnit.HOURS);
            return true;
        }
        
        return false;
    }
}

class TokenBucket {
    private long tokens;
    private long capacity;
    private long refillRate;  // tokens per second
    private long lastRefillTime;
    
    public TokenBucket(long capacity, long refillRate) {
        this.tokens = capacity;
        this.capacity = capacity;
        this.refillRate = refillRate;
        this.lastRefillTime = System.currentTimeMillis();
    }
    
    public void refill(long now) {
        long elapsedTime = (now - lastRefillTime) / 1000;  // seconds
        long tokensToAdd = elapsedTime * refillRate;
        
        tokens = Math.min(capacity, tokens + tokensToAdd);
        lastRefillTime = now;
    }
    
    public boolean tryConsume() {
        if (tokens > 0) {
            tokens--;
            return true;
        }
        return false;
    }
}
```

**2. Sliding Window:**
```java
@Service
public class SlidingWindowRateLimiter {
    
    @Autowired
    private RedisTemplate<String, String> redisTemplate;
    
    public boolean allowRequest(String userId, int maxRequests, int windowSeconds) {
        String key = "sliding_window:" + userId;
        long now = System.currentTimeMillis();
        long windowStart = now - (windowSeconds * 1000);
        
        // Remove old entries
        redisTemplate.opsForZSet().removeRangeByScore(key, 0, windowStart);
        
        // Count requests in window
        Long count = redisTemplate.opsForZSet().count(key, windowStart, now);
        
        if (count < maxRequests) {
            // Add current request
            redisTemplate.opsForZSet().add(key, String.valueOf(now), now);
            redisTemplate.expire(key, windowSeconds, TimeUnit.SECONDS);
            return true;
        }
        
        return false;
    }
}
```

---

## Question 5: Design Distributed Caching System

**Requirements:**
- Cache frequently accessed data
- Automatic eviction
- Cache invalidation
- Distributed across servers
- High availability

**Multi-Level Caching:**

```java
@Configuration
@EnableCaching
public class CacheConfig {
    
    // L1: Local Caffeine cache
    @Bean
    public CaffeineCacheManager localCacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();
        cacheManager.setCaffeine(Caffeine.newBuilder()
            .maximumSize(10000)
            .expireAfterWrite(5, TimeUnit.MINUTES)
            .recordStats());
        return cacheManager;
    }
    
    // L2: Distributed Redis cache
    @Bean
    public RedisCacheManager redisCacheManager(RedisConnectionFactory factory) {
        RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig()
            .entryTtl(Duration.ofMinutes(30))
            .disableCachingNullValues();
        
        return RedisCacheManager.builder(factory)
            .cacheDefaults(config)
            .build();
    }
}

// Multi-level cache service
@Service
public class MultiLevelCacheService {
    
    @Autowired
    @Qualifier("localCacheManager")
    private CacheManager localCacheManager;
    
    @Autowired
    @Qualifier("redisCacheManager")
    private CacheManager redisCacheManager;
    
    @Autowired
    private ProductRepository productRepository;
    
    public Product getProduct(Long id) {
        // L1: Check local cache
        Cache localCache = localCacheManager.getCache("products");
        Product product = localCache.get(id, Product.class);
        if (product != null) {
            return product;
        }
        
        // L2: Check Redis
        Cache redisCache = redisCacheManager.getCache("products");
        product = redisCache.get(id, Product.class);
        if (product != null) {
            localCache.put(id, product);  // Populate L1
            return product;
        }
        
        // L3: Database
        product = productRepository.findById(id).orElseThrow();
        
        // Populate caches
        redisCache.put(id, product);
        localCache.put(id, product);
        
        return product;
    }
    
    @CacheEvict(value = "products", key = "#id", cacheManager = "redisCacheManager")
    public void evictProduct(Long id) {
        // Also evict from local cache
        localCacheManager.getCache("products").evict(id);
        
        // Publish cache invalidation event
        redisTemplate.convertAndSend("cache:invalidate", "products:" + id);
    }
}

// Listen for cache invalidation events
@Component
public class CacheInvalidationListener {
    
    @Autowired
    private CacheManager localCacheManager;
    
    @RedisListener(topics = "cache:invalidate")
    public void handleInvalidation(String message) {
        String[] parts = message.split(":");
        String cacheName = parts[0];
        String key = parts[1];
        
        Cache cache = localCacheManager.getCache(cacheName);
        if (cache != null) {
            cache.evict(key);
        }
    }
}
```

---

## Summary

System design solutions: **URL Shortener** (Base62 encoding for short codes, Redis caching for hot URLs, Kafka for analytics, database sharding by hash, unique index on short_code, click tracking with batched writes, expiration handling), **E-Commerce** (microservices architecture with API Gateway, Saga pattern for distributed transactions, Order/Inventory/Payment services with event-driven communication, Elasticsearch for product search with filters, compensating transactions for failures, database per service), **Notification System** (multi-channel delivery with FCM/Email/SMS/WebSocket, user preferences, Kafka for event consumption, async processing, delivery tracking, Firebase for push notifications, WebSocket for real-time in-app notifications), **Rate Limiter** (Token Bucket algorithm with Redis, configurable tiers, sliding window counter, distributed implementation, refill rates, capacity limits), **Distributed Cache** (multi-level caching with Caffeine L1 + Redis L2, cache-aside pattern, invalidation with pub/sub, TTL configuration, cache warming, eviction policies). Key patterns: Event-driven architecture, CQRS, Saga for distributed transactions, Circuit Breaker, Service Discovery, API Gateway, Database sharding, CAP theorem trade-offs.

---

**Next:** Module README →
