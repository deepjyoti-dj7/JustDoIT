# Centralized Logging - Complete Guide

---

## 1. Introduction to Centralized Logging

Centralized logging aggregates logs from multiple services/instances into a single location for unified searching, monitoring, and analysis.

### Why Centralized Logging?

- **Unified view** - All logs in one place
- **Distributed systems** - Track requests across services
- **Scalability** - Handle logs from multiple instances
- **Search & analysis** - Powerful querying capabilities
- **Alerting** - Real-time notifications
- **Compliance** - Audit trail and retention

### Common Solutions

- **ELK Stack** (Elasticsearch, Logstash, Kibana)
- **Splunk** (Enterprise)
- **AWS CloudWatch Logs**
- **Google Cloud Logging**
- **Datadog Logs**
- **Fluentd/Fluent Bit**

---

## 2. ELK Stack Architecture

### 2.1. Components

```
Spring Boot Apps (Multiple Instances)
         ↓
    Logstash/Filebeat (Log Collection)
         ↓
   Elasticsearch (Storage & Search)
         ↓
     Kibana (Visualization)
```

**Elasticsearch**: Distributed search and analytics engine  
**Logstash**: Log processing pipeline  
**Kibana**: Visualization and dashboards  
**Filebeat**: Lightweight log shipper (alternative to Logstash)

### 2.2. Flow

1. Applications write JSON logs to files/stdout
2. Filebeat/Logstash collects and ships logs
3. Elasticsearch indexes and stores logs
4. Kibana provides search and visualization

---

## 3. Spring Boot Configuration for ELK

### 3.1. JSON Logging Setup

**Dependencies:**
```xml
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version>
</dependency>
```

**logback-spring.xml:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    
    <!-- JSON File Appender for ELK -->
    <appender name="JSON_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/application.json</file>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <!-- Custom fields -->
            <customFields>{"application":"my-service","environment":"${ENVIRONMENT:-dev}"}</customFields>
            
            <!-- Include MDC keys -->
            <includeMdcKeyName>traceId</includeMdcKeyName>
            <includeMdcKeyName>spanId</includeMdcKeyName>
            <includeMdcKeyName>userId</includeMdcKeyName>
            <includeMdcKeyName>requestId</includeMdcKeyName>
        </encoder>
        
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/application-%d{yyyy-MM-dd}.json</fileNamePattern>
            <maxHistory>7</maxHistory>
            <totalSizeCap>1GB</totalSizeCap>
        </rollingPolicy>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="JSON_FILE" />
    </root>
    
</configuration>
```

### 3.2. Application Properties

```yaml
spring:
  application:
    name: product-service

logging:
  level:
    root: INFO
    com.example: DEBUG
  file:
    name: logs/application.json
```

---

## 4. Filebeat Configuration

### 4.1. filebeat.yml

```yaml
# Filebeat configuration for shipping logs to Elasticsearch

filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/application/*.json
    json.keys_under_root: true
    json.add_error_key: true
    fields:
      environment: production
      service: product-service
    fields_under_root: true

# Elasticsearch output
output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "application-logs-%{+yyyy.MM.dd}"
  username: "elastic"
  password: "${ELASTICSEARCH_PASSWORD}"

# Kibana endpoint for dashboards
setup.kibana:
  host: "kibana:5601"

# Index template
setup.template.name: "application-logs"
setup.template.pattern: "application-logs-*"
setup.template.settings:
  index.number_of_shards: 1
  index.number_of_replicas: 1

# Processors
processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
```

### 4.2. Running Filebeat

```bash
# Download and install
curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.11.0-linux-x86_64.tar.gz
tar xzvf filebeat-8.11.0-linux-x86_64.tar.gz
cd filebeat-8.11.0-linux-x86_64/

# Test configuration
./filebeat test config -e

# Run Filebeat
./filebeat -e
```

---

## 5. Logstash Configuration

### 5.1. logstash.conf

```ruby
# Input - Read JSON logs
input {
  file {
    path => "/var/log/application/*.json"
    codec => "json"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

# Filter - Process and enrich logs
filter {
  # Parse timestamp
  date {
    match => ["@timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  # Add geolocation for IP addresses
  if [clientIp] {
    geoip {
      source => "clientIp"
      target => "geoip"
    }
  }
  
  # Parse user agent
  if [userAgent] {
    useragent {
      source => "userAgent"
      target => "user_agent"
    }
  }
  
  # Extract error details
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
    }
  }
  
  # Add hostname
  mutate {
    add_field => {
      "hostname" => "%{host}"
    }
  }
}

# Output - Send to Elasticsearch
output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "application-logs-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "${ELASTICSEARCH_PASSWORD}"
  }
  
  # Also output to console for debugging
  stdout {
    codec => rubydebug
  }
}
```

### 5.2. Running Logstash

```bash
# Install Logstash
wget https://artifacts.elastic.co/downloads/logstash/logstash-8.11.0-linux-x86_64.tar.gz
tar -xzf logstash-8.11.0-linux-x86_64.tar.gz
cd logstash-8.11.0

# Run with config
bin/logstash -f logstash.conf
```

---

## 6. Elasticsearch Configuration

### 6.1. Index Template

```json
PUT _index_template/application-logs
{
  "index_patterns": ["application-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.lifecycle.name": "application-logs-policy"
    },
    "mappings": {
      "properties": {
        "@timestamp": { "type": "date" },
        "level": { "type": "keyword" },
        "message": { "type": "text" },
        "logger_name": { "type": "keyword" },
        "thread_name": { "type": "keyword" },
        "application": { "type": "keyword" },
        "environment": { "type": "keyword" },
        "userId": { "type": "keyword" },
        "requestId": { "type": "keyword" },
        "traceId": { "type": "keyword" },
        "spanId": { "type": "keyword" },
        "duration": { "type": "long" },
        "statusCode": { "type": "integer" },
        "method": { "type": "keyword" },
        "uri": { "type": "keyword" },
        "clientIp": { "type": "ip" }
      }
    }
  }
}
```

### 6.2. Index Lifecycle Policy

```json
PUT _ilm/policy/application-logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_age": "1d",
            "max_size": "50GB"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

---

## 7. Kibana Queries and Dashboards

### 7.1. Common Queries

**Find errors in last hour:**
```
level:"ERROR" AND @timestamp:[now-1h TO now]
```

**Search by user:**
```
userId:"john" AND @timestamp:[now-24h TO now]
```

**Find slow requests:**
```
duration:>5000 AND @timestamp:[now-1h TO now]
```

**Search by trace ID:**
```
traceId:"abc-123-def"
```

**Find 500 errors:**
```
statusCode:500 AND @timestamp:[now-1h TO now]
```

### 7.2. Creating Visualizations

**Error Rate Over Time:**
```json
{
  "query": {
    "bool": {
      "must": [
        { "term": { "level": "ERROR" } },
        { "range": { "@timestamp": { "gte": "now-24h" } } }
      ]
    }
  },
  "aggs": {
    "errors_over_time": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "1h"
      }
    }
  }
}
```

**Response Time Percentiles:**
```json
{
  "aggs": {
    "response_time_percentiles": {
      "percentiles": {
        "field": "duration",
        "percents": [50, 95, 99]
      }
    }
  }
}
```

---

## 8. Docker Compose ELK Stack

### 8.1. docker-compose.yml

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - elk

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    volumes:
      - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ./logs:/var/log/application
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch
    networks:
      - elk

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - elk

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/application:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
      - kibana
    networks:
      - elk

volumes:
  elasticsearch-data:
    driver: local

networks:
  elk:
    driver: bridge
```

### 8.2. Running the Stack

```bash
# Start ELK stack
docker-compose up -d

# Check status
docker-compose ps

# View Kibana
open http://localhost:5601

# View Elasticsearch
curl http://localhost:9200/_cluster/health

# Stop stack
docker-compose down
```

---

## 9. AWS CloudWatch Logs

### 9.1. Dependencies

```xml
<dependency>
    <groupId>ca.pjer</groupId>
    <artifactId>logback-awslogs-appender</artifactId>
    <version>1.6.0</version>
</dependency>
```

### 9.2. Configuration

```xml
<!-- logback-spring.xml -->
<configuration>
    
    <appender name="CLOUDWATCH" class="ca.pjer.logback.AwsLogsAppender">
        <logGroupName>/aws/application/product-service</logGroupName>
        <logStreamName>${HOSTNAME}-${START_TIME}</logStreamName>
        <logRegion>us-east-1</logRegion>
        <layout>
            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
        </layout>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="CLOUDWATCH" />
    </root>
    
</configuration>
```

### 9.3. IAM Permissions

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogStreams"
      ],
      "Resource": "arn:aws:logs:*:*:log-group:/aws/application/*"
    }
  ]
}
```

---

## 10. Splunk Integration

### 10.1. HTTP Event Collector (HEC)

**Dependencies:**
```xml
<dependency>
    <groupId>com.splunk.logging</groupId>
    <artifactId>splunk-library-javalogging</artifactId>
    <version>1.11.5</version>
</dependency>
```

**Configuration:**
```xml
<!-- logback-spring.xml -->
<configuration>
    
    <appender name="SPLUNK" class="com.splunk.logging.HttpEventCollectorLogbackAppender">
        <url>https://splunk.example.com:8088</url>
        <token>YOUR-HEC-TOKEN</token>
        <index>application-logs</index>
        <source>product-service</source>
        <sourcetype>_json</sourcetype>
        <batch_size_count>10</batch_size_count>
        <batch_interval>1000</batch_interval>
        <layout class="com.splunk.logging.HttpEventCollectorEventInfo">
            <pattern>%msg</pattern>
        </layout>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="SPLUNK" />
    </root>
    
</configuration>
```

### 10.2. Splunk Queries

**Search for errors:**
```
index=application-logs level=ERROR | stats count by logger_name
```

**Top users by activity:**
```
index=application-logs | stats count by userId | sort -count | head 10
```

**Response time analysis:**
```
index=application-logs | stats avg(duration), p95(duration), p99(duration)
```

---

## 11. Log Aggregation Best Practices

### 11.1. Structured Logging

```java
import static net.logstash.logback.argument.StructuredArguments.*;

@Service
@Slf4j
public class OrderService {
    
    public Order createOrder(OrderRequest request) {
        log.info("Creating order",
            keyValue("userId", request.getUserId()),
            keyValue("items", request.getItems().size()),
            keyValue("total", request.getTotal())
        );
        
        Order order = processOrder(request);
        
        log.info("Order created successfully",
            keyValue("orderId", order.getId()),
            keyValue("status", order.getStatus())
        );
        
        return order;
    }
}
```

### 11.2. Correlation IDs

```java
@Component
public class CorrelationIdFilter extends OncePerRequestFilter {
    
    @Override
    protected void doFilterInternal(HttpServletRequest request,
                                    HttpServletResponse response,
                                    FilterChain filterChain) 
            throws ServletException, IOException {
        
        String correlationId = request.getHeader("X-Correlation-ID");
        if (correlationId == null) {
            correlationId = UUID.randomUUID().toString();
        }
        
        MDC.put("correlationId", correlationId);
        response.setHeader("X-Correlation-ID", correlationId);
        
        try {
            filterChain.doFilter(request, response);
        } finally {
            MDC.remove("correlationId");
        }
    }
}
```

### 11.3. Log Sampling

```java
@Component
public class SamplingFilter extends Filter<ILoggingEvent> {
    
    private final Random random = new Random();
    private double sampleRate = 0.1; // 10%
    
    @Override
    public FilterReply decide(ILoggingEvent event) {
        if (event.getLevel().isGreaterOrEqual(Level.WARN)) {
            return FilterReply.NEUTRAL; // Always log warnings and errors
        }
        
        if (random.nextDouble() < sampleRate) {
            return FilterReply.NEUTRAL; // Sample 10% of DEBUG/INFO
        }
        
        return FilterReply.DENY;
    }
}
```

---

## 12. Monitoring and Alerting

### 12.1. Kibana Alerts

**Error Rate Alert:**
```json
{
  "trigger": {
    "schedule": {
      "interval": "5m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["application-logs-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                { "term": { "level": "ERROR" } },
                { "range": { "@timestamp": { "gte": "now-5m" } } }
              ]
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.hits.total": {
        "gt": 100
      }
    }
  },
  "actions": {
    "send_email": {
      "email": {
        "to": "ops@example.com",
        "subject": "High Error Rate Detected",
        "body": "Error count exceeded threshold"
      }
    }
  }
}
```

### 12.2. ElastAlert Configuration

```yaml
# elastalert_config.yaml
es_host: elasticsearch
es_port: 9200

rules_folder: rules
run_every:
  minutes: 1

buffer_time:
  minutes: 15

writeback_index: elastalert_status
```

**Error Alert Rule:**
```yaml
# rules/error_alert.yaml
name: High Error Rate
type: frequency
index: application-logs-*
num_events: 100
timeframe:
  minutes: 5

filter:
- term:
    level: "ERROR"

alert:
- email
email:
- ops@example.com

alert_subject: "High Error Rate: {0} errors in 5 minutes"
alert_text: "Application experiencing high error rate"
```

---

## 13. Performance Optimization

### 13.1. Buffered Logging

```xml
<appender name="ASYNC_JSON" class="ch.qos.logback.classic.AsyncAppender">
    <queueSize>512</queueSize>
    <discardingThreshold>0</discardingThreshold>
    <neverBlock>true</neverBlock>
    <appender-ref ref="JSON_FILE" />
</appender>
```

### 13.2. Index Optimization

```json
PUT /application-logs-*/_settings
{
  "index": {
    "refresh_interval": "30s",
    "number_of_replicas": 1
  }
}
```

### 13.3. Log Rotation

```xml
<rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
    <fileNamePattern>logs/application-%d{yyyy-MM-dd}.%i.json</fileNamePattern>
    <maxFileSize>100MB</maxFileSize>
    <maxHistory>30</maxHistory>
    <totalSizeCap>10GB</totalSizeCap>
</rollingPolicy>
```

---

## 14. Interview Questions

**Q1: What is centralized logging?**
**A:** Aggregating logs from multiple services/instances into single location.

**Q2: What is the ELK stack?**
**A:** Elasticsearch (storage), Logstash (processing), Kibana (visualization).

**Q3: Filebeat vs Logstash?**
**A:** Filebeat: Lightweight shipper. Logstash: Full processing pipeline with filters.

**Q4: Why use JSON logging for ELK?**
**A:** Structured format, easier parsing, better querying in Elasticsearch.

**Q5: What is an index template in Elasticsearch?**
**A:** Defines mappings and settings for indices matching a pattern.

**Q6: How to search logs by trace ID?**
**A:** Query: `traceId:"abc-123"` in Kibana/Elasticsearch.

**Q7: What is log retention policy?**
**A:** Automated deletion of old logs based on age/size (ILM policy).

**Q8: How to handle high log volume?**
**A:** Use sampling, async logging, compression, appropriate retention.

**Q9: What is correlation ID?**
**A:** Unique ID tracking request across multiple services.

**Q10: How to alert on errors?**
**A:** Use Kibana Watcher or ElastAlert to trigger on error count threshold.

**Q11: Splunk vs ELK?**
**A:** Splunk: Commercial, easier setup. ELK: Open source, more customizable.

**Q12: What is AWS CloudWatch Logs?**
**A:** AWS managed log aggregation and monitoring service.

**Q13: How to optimize Elasticsearch performance?**
**A:** Proper sharding, refresh interval, index lifecycle management.

**Q14: What is log sampling?**
**A:** Logging only a percentage of events to reduce volume.

**Q15: How to query logs across multiple services?**
**A:** Use correlation ID or trace ID to link related log entries.

**Q16: What is Filebeat module?**
**A:** Pre-configured setup for specific applications (Nginx, MySQL, etc.).

**Q17: How to secure Elasticsearch?**
**A:** Enable authentication, TLS/SSL, network isolation, role-based access.

**Q18: What is index lifecycle management?**
**A:** Automated policy for hot/warm/cold/delete phases.

**Q19: How to debug Logstash pipeline?**
**A:** Use stdout output with rubydebug codec.

**Q20: What metrics to monitor for logging infrastructure?**
**A:** Ingestion rate, index size, query latency, disk usage, error rate.

---

## 15. Summary

Centralized logging aggregates logs from distributed services. ELK stack (Elasticsearch, Logstash, Kibana) is popular open-source solution. Configure Spring Boot with JSON logging using logstash-logback-encoder. Ship logs with Filebeat or Logstash. Create dashboards and alerts in Kibana. Alternatives: Splunk (commercial), CloudWatch (AWS). Use correlation IDs for request tracking. Implement log retention policies. Optimize with async logging and sampling.

---

**Next:** Metrics Collection →
