# Spring Kafka - Complete Guide

---

## 1. Introduction to Apache Kafka

Apache Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant, and scalable messaging. It is widely used for building real-time data pipelines, event-driven architectures, and streaming applications.

### Key Features
- **High Throughput:** Handles millions of messages per second
- **Scalability:** Horizontal scaling with partitions
- **Durability:** Persistent storage with replication
- **Fault Tolerance:** Automatic failover and recovery
- **Real-time Processing:** Stream processing with Kafka Streams
- **Decoupling:** Producers and consumers are independent

---

## 2. Kafka Architecture

### Core Components

```
┌─────────────┐
│  Producer   │
└──────┬──────┘
       │
       ▼
┌─────────────────────────────────┐
│     Kafka Cluster (Brokers)     │
│  ┌─────────┬─────────┬────────┐ │
│  │ Topic A │ Topic B │ Topic C│ │
│  │ P0 P1   │ P0 P1 P2│ P0     │ │
│  └─────────┴─────────┴────────┘ │
└─────────────────────────────────┘
       │
       ▼
┌─────────────┐
│  Consumer   │
│   Group     │
└─────────────┘
```

| Component        | Description                                              |
|------------------|----------------------------------------------------------|
| **Broker**       | Kafka server that stores and serves messages             |
| **Topic**        | Logical channel for messages                             |
| **Partition**    | Subdivision of a topic for parallelism                   |
| **Offset**       | Unique sequential ID of a message within a partition     |
| **Producer**     | Publishes messages to topics                             |
| **Consumer**     | Subscribes to topics and processes messages              |
| **Consumer Group** | Multiple consumers sharing the load                    |
| **ZooKeeper**    | Coordinates Kafka cluster (being replaced by KRaft)      |

---

## 3. Kafka vs Traditional Message Queues

| Feature              | Kafka                          | RabbitMQ/ActiveMQ           |
|----------------------|--------------------------------|-----------------------------|
| **Model**            | Log-based, append-only         | Queue-based                 |
| **Retention**        | Configurable (days/weeks)      | Until consumed              |
| **Replay**           | Yes (by offset)                | No                          |
| **Throughput**       | Very high (millions/sec)       | Moderate                    |
| **Ordering**         | Per partition                  | Per queue                   |
| **Use Case**         | Event streaming, pipelines     | Task queues, RPC            |

---

## 4. Spring Kafka Setup

### 4.1. Dependencies

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>

<!-- For testing -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka-test</artifactId>
    <scope>test</scope>
</dependency>
```

### 4.2. Application Configuration

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    
    # Producer Configuration
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all
      retries: 3
      properties:
        linger.ms: 10
        batch.size: 16384
    
    # Consumer Configuration
    consumer:
      group-id: my-consumer-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: com.example.model
      enable-auto-commit: false
    
    # Listener Configuration
    listener:
      ack-mode: manual
```

---

## 5. Kafka Topics

### 5.1. Topic Basics

- **Partitions:** Enable parallelism and scalability
- **Replication Factor:** Number of copies for fault tolerance
- **Retention:** How long messages are stored

### 5.2. Creating Topics

**Using Kafka CLI:**
```bash
kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic order-events \
  --partitions 3 \
  --replication-factor 2
```

**Programmatically:**
```java
@Configuration
public class KafkaTopicConfig {
    
    @Bean
    public NewTopic orderTopic() {
        return TopicBuilder.name("order-events")
                .partitions(3)
                .replicas(2)
                .config(TopicConfig.RETENTION_MS_CONFIG, "604800000") // 7 days
                .build();
    }
    
    @Bean
    public NewTopic compactedTopic() {
        return TopicBuilder.name("user-profiles")
                .partitions(1)
                .replicas(1)
                .config(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT)
                .build();
    }
}
```

---

## 6. Partitioning Strategy

### 6.1. Default Partitioner
- If key is provided: `hash(key) % numPartitions`
- If no key: Round-robin

### 6.2. Custom Partitioner

```java
public class CustomPartitioner implements Partitioner {
    
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        
        if (key == null) {
            return ThreadLocalRandom.current().nextInt(numPartitions);
        }
        
        // Custom logic
        String keyStr = (String) key;
        if (keyStr.startsWith("VIP")) {
            return 0; // VIP partition
        }
        return Math.abs(keyStr.hashCode()) % numPartitions;
    }
    
    @Override
    public void close() {}
    
    @Override
    public void configure(Map<String, ?> configs) {}
}
```

**Configuration:**
```yaml
spring:
  kafka:
    producer:
      properties:
        partitioner.class: com.example.CustomPartitioner
```

---

## 7. Message Serialization

### 7.1. Built-in Serializers
- `StringSerializer`
- `IntegerSerializer`
- `LongSerializer`
- `ByteArraySerializer`

### 7.2. JSON Serialization

**Producer:**
```yaml
spring:
  kafka:
    producer:
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
```

**Consumer:**
```yaml
spring:
  kafka:
    consumer:
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: com.example.model
```

### 7.3. Avro Serialization

```xml
<dependency>
    <groupId>io.confluent</groupId>
    <artifactId>kafka-avro-serializer</artifactId>
    <version>7.5.0</version>
</dependency>
```

```yaml
spring:
  kafka:
    producer:
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
      properties:
        schema.registry.url: http://localhost:8081
```

---

## 8. Consumer Groups

### 8.1. Concept
- Multiple consumers in a group share the workload
- Each partition is consumed by only one consumer in a group
- Enables horizontal scaling

### 8.2. Partition Assignment

```
Topic: order-events (3 partitions)
Consumer Group: order-processors (2 consumers)

Consumer 1: P0, P1
Consumer 2: P2
```

### 8.3. Rebalancing
- Triggered when consumers join/leave the group
- Partitions are reassigned

---

## 9. Offset Management

### 9.1. Auto Commit

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: true
      auto-commit-interval: 5000
```

### 9.2. Manual Commit

```java
@KafkaListener(topics = "order-events")
public void listen(ConsumerRecord<String, Order> record, Acknowledgment ack) {
    processOrder(record.value());
    ack.acknowledge(); // Commit offset
}
```

### 9.3. Seeking to Specific Offset

```java
@Component
public class OffsetSeeker {
    
    @Autowired
    private KafkaListenerEndpointRegistry registry;
    
    public void seekToBeginning(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        container.pause();
        
        container.getAssignedPartitions().forEach(partition -> {
            ConsumerSeekCallback callback = (consumer, topicPartition) -> 
                consumer.seekToBeginning(Collections.singleton(topicPartition));
        });
        
        container.resume();
    }
}
```

---

## 10. Error Handling

### 10.1. Retry Configuration

```yaml
spring:
  kafka:
    producer:
      retries: 3
      properties:
        retry.backoff.ms: 1000
```

### 10.2. Error Handler

```java
@Configuration
public class KafkaErrorConfig {
    
    @Bean
    public DefaultErrorHandler errorHandler() {
        BackOff fixedBackOff = new FixedBackOff(1000L, 3L);
        DefaultErrorHandler errorHandler = new DefaultErrorHandler(fixedBackOff);
        
        errorHandler.addNotRetryableExceptions(IllegalArgumentException.class);
        
        return errorHandler;
    }
}
```

### 10.3. Dead Letter Topic

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);
    return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}
```

---

## 11. Monitoring & Metrics

### 11.1. Actuator Endpoints

```yaml
management:
  endpoints:
    web:
      exposure:
        include: health,metrics,prometheus
  metrics:
    export:
      prometheus:
        enabled: true
```

### 11.2. Key Metrics
- **Producer:** `kafka.producer.record-send-rate`
- **Consumer:** `kafka.consumer.records-consumed-total`
- **Lag:** `kafka.consumer.lag`

### 11.3. Health Checks

```java
@Component
public class KafkaHealthIndicator implements HealthIndicator {
    
    @Autowired
    private KafkaAdmin kafkaAdmin;
    
    @Override
    public Health health() {
        try {
            kafkaAdmin.describeCluster();
            return Health.up().withDetail("kafka", "Available").build();
        } catch (Exception e) {
            return Health.down().withDetail("kafka", "Unavailable").build();
        }
    }
}
```

---

## 12. Testing

### 12.1. Embedded Kafka

```java
@SpringBootTest
@EmbeddedKafka(partitions = 1, topics = {"test-topic"})
public class KafkaIntegrationTest {
    
    @Autowired
    private KafkaTemplate<String, String> template;
    
    @Autowired
    private EmbeddedKafkaBroker embeddedKafka;
    
    @Test
    public void testSendMessage() {
        template.send("test-topic", "Hello Kafka");
        // Assert message received
    }
}
```

### 12.2. Test Containers

```xml
<dependency>
    <groupId>org.testcontainers</groupId>
    <artifactId>kafka</artifactId>
    <scope>test</scope>
</dependency>
```

```java
@Testcontainers
public class KafkaContainerTest {
    
    @Container
    static KafkaContainer kafka = new KafkaContainer(
        DockerImageName.parse("confluentinc/cp-kafka:7.5.0")
    );
    
    @DynamicPropertySource
    static void kafkaProperties(DynamicPropertyRegistry registry) {
        registry.add("spring.kafka.bootstrap-servers", kafka::getBootstrapServers);
    }
}
```

---

## 13. Performance Tuning

### 13.1. Producer Optimization

```yaml
spring:
  kafka:
    producer:
      batch-size: 32768        # Larger batches
      linger-ms: 20            # Wait for more messages
      compression-type: snappy # Compress messages
      buffer-memory: 33554432  # 32MB buffer
```

### 13.2. Consumer Optimization

```yaml
spring:
  kafka:
    consumer:
      fetch-min-size: 1024           # Minimum fetch size
      fetch-max-wait: 500            # Max wait time
      max-poll-records: 500          # Records per poll
      max-partition-fetch-bytes: 1048576  # 1MB per partition
    listener:
      concurrency: 3  # Multiple threads
```

---

## 14. Security

### 14.1. SSL Configuration

```yaml
spring:
  kafka:
    properties:
      security.protocol: SSL
      ssl.truststore.location: /path/to/truststore.jks
      ssl.truststore.password: password
      ssl.keystore.location: /path/to/keystore.jks
      ssl.keystore.password: password
      ssl.key.password: password
```

### 14.2. SASL Authentication

```yaml
spring:
  kafka:
    properties:
      security.protocol: SASL_SSL
      sasl.mechanism: PLAIN
      sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="user" password="pass";
```

---

## 15. Best Practices

1. **Use appropriate partition count** based on expected throughput
2. **Set proper retention** to balance storage and replay needs
3. **Enable idempotence** for exactly-once semantics
4. **Monitor consumer lag** to detect processing delays
5. **Use compression** to reduce network and storage overhead
6. **Handle errors gracefully** with DLTs and retry logic
7. **Tune batch size and linger** for throughput vs latency
8. **Use consumer groups** for horizontal scaling
9. **Secure with SSL/SASL** in production
10. **Test with embedded Kafka** or containers

---

## 16. Common Patterns

### 16.1. Event Sourcing
Store all state changes as events in Kafka topics.

### 16.2. CQRS
Separate read and write models using Kafka for synchronization.

### 16.3. CDC (Change Data Capture)
Capture database changes and stream to Kafka (Debezium).

### 16.4. Stream Processing
Process events in real-time using Kafka Streams or Apache Flink.

---

## 17. Interview Questions

**Q1: What is Kafka?**
**A:** Distributed event streaming platform for high-throughput, fault-tolerant messaging and real-time data processing.

**Q2: What is a partition?**
**A:** Subdivision of a topic that enables parallelism and scalability. Each partition is an ordered, immutable log.

**Q3: How does Kafka ensure ordering?**
**A:** Messages with the same key go to the same partition, guaranteeing order within that partition.

**Q4: What is a consumer group?**
**A:** Group of consumers sharing the workload. Each partition is consumed by only one consumer in the group.

**Q5: What is offset?**
**A:** Unique sequential ID of a message within a partition. Used to track consumer position.

**Q6: Auto-commit vs manual-commit?**
**A:** Auto: Offsets committed automatically at intervals. Manual: Application controls when to commit.

**Q7: What is rebalancing?**
**A:** Reassigning partitions when consumers join/leave a group. Can cause temporary processing delays.

**Q8: How to achieve exactly-once semantics?**
**A:** Enable idempotent producer + transactional writes + read committed isolation.

**Q9: What is log compaction?**
**A:** Retention policy that keeps only the latest value for each key, useful for changelog topics.

**Q10: Kafka vs RabbitMQ?**
**A:** Kafka: Log-based, high throughput, replay. RabbitMQ: Queue-based, flexible routing, message TTL.

**Q11: What is ISR?**
**A:** In-Sync Replicas - replicas that are fully caught up with the leader.

**Q12: What is acks configuration?**
**A:** Controls producer acknowledgment: 0 (no ack), 1 (leader ack), all (all ISRs ack).

**Q13: How to handle poison messages?**
**A:** Use error handlers with DLTs (Dead Letter Topics) to isolate problematic messages.

**Q14: What is consumer lag?**
**A:** Difference between latest offset and consumer's current offset. Indicates processing delay.

**Q15: Can Kafka guarantee message order across partitions?**
**A:** No. Order is guaranteed only within a partition.

**Q16: What is ZooKeeper's role?**
**A:** Manages cluster metadata, leader election, configuration. Being replaced by KRaft.

**Q17: How to scale consumers?**
**A:** Add more consumers to the group (up to number of partitions).

**Q18: What is retention period?**
**A:** How long messages are kept in Kafka (time-based or size-based).

**Q19: How does Kafka handle failures?**
**A:** Replica failover, automatic leader election, partition reassignment.

**Q20: What is idempotent producer?**
**A:** Producer that guarantees exactly-once delivery per partition by deduplicating retries.

---

## 18. Summary

Spring Kafka provides powerful abstractions for building event-driven applications with Apache Kafka. Understanding topics, partitions, consumer groups, and offset management is crucial for building scalable, reliable messaging systems. Proper configuration, error handling, and monitoring ensure production-ready Kafka applications.

---

**Next:** Producer & Consumer →
