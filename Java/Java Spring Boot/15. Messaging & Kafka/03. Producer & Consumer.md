# Kafka Producer & Consumer - Complete Guide

---

## 1. Introduction

Producers and consumers are the core components of any Kafka application. Producers send messages to Kafka topics, while consumers read and process those messages. Understanding their configuration, lifecycle, and best practices is essential for building robust event-driven systems.

---

## 2. Kafka Producer

### 2.1. Producer Basics

A Kafka producer sends records to topics. Each record consists of:
- **Key:** Optional, used for partitioning
- **Value:** The actual message payload
- **Timestamp:** When the message was created
- **Headers:** Optional metadata

### 2.2. Producer Configuration

**application.yml:**
```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all
      retries: 3
      properties:
        linger.ms: 10
        batch.size: 16384
        compression.type: snappy
        max.in.flight.requests.per.connection: 5
        enable.idempotence: true
```

**Key Properties:**

| Property                                | Description                                    | Default |
|-----------------------------------------|------------------------------------------------|---------|
| `acks`                                  | 0 (no ack), 1 (leader), all (ISR)              | 1       |
| `retries`                               | Number of retry attempts                       | 0       |
| `batch.size`                            | Batch size in bytes                            | 16384   |
| `linger.ms`                             | Wait time to batch messages                    | 0       |
| `compression.type`                      | none, gzip, snappy, lz4, zstd                  | none    |
| `max.in.flight.requests.per.connection` | Concurrent requests per connection             | 5       |
| `enable.idempotence`                    | Exactly-once semantics                         | false   |
| `buffer.memory`                         | Total memory for buffering                     | 32MB    |

---

## 3. Using KafkaTemplate

### 3.1. Basic Send

```java
@Service
public class OrderProducer {
    
    @Autowired
    private KafkaTemplate<String, Order> kafkaTemplate;
    
    public void sendOrder(Order order) {
        kafkaTemplate.send("order-events", order.getId(), order);
    }
}
```

### 3.2. Send with Callback

```java
public void sendOrderWithCallback(Order order) {
    CompletableFuture<SendResult<String, Order>> future = 
        kafkaTemplate.send("order-events", order.getId(), order);
    
    future.whenComplete((result, ex) -> {
        if (ex == null) {
            RecordMetadata metadata = result.getRecordMetadata();
            log.info("Sent message to topic={}, partition={}, offset={}", 
                metadata.topic(), metadata.partition(), metadata.offset());
        } else {
            log.error("Failed to send message", ex);
        }
    });
}
```

### 3.3. Blocking Send

```java
public void sendOrderSync(Order order) {
    try {
        SendResult<String, Order> result = 
            kafkaTemplate.send("order-events", order.getId(), order).get();
        log.info("Message sent successfully: {}", result.getRecordMetadata());
    } catch (InterruptedException | ExecutionException e) {
        log.error("Failed to send message", e);
    }
}
```

### 3.4. Send with Headers

```java
public void sendWithHeaders(Order order) {
    ProducerRecord<String, Order> record = 
        new ProducerRecord<>("order-events", order.getId(), order);
    
    record.headers().add("source", "order-service".getBytes());
    record.headers().add("version", "1.0".getBytes());
    
    kafkaTemplate.send(record);
}
```

### 3.5. Send to Partition

```java
public void sendToPartition(Order order, int partition) {
    kafkaTemplate.send("order-events", partition, order.getId(), order);
}
```

---

## 4. Custom Producer Configuration

```java
@Configuration
public class KafkaProducerConfig {
    
    @Bean
    public ProducerFactory<String, Order> producerFactory() {
        Map<String, Object> config = new HashMap<>();
        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        config.put(ProducerConfig.ACKS_CONFIG, "all");
        config.put(ProducerConfig.RETRIES_CONFIG, 3);
        config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        
        return new DefaultKafkaProducerFactory<>(config);
    }
    
    @Bean
    public KafkaTemplate<String, Order> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```

---

## 5. Transactional Producer

### 5.1. Configuration

```yaml
spring:
  kafka:
    producer:
      transaction-id-prefix: tx-order-
```

```java
@Configuration
@EnableTransactionManagement
public class TransactionalConfig {
    
    @Bean
    public ProducerFactory<String, Order> producerFactory() {
        DefaultKafkaProducerFactory<String, Order> factory = 
            new DefaultKafkaProducerFactory<>(producerConfigs());
        factory.setTransactionIdPrefix("tx-order-");
        return factory;
    }
}
```

### 5.2. Using Transactions

```java
@Service
public class OrderService {
    
    @Autowired
    private KafkaTemplate<String, Order> kafkaTemplate;
    
    @Transactional
    public void processOrder(Order order) {
        kafkaTemplate.send("order-events", order);
        kafkaTemplate.send("audit-events", createAudit(order));
        // Both messages committed atomically
    }
    
    public void manualTransaction(Order order) {
        kafkaTemplate.executeInTransaction(template -> {
            template.send("order-events", order);
            template.send("audit-events", createAudit(order));
            return true;
        });
    }
}
```

---

## 6. Kafka Consumer

### 6.1. Consumer Basics

Consumers subscribe to topics and process messages. They can work individually or as part of a consumer group for load balancing.

### 6.2. Consumer Configuration

```yaml
spring:
  kafka:
    consumer:
      bootstrap-servers: localhost:9092
      group-id: order-consumer-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: com.example.model
      enable-auto-commit: false
      max-poll-records: 500
      fetch-min-size: 1024
      fetch-max-wait: 500
    listener:
      ack-mode: manual
      concurrency: 3
```

**Key Properties:**

| Property                | Description                                | Default       |
|-------------------------|--------------------------------------------|---------------|
| `group-id`              | Consumer group identifier                  | -             |
| `auto-offset-reset`     | earliest, latest, none                     | latest        |
| `enable-auto-commit`    | Auto-commit offsets                        | true          |
| `auto-commit-interval`  | Auto-commit interval (ms)                  | 5000          |
| `max-poll-records`      | Max records per poll                       | 500           |
| `fetch-min-size`        | Minimum fetch size                         | 1             |
| `fetch-max-wait`        | Max wait time for fetch                    | 500           |
| `session-timeout`       | Consumer heartbeat timeout                 | 10000         |
| `max-poll-interval`     | Max time between polls                     | 300000        |

---

## 7. Basic Consumer

### 7.1. Simple Listener

```java
@Component
public class OrderConsumer {
    
    @KafkaListener(topics = "order-events", groupId = "order-group")
    public void consume(Order order) {
        log.info("Received order: {}", order);
        processOrder(order);
    }
}
```

### 7.2. Consumer with Record Metadata

```java
@KafkaListener(topics = "order-events")
public void consume(ConsumerRecord<String, Order> record) {
    log.info("Topic: {}, Partition: {}, Offset: {}, Key: {}", 
        record.topic(), record.partition(), record.offset(), record.key());
    log.info("Order: {}", record.value());
}
```

### 7.3. Consumer with Headers

```java
@KafkaListener(topics = "order-events")
public void consume(
    @Payload Order order,
    @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
    @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
    @Header(KafkaHeaders.OFFSET) long offset,
    @Header(value = "source", required = false) String source
) {
    log.info("Received from topic: {}, partition: {}, offset: {}", topic, partition, offset);
    log.info("Source: {}, Order: {}", source, order);
}
```

### 7.4. Batch Listener

```java
@KafkaListener(topics = "order-events")
public void consumeBatch(List<Order> orders) {
    log.info("Received batch of {} orders", orders.size());
    orders.forEach(this::processOrder);
}
```

```java
@KafkaListener(topics = "order-events")
public void consumeBatchWithMetadata(List<ConsumerRecord<String, Order>> records) {
    records.forEach(record -> {
        log.info("Processing: {}", record.value());
    });
}
```

---

## 8. Manual Offset Commit

### 8.1. Configuration

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: false
    listener:
      ack-mode: manual
```

### 8.2. Manual Acknowledgment

```java
@KafkaListener(topics = "order-events")
public void consume(ConsumerRecord<String, Order> record, Acknowledgment ack) {
    try {
        processOrder(record.value());
        ack.acknowledge(); // Commit offset
    } catch (Exception e) {
        log.error("Failed to process order", e);
        // Don't acknowledge - message will be reprocessed
    }
}
```

### 8.3. Batch Acknowledgment

```java
@KafkaListener(topics = "order-events")
public void consumeBatch(
    List<ConsumerRecord<String, Order>> records, 
    Acknowledgment ack
) {
    try {
        records.forEach(record -> processOrder(record.value()));
        ack.acknowledge(); // Commit entire batch
    } catch (Exception e) {
        log.error("Batch processing failed", e);
    }
}
```

---

## 9. Consumer Groups

### 9.1. Multiple Consumers in Group

```java
// Consumer 1
@KafkaListener(topics = "order-events", groupId = "order-group", 
               concurrency = "3")
public void consumer1(Order order) {
    log.info("Consumer 1: {}", order);
}

// Consumer 2 (different instance)
@KafkaListener(topics = "order-events", groupId = "order-group")
public void consumer2(Order order) {
    log.info("Consumer 2: {}", order);
}
```

### 9.2. Dynamic Concurrency

```yaml
spring:
  kafka:
    listener:
      concurrency: 5  # 5 consumer threads
```

---

## 10. Multiple Topics & Patterns

### 10.1. Multiple Topics

```java
@KafkaListener(topics = {"order-events", "payment-events"})
public void consumeMultiple(ConsumerRecord<String, Object> record) {
    if (record.topic().equals("order-events")) {
        processOrder((Order) record.value());
    } else if (record.topic().equals("payment-events")) {
        processPayment((Payment) record.value());
    }
}
```

### 10.2. Topic Pattern

```java
@KafkaListener(topicPattern = ".*-events")
public void consumePattern(ConsumerRecord<String, String> record) {
    log.info("Consumed from {}: {}", record.topic(), record.value());
}
```

---

## 11. Partition Assignment

### 11.1. Specific Partitions

```java
@KafkaListener(
    topicPartitions = @TopicPartition(
        topic = "order-events",
        partitions = {"0", "1"}
    )
)
public void consumePartitions(Order order) {
    log.info("Consumed from partition 0 or 1: {}", order);
}
```

### 11.2. Partition with Offset

```java
@KafkaListener(
    topicPartitions = @TopicPartition(
        topic = "order-events",
        partitionOffsets = {
            @PartitionOffset(partition = "0", initialOffset = "0"),
            @PartitionOffset(partition = "1", initialOffset = "100")
        }
    )
)
public void consumeFromOffset(Order order) {
    log.info("Consumed: {}", order);
}
```

---

## 12. Custom Consumer Configuration

```java
@Configuration
public class KafkaConsumerConfig {
    
    @Bean
    public ConsumerFactory<String, Order> consumerFactory() {
        Map<String, Object> config = new HashMap<>();
        config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        config.put(ConsumerConfig.GROUP_ID_CONFIG, "order-group");
        config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        config.put(JsonDeserializer.TRUSTED_PACKAGES, "com.example.model");
        
        return new DefaultKafkaConsumerFactory<>(config);
    }
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Order> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        factory.setConcurrency(3);
        factory.getContainerProperties().setAckMode(AckMode.MANUAL);
        return factory;
    }
}
```

---

## 13. Consumer Lifecycle

### 13.1. Lifecycle Events

```java
@Component
public class ConsumerLifecycle {
    
    @EventListener
    public void onPartitionsAssigned(ConsumerPartitionsAssignedEvent event) {
        log.info("Partitions assigned: {}", event.getPartitions());
    }
    
    @EventListener
    public void onPartitionsRevoked(ConsumerPartitionsRevokedEvent event) {
        log.info("Partitions revoked: {}", event.getPartitions());
    }
}
```

### 13.2. Pause and Resume

```java
@Component
public class ConsumerController {
    
    @Autowired
    private KafkaListenerEndpointRegistry registry;
    
    public void pauseConsumer(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        container.pause();
    }
    
    public void resumeConsumer(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        container.resume();
    }
}
```

---

## 14. Seeking & Replaying

### 14.1. Seek to Beginning

```java
@KafkaListener(id = "order-listener", topics = "order-events")
public void consume(Order order) {
    log.info("Order: {}", order);
}

public void replayFromBeginning() {
    MessageListenerContainer container = registry.getListenerContainer("order-listener");
    container.pause();
    
    ConsumerSeekCallback callback = (consumer, topicPartition) -> 
        consumer.seekToBeginning(Collections.singleton(topicPartition));
    
    container.getContainerProperties().setConsumerStartTimestamp(callback);
    container.resume();
}
```

### 14.2. Seek to Timestamp

```java
public void seekToTimestamp(long timestamp) {
    ConsumerSeekCallback callback = (consumer, topicPartition) -> {
        Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();
        timestampsToSearch.put(topicPartition, timestamp);
        
        Map<TopicPartition, OffsetAndTimestamp> offsets = 
            consumer.offsetsForTimes(timestampsToSearch);
        
        if (offsets.get(topicPartition) != null) {
            consumer.seek(topicPartition, offsets.get(topicPartition).offset());
        }
    };
}
```

---

## 15. Filtering Messages

```java
@Bean
public ConcurrentKafkaListenerContainerFactory<String, Order> filterFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
        new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    factory.setRecordFilterStrategy(record -> {
        Order order = record.value();
        return order.getStatus().equals("CANCELLED"); // Filter out cancelled
    });
    return factory;
}

@KafkaListener(topics = "order-events", containerFactory = "filterFactory")
public void consumeFiltered(Order order) {
    log.info("Active order: {}", order);
}
```

---

## 16. Best Practices

### Producer Best Practices
1. **Enable idempotence** for exactly-once semantics
2. **Use appropriate acks** (all for critical data)
3. **Configure retries** for transient failures
4. **Batch messages** with linger.ms for throughput
5. **Compress messages** (snappy, lz4) to reduce size
6. **Use async send** for high throughput
7. **Handle callbacks** for error detection
8. **Set proper timeouts** to avoid blocking

### Consumer Best Practices
1. **Process idempotently** (handle duplicates)
2. **Use manual commit** for control
3. **Configure appropriate poll size** and timeout
4. **Handle rebalancing** gracefully
5. **Monitor consumer lag** for delays
6. **Use batch processing** for efficiency
7. **Implement error handling** with DLT
8. **Scale with concurrency** or multiple instances

---

## 17. Interview Questions

**Q1: What is the difference between synchronous and asynchronous send?**
**A:** Sync waits for acknowledgment (blocking). Async returns immediately with a Future/callback.

**Q2: What does acks=all mean?**
**A:** Producer waits for acknowledgment from all in-sync replicas before considering the send successful.

**Q3: How does idempotent producer work?**
**A:** Producer assigns sequence numbers to messages. Kafka deduplicates based on producer ID and sequence.

**Q4: What is auto-offset-reset?**
**A:** Defines consumer behavior when no offset exists: earliest (from beginning), latest (new messages), none (throw exception).

**Q5: Manual commit vs auto-commit?**
**A:** Manual: Application controls commit timing (after processing). Auto: Commits at fixed intervals.

**Q6: How to ensure message ordering?**
**A:** Use the same key for related messages. They go to the same partition, maintaining order.

**Q7: What happens during consumer rebalancing?**
**A:** Partitions are reassigned among consumers. Processing pauses briefly during rebalancing.

**Q8: How to handle slow consumers?**
**A:** Increase max.poll.interval.ms, reduce max.poll.records, or scale with more consumers.

**Q9: What is consumer lag?**
**A:** Difference between latest offset and consumer's current offset. High lag indicates processing delays.

**Q10: How to replay messages?**
**A:** Seek to specific offset, timestamp, or beginning using ConsumerSeekCallback.

**Q11: What is batch processing?**
**A:** Consuming multiple messages at once for efficiency. Configure with batch listener mode.

**Q12: How to filter messages?**
**A:** Use RecordFilterStrategy in listener container factory.

**Q13: What is linger.ms?**
**A:** Time producer waits to batch more messages. Trade-off between latency and throughput.

**Q14: How to send to specific partition?**
**A:** Use kafkaTemplate.send(topic, partition, key, value) or custom partitioner.

**Q15: What is session.timeout.ms?**
**A:** Max time consumer can be inactive before being considered dead and triggering rebalance.

**Q16: How to handle poison messages?**
**A:** Use error handlers with Dead Letter Topics (DLT) to isolate problematic messages.

**Q17: What is the role of consumer group?**
**A:** Load balancing - partitions distributed among consumers in the group.

**Q18: How many consumers per partition?**
**A:** Only one consumer per partition within a group. More consumers than partitions = idle consumers.

**Q19: What is batch.size?**
**A:** Producer batch size in bytes. Larger batches improve throughput but increase latency.

**Q20: How to implement exactly-once?**
**A:** Enable idempotent producer + transactional writes + read_committed isolation on consumer.

---

## 18. Summary

Mastering Kafka producers and consumers requires understanding configuration, acknowledgment modes, offset management, and best practices. Proper tuning of batch sizes, commit strategies, and error handling ensures reliable, high-performance messaging in production environments.

---

**Next:** Kafka Templates â†’
