# Kafka Error Handling - Complete Guide

---

## 1. Introduction to Error Handling

Error handling in Kafka is crucial for building resilient messaging applications. Errors can occur at various stages: serialization, network issues, broker unavailability, processing failures, and more. Proper error handling ensures messages are not lost and failures are manageable.

### Types of Errors
- **Producer errors**: Send failures, serialization errors
- **Consumer errors**: Deserialization errors, processing exceptions
- **Infrastructure errors**: Network failures, broker unavailability
- **Application errors**: Business logic failures, validation errors

---

## 2. Producer Error Handling

### 2.1. Synchronous Error Handling

```java
public void sendSync(Order order) {
    try {
        SendResult<String, Order> result = 
            kafkaTemplate.send("orders", order).get(5, TimeUnit.SECONDS);
        log.info("Message sent: offset={}", result.getRecordMetadata().offset());
    } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
        log.error("Send interrupted", e);
    } catch (ExecutionException e) {
        log.error("Send failed", e.getCause());
        handleSendFailure(order, e.getCause());
    } catch (TimeoutException e) {
        log.error("Send timeout", e);
        handleTimeout(order);
    }
}
```

### 2.2. Asynchronous Error Handling

```java
public void sendAsync(Order order) {
    CompletableFuture<SendResult<String, Order>> future = 
        kafkaTemplate.send("orders", order.getId(), order);
    
    future.whenComplete((result, ex) -> {
        if (ex == null) {
            log.info("Sent successfully: offset={}", result.getRecordMetadata().offset());
        } else {
            log.error("Send failed for order: {}", order.getId(), ex);
            
            if (ex.getCause() instanceof TimeoutException) {
                handleTimeout(order);
            } else if (ex.getCause() instanceof SerializationException) {
                handleSerializationError(order);
            } else {
                handleGenericError(order, ex);
            }
        }
    });
}
```

### 2.3. Producer Callback

```java
@Bean
public KafkaTemplate<String, Order> kafkaTemplate() {
    KafkaTemplate<String, Order> template = new KafkaTemplate<>(producerFactory());
    
    template.setProducerListener(new ProducerListener<String, Order>() {
        @Override
        public void onSuccess(ProducerRecord<String, Order> record, RecordMetadata metadata) {
            log.info("Message sent: topic={}, partition={}, offset={}", 
                metadata.topic(), metadata.partition(), metadata.offset());
        }
        
        @Override
        public void onError(ProducerRecord<String, Order> record, 
                           RecordMetadata metadata, Exception exception) {
            log.error("Failed to send: {}", record.value(), exception);
            storeFailedMessage(record, exception);
        }
    });
    
    return template;
}
```

### 2.4. Retry Configuration

```yaml
spring:
  kafka:
    producer:
      retries: 3
      retry-backoff-ms: 1000
      properties:
        max.in.flight.requests.per.connection: 5
        enable.idempotence: true
```

```java
@Bean
public ProducerFactory<String, Order> producerFactory() {
    Map<String, Object> config = new HashMap<>();
    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    config.put(ProducerConfig.RETRIES_CONFIG, 3);
    config.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 1000);
    config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
    config.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
    
    return new DefaultKafkaProducerFactory<>(config);
}
```

---

## 3. Consumer Error Handling

### 3.1. DefaultErrorHandler

```java
@Bean
public DefaultErrorHandler errorHandler() {
    // Fixed backoff: 1 second interval, 3 retries
    BackOff fixedBackOff = new FixedBackOff(1000L, 3L);
    DefaultErrorHandler handler = new DefaultErrorHandler(fixedBackOff);
    
    // Don't retry for these exceptions
    handler.addNotRetryableExceptions(
        IllegalArgumentException.class,
        NullPointerException.class,
        ValidationException.class
    );
    
    // Retry for these exceptions
    handler.addRetryableExceptions(
        NetworkException.class,
        TimeoutException.class
    );
    
    return handler;
}

@Bean
public ConcurrentKafkaListenerContainerFactory<String, Order> kafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
        new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    factory.setCommonErrorHandler(errorHandler());
    return factory;
}
```

### 3.2. Exponential Backoff

```java
@Bean
public DefaultErrorHandler errorHandler() {
    ExponentialBackOff backOff = new ExponentialBackOff(
        1000L,  // Initial interval: 1 second
        2.0     // Multiplier: doubles each retry
    );
    backOff.setMaxInterval(10000L);     // Max 10 seconds between retries
    backOff.setMaxElapsedTime(60000L);  // Max 1 minute total retry time
    
    return new DefaultErrorHandler(backOff);
}
```

### 3.3. Custom Backoff

```java
@Bean
public DefaultErrorHandler errorHandler() {
    BackOff customBackOff = new BackOff() {
        @Override
        public BackOffExecution start() {
            return new BackOffExecution() {
                private int attempts = 0;
                
                @Override
                public long nextBackOff() {
                    if (attempts++ >= 5) {
                        return BackOffExecution.STOP;
                    }
                    // Custom logic: Fibonacci sequence
                    return fibonacci(attempts) * 1000L;
                }
            };
        }
    };
    
    return new DefaultErrorHandler(customBackOff);
}
```

---

## 4. Dead Letter Topic (DLT)

### 4.1. Basic DLT Configuration

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);
    return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}

@KafkaListener(topics = "order-events.DLT", groupId = "dlt-group")
public void listenDLT(ConsumerRecord<String, Order> record) {
    log.error("Message in DLT: topic={}, partition={}, offset={}, value={}", 
        record.topic(), record.partition(), record.offset(), record.value());
    
    // Manual intervention, alerting, or special processing
    notifyAdministrator(record);
}
```

### 4.2. Custom DLT Naming

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
        template,
        (record, ex) -> {
            // Custom DLT naming strategy
            String dltTopic = record.topic() + ".DLT";
            return new TopicPartition(dltTopic, record.partition());
        }
    );
    
    return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}
```

### 4.3. DLT with Exception Headers

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template,
        (record, ex) -> new TopicPartition(record.topic() + ".DLT", record.partition())
    );
    
    // Add exception information to headers
    recoverer.setHeadersFunction((consumerRecord, exception) -> {
        Headers headers = new RecordHeaders();
        headers.add("dlt-exception", exception.getClass().getName().getBytes());
        headers.add("dlt-exception-message", exception.getMessage().getBytes());
        headers.add("dlt-original-topic", consumerRecord.topic().getBytes());
        headers.add("dlt-original-partition", 
            String.valueOf(consumerRecord.partition()).getBytes());
        headers.add("dlt-original-offset", 
            String.valueOf(consumerRecord.offset()).getBytes());
        headers.add("dlt-timestamp", 
            String.valueOf(System.currentTimeMillis()).getBytes());
        return headers;
    });
    
    return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}
```

---

## 5. Retry Topics

### 5.1. Non-Blocking Retries

```java
@Bean
public DefaultErrorHandler errorHandler(
    KafkaTemplate<String, Object> template,
    DestinationTopicResolver destinationTopicResolver
) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
        template, destinationTopicResolver
    );
    
    return new DefaultErrorHandler(recoverer, new FixedBackOff(0L, 0L));
}

@Bean
public DestinationTopicResolver destinationTopicResolver() {
    return (record, ex) -> {
        if (ex instanceof RetryableException) {
            // Send to retry topic
            return new TopicPartition(record.topic() + ".retry", 0);
        } else {
            // Send to DLT
            return new TopicPartition(record.topic() + ".DLT", 0);
        }
    };
}

@KafkaListener(topics = "order-events.retry")
public void retryListener(Order order) {
    // Retry processing
    processOrder(order);
}
```

### 5.2. Multiple Retry Levels

```java
@Bean
public DestinationTopicResolver multiLevelRetryResolver() {
    return (record, ex) -> {
        String originalTopic = record.topic();
        
        if (originalTopic.endsWith(".retry1")) {
            return new TopicPartition(originalTopic.replace(".retry1", ".retry2"), 0);
        } else if (originalTopic.endsWith(".retry2")) {
            return new TopicPartition(originalTopic.replace(".retry2", ".retry3"), 0);
        } else if (originalTopic.endsWith(".retry3")) {
            return new TopicPartition(originalTopic + ".DLT", 0);
        } else {
            return new TopicPartition(originalTopic + ".retry1", 0);
        }
    };
}

@KafkaListener(topics = "orders")
public void mainListener(Order order) { processOrder(order); }

@KafkaListener(topics = "orders.retry1")
public void retry1Listener(Order order) throws InterruptedException {
    Thread.sleep(5000); // 5 second delay
    processOrder(order);
}

@KafkaListener(topics = "orders.retry2")
public void retry2Listener(Order order) throws InterruptedException {
    Thread.sleep(15000); // 15 second delay
    processOrder(order);
}

@KafkaListener(topics = "orders.retry3")
public void retry3Listener(Order order) throws InterruptedException {
    Thread.sleep(30000); // 30 second delay
    processOrder(order);
}
```

---

## 6. Deserialization Errors

### 6.1. Error Handling Deserializer

```java
@Bean
public ConsumerFactory<String, Order> consumerFactory() {
    Map<String, Object> config = new HashMap<>();
    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);
    config.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class);
    config.put(JsonDeserializer.TRUSTED_PACKAGES, "com.example.model");
    
    return new DefaultKafkaConsumerFactory<>(config);
}
```

### 6.2. Handle Deserialization Errors

```java
@KafkaListener(topics = "orders")
public void listen(ConsumerRecord<String, Order> record) {
    if (record.value() == null) {
        // Deserialization failed
        DeserializationException ex = (DeserializationException) 
            record.headers().lastHeader(ErrorHandlingDeserializer.VALUE_DESERIALIZER_EXCEPTION_HEADER);
        
        log.error("Deserialization error: {}", new String(ex.getData()));
        handleDeserializationError(record);
        return;
    }
    
    processOrder(record.value());
}
```

### 6.3. Custom Deserializer with Fallback

```java
public class SafeJsonDeserializer<T> implements Deserializer<T> {
    
    private final JsonDeserializer<T> delegate = new JsonDeserializer<>();
    
    @Override
    public T deserialize(String topic, byte[] data) {
        try {
            return delegate.deserialize(topic, data);
        } catch (Exception e) {
            log.error("Deserialization failed, returning null", e);
            return null;
        }
    }
    
    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {
        delegate.configure(configs, isKey);
    }
    
    @Override
    public void close() {
        delegate.close();
    }
}
```

---

## 7. Exception-Based Routing

### 7.1. Route by Exception Type

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
        template,
        (record, ex) -> {
            if (ex.getCause() instanceof ValidationException) {
                return new TopicPartition(record.topic() + ".validation-errors", 0);
            } else if (ex.getCause() instanceof BusinessException) {
                return new TopicPartition(record.topic() + ".business-errors", 0);
            } else {
                return new TopicPartition(record.topic() + ".DLT", 0);
            }
        }
    );
    
    return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}

@KafkaListener(topics = "orders.validation-errors")
public void handleValidationErrors(Order order) {
    log.warn("Validation error for order: {}", order);
    notifyCustomerService(order);
}

@KafkaListener(topics = "orders.business-errors")
public void handleBusinessErrors(Order order) {
    log.warn("Business error for order: {}", order);
    notifyBusinessTeam(order);
}
```

---

## 8. Circuit Breaker Pattern

### 8.1. Resilience4j Integration

```xml
<dependency>
    <groupId>io.github.resilience4j</groupId>
    <artifactId>resilience4j-spring-boot2</artifactId>
</dependency>
```

```yaml
resilience4j:
  circuitbreaker:
    instances:
      orderService:
        failure-rate-threshold: 50
        wait-duration-in-open-state: 10000
        sliding-window-size: 10
```

```java
@Service
public class OrderProcessingService {
    
    @CircuitBreaker(name = "orderService", fallbackMethod = "fallback")
    public void processOrder(Order order) {
        // External service call
        externalOrderService.process(order);
    }
    
    public void fallback(Order order, Exception ex) {
        log.error("Circuit breaker activated, sending to DLT", ex);
        kafkaTemplate.send("orders.circuit-breaker-errors", order);
    }
}
```

---

## 9. Monitoring and Alerting

### 9.1. Error Metrics

```java
@Component
public class ErrorMetrics {
    
    private final Counter errorCounter;
    private final Counter dltCounter;
    
    public ErrorMetrics(MeterRegistry registry) {
        this.errorCounter = Counter.builder("kafka.consumer.errors")
            .description("Total consumer errors")
            .register(registry);
        
        this.dltCounter = Counter.builder("kafka.dlt.messages")
            .description("Messages sent to DLT")
            .register(registry);
    }
    
    public void recordError(String topic, Exception ex) {
        errorCounter.increment();
        log.error("Error processing message from {}", topic, ex);
    }
    
    public void recordDLT(String topic) {
        dltCounter.increment();
        log.warn("Message sent to DLT from {}", topic);
    }
}
```

### 9.2. Custom Error Handler with Metrics

```java
@Bean
public DefaultErrorHandler errorHandler(
    KafkaTemplate<String, Object> template,
    ErrorMetrics metrics
) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
        template,
        (record, ex) -> {
            metrics.recordDLT(record.topic());
            return new TopicPartition(record.topic() + ".DLT", record.partition());
        }
    );
    
    DefaultErrorHandler handler = new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
    
    handler.setRetryListeners((record, ex, deliveryAttempt) -> {
        metrics.recordError(record.topic(), ex);
        log.warn("Retry attempt {} for record from {}", deliveryAttempt, record.topic());
    });
    
    return handler;
}
```

---

## 10. Poison Message Handling

### 10.1. Skip and Log

```java
@Bean
public DefaultErrorHandler errorHandler() {
    DefaultErrorHandler handler = new DefaultErrorHandler((record, ex) -> {
        log.error("Poison message detected: topic={}, partition={}, offset={}, value={}", 
            record.topic(), record.partition(), record.offset(), record.value(), ex);
        
        // Store for analysis
        storePoisonMessage(record, ex);
    }, new FixedBackOff(0L, 0L)); // No retries
    
    return handler;
}
```

### 10.2. Quarantine Topic

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
        template,
        (record, ex) -> {
            // Always send poison messages to quarantine
            return new TopicPartition("quarantine-topic", 0);
        }
    );
    
    return new DefaultErrorHandler(recoverer, new FixedBackOff(0L, 0L));
}

@KafkaListener(topics = "quarantine-topic")
public void handleQuarantine(ConsumerRecord<String, byte[]> record) {
    log.error("Quarantined message: {}", new String(record.value()));
    alertAdministrator(record);
}
```

---

## 11. Transaction Error Handling

### 11.1. Rollback on Error

```java
@Service
public class TransactionalOrderService {
    
    @Autowired
    private KafkaTemplate<String, Order> kafkaTemplate;
    
    @Autowired
    private OrderRepository orderRepository;
    
    @Transactional("kafkaTransactionManager")
    public void processOrder(Order order) {
        try {
            orderRepository.save(order);
            kafkaTemplate.send("orders", order);
            kafkaTemplate.send("audit", createAudit(order));
            // All committed together
        } catch (Exception e) {
            log.error("Transaction failed, rolling back", e);
            throw e; // Trigger rollback
        }
    }
}
```

---

## 12. Best Practices

1. **Configure appropriate retry backoff** - avoid overwhelming the system
2. **Use DLT for unrecoverable errors** - preserve failed messages
3. **Add comprehensive error metadata** - track original topic, offset, timestamp
4. **Implement idempotent consumers** - handle retries gracefully
5. **Monitor error rates** - set up alerts for anomalies
6. **Classify exceptions** - retryable vs non-retryable
7. **Use ErrorHandlingDeserializer** - handle deserialization failures
8. **Implement circuit breakers** - protect external dependencies
9. **Log detailed error context** - aid troubleshooting
10. **Test error scenarios** - ensure recovery mechanisms work

---

## 13. Interview Questions

**Q1: What is DefaultErrorHandler?**
**A:** Spring Kafka's error handler for consumer exceptions with retry and recovery mechanisms.

**Q2: What is a Dead Letter Topic?**
**A:** Topic for storing messages that failed processing after all retry attempts.

**Q3: FixedBackOff vs ExponentialBackOff?**
**A:** Fixed: Same interval between retries. Exponential: Interval increases (e.g., 1s, 2s, 4s, 8s).

**Q4: How to handle deserialization errors?**
**A:** Use ErrorHandlingDeserializer which wraps actual deserializer and adds error to headers.

**Q5: What are retryable vs non-retryable exceptions?**
**A:** Retryable: Transient errors (network). Non-retryable: Permanent errors (validation).

**Q6: How to prevent infinite retries?**
**A:** Set max retry attempts in BackOff or max elapsed time in ExponentialBackOff.

**Q7: What is poison message?**
**A:** Message that always causes processing to fail, potentially blocking the queue.

**Q8: How to route errors by exception type?**
**A:** Use custom DestinationTopicResolver in DeadLetterPublishingRecoverer.

**Q9: What is non-blocking retry?**
**A:** Sending failed messages to retry topic instead of blocking consumer thread.

**Q10: How to add error metadata to DLT?**
**A:** Use setHeadersFunction() on DeadLetterPublishingRecoverer to add custom headers.

**Q11: What is circuit breaker pattern?**
**A:** Stops calling failing service after threshold, preventing cascade failures.

**Q12: How to monitor error rates?**
**A:** Use Micrometer metrics and retry listeners to track errors and DLT messages.

**Q13: What is ErrorHandlingDeserializer?**
**A:** Deserializer wrapper that catches exceptions and adds them to record headers.

**Q14: How to handle producer errors?**
**A:** Use callbacks on CompletableFuture or ProducerListener for success/failure handling.

**Q15: What is retry listener?**
**A:** Callback invoked on each retry attempt, useful for logging and metrics.

**Q16: How to implement multi-level retries?**
**A:** Create multiple retry topics with increasing delays (retry1, retry2, retry3, DLT).

**Q17: What is quarantine topic?**
**A:** Special topic for poison messages requiring manual intervention.

**Q18: How to rollback Kafka transactions?**
**A:** Throw exception in @Transactional method to trigger automatic rollback.

**Q19: What is BackOffExecution?**
**A:** Interface defining retry delay logic, returning STOP when max attempts reached.

**Q20: How to test error handling?**
**A:** Throw exceptions in test listeners and verify DLT messages or retry behavior.

---

## 14. Summary

Effective error handling in Kafka requires understanding producer and consumer error scenarios, configuring appropriate retry strategies, using Dead Letter Topics for failed messages, implementing deserialization error handling, monitoring error metrics, and testing recovery mechanisms. Proper error handling ensures system resilience and prevents message loss.

---

**Next:** RabbitMQ Integration â†’
