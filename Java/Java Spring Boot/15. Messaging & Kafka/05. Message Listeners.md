# Kafka Message Listeners - Complete Guide

---

## 1. Introduction to Kafka Listeners

Kafka listeners are components that consume messages from Kafka topics. Spring Kafka provides the `@KafkaListener` annotation for declarative message consumption with powerful configuration options for concurrency, error handling, and message processing.

### Key Features
- **Declarative consumption** with @KafkaListener
- **Flexible message mapping** (payload, headers, metadata)
- **Concurrent processing** with multiple threads
- **Batch and record listeners**
- **Manual and automatic offset management**
- **Error handling** and retry mechanisms

---

## 2. Basic @KafkaListener

### 2.1. Simple Listener

```java
@Component
public class OrderListener {
    
    @KafkaListener(topics = "order-events", groupId = "order-group")
    public void listen(String message) {
        log.info("Received message: {}", message);
    }
}
```

### 2.2. Typed Message Listener

```java
@Component
public class OrderListener {
    
    @KafkaListener(topics = "order-events", groupId = "order-group")
    public void listen(Order order) {
        log.info("Received order: {}", order);
        processOrder(order);
    }
}
```

---

## 3. Message Metadata Access

### 3.1. ConsumerRecord

```java
@KafkaListener(topics = "order-events")
public void listen(ConsumerRecord<String, Order> record) {
    log.info("Topic: {}", record.topic());
    log.info("Partition: {}", record.partition());
    log.info("Offset: {}", record.offset());
    log.info("Key: {}", record.key());
    log.info("Timestamp: {}", record.timestamp());
    log.info("Value: {}", record.value());
    
    record.headers().forEach(header -> {
        log.info("Header: {} = {}", header.key(), new String(header.value()));
    });
}
```

### 3.2. Using @Header Annotation

```java
@KafkaListener(topics = "order-events")
public void listen(
    @Payload Order order,
    @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
    @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
    @Header(KafkaHeaders.OFFSET) long offset,
    @Header(KafkaHeaders.RECEIVED_KEY) String key,
    @Header(KafkaHeaders.RECEIVED_TIMESTAMP) long timestamp
) {
    log.info("Received from topic={}, partition={}, offset={}", topic, partition, offset);
    log.info("Key={}, Timestamp={}", key, timestamp);
    log.info("Order: {}", order);
}
```

### 3.3. Custom Headers

```java
@KafkaListener(topics = "order-events")
public void listen(
    Order order,
    @Header(value = "source", required = false) String source,
    @Header(value = "correlation-id", required = false) String correlationId,
    @Header(value = "version", required = false, defaultValue = "1.0") String version
) {
    log.info("Source: {}, CorrelationId: {}, Version: {}", source, correlationId, version);
    log.info("Order: {}", order);
}
```

---

## 4. Multiple Topics and Patterns

### 4.1. Multiple Topics

```java
@KafkaListener(topics = {"order-events", "payment-events", "shipment-events"})
public void listenMultiple(ConsumerRecord<String, Object> record) {
    String topic = record.topic();
    
    switch (topic) {
        case "order-events":
            processOrder((Order) record.value());
            break;
        case "payment-events":
            processPayment((Payment) record.value());
            break;
        case "shipment-events":
            processShipment((Shipment) record.value());
            break;
    }
}
```

### 4.2. Topic Pattern

```java
@KafkaListener(topicPattern = ".*-events", groupId = "event-processor")
public void listenPattern(ConsumerRecord<String, String> record) {
    log.info("Received from {}: {}", record.topic(), record.value());
}
```

---

## 5. Partition Assignment

### 5.1. Specific Partitions

```java
@KafkaListener(
    topicPartitions = @TopicPartition(
        topic = "order-events",
        partitions = {"0", "1", "2"}
    ),
    groupId = "partition-group"
)
public void listenPartitions(Order order) {
    log.info("Received from specific partitions: {}", order);
}
```

### 5.2. Partitions with Initial Offset

```java
@KafkaListener(
    topicPartitions = @TopicPartition(
        topic = "order-events",
        partitionOffsets = {
            @PartitionOffset(partition = "0", initialOffset = "0"),
            @PartitionOffset(partition = "1", initialOffset = "100"),
            @PartitionOffset(partition = "2", initialOffset = "500")
        }
    ),
    groupId = "offset-group"
)
public void listenFromOffset(Order order) {
    log.info("Order from specific offset: {}", order);
}
```

### 5.3. Dynamic Partition Assignment

```java
@KafkaListener(
    topicPartitions = @TopicPartition(
        topic = "order-events",
        partitions = "#{@partitionResolver.getPartitions()}"
    )
)
public void listenDynamic(Order order) {
    log.info("Dynamic partition assignment: {}", order);
}

@Component
class PartitionResolver {
    public String[] getPartitions() {
        return new String[]{"0", "1"};
    }
}
```

---

## 6. Batch Listeners

### 6.1. Simple Batch Listener

```java
@KafkaListener(topics = "order-events", groupId = "batch-group")
public void listenBatch(List<Order> orders) {
    log.info("Received batch of {} orders", orders.size());
    orders.forEach(this::processOrder);
}
```

### 6.2. Batch with Metadata

```java
@KafkaListener(topics = "order-events")
public void listenBatchWithMetadata(List<ConsumerRecord<String, Order>> records) {
    log.info("Processing batch of {} records", records.size());
    
    records.forEach(record -> {
        log.info("Partition: {}, Offset: {}, Order: {}", 
            record.partition(), record.offset(), record.value());
        processOrder(record.value());
    });
}
```

### 6.3. Batch Configuration

```yaml
spring:
  kafka:
    listener:
      type: batch
    consumer:
      max-poll-records: 100
```

```java
@Configuration
public class BatchListenerConfig {
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Order> batchFactory(
        ConsumerFactory<String, Order> consumerFactory
    ) {
        ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory);
        factory.setBatchListener(true);
        return factory;
    }
}

@KafkaListener(topics = "orders", containerFactory = "batchFactory")
public void listenBatch(List<Order> orders) {
    // Process batch
}
```

---

## 7. Concurrency

### 7.1. Concurrent Consumers

```java
@KafkaListener(
    topics = "order-events",
    groupId = "concurrent-group",
    concurrency = "3"  // 3 consumer threads
)
public void listenConcurrent(Order order) {
    log.info("Thread: {}, Order: {}", Thread.currentThread().getName(), order);
}
```

### 7.2. Global Concurrency

```yaml
spring:
  kafka:
    listener:
      concurrency: 5
```

### 7.3. Dynamic Concurrency with SpEL

```java
@KafkaListener(
    topics = "order-events",
    concurrency = "#{@environment.getProperty('kafka.concurrency', '3')}"
)
public void listen(Order order) {
    processOrder(order);
}
```

---

## 8. Manual Offset Management

### 8.1. Manual Acknowledgment

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: false
    listener:
      ack-mode: manual
```

```java
@KafkaListener(topics = "order-events")
public void listen(ConsumerRecord<String, Order> record, Acknowledgment ack) {
    try {
        processOrder(record.value());
        ack.acknowledge(); // Commit offset
    } catch (Exception e) {
        log.error("Processing failed", e);
        // Don't acknowledge - message will be reprocessed
    }
}
```

### 8.2. Batch Manual Acknowledgment

```java
@KafkaListener(topics = "order-events")
public void listenBatch(
    List<ConsumerRecord<String, Order>> records,
    Acknowledgment ack
) {
    try {
        records.forEach(record -> processOrder(record.value()));
        ack.acknowledge(); // Commit entire batch
    } catch (Exception e) {
        log.error("Batch processing failed", e);
    }
}
```

### 8.3. Acknowledgment Modes

```java
@Bean
public ConcurrentKafkaListenerContainerFactory<String, Order> kafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
        new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    
    // Set acknowledgment mode
    factory.getContainerProperties().setAckMode(AckMode.MANUAL);
    // Other modes: RECORD, BATCH, TIME, COUNT, COUNT_TIME, MANUAL_IMMEDIATE
    
    return factory;
}
```

**Acknowledgment Modes:**
- **RECORD**: Commit after each record
- **BATCH**: Commit after each batch poll
- **TIME**: Commit at time intervals
- **COUNT**: Commit after N messages
- **COUNT_TIME**: Commit after N messages or time
- **MANUAL**: Application calls acknowledge()
- **MANUAL_IMMEDIATE**: Immediate commit on acknowledge()

---

## 9. Consumer Seek

### 9.1. Seek to Beginning

```java
@Component
public class ConsumerSeekController {
    
    @Autowired
    private KafkaListenerEndpointRegistry registry;
    
    public void seekToBeginning(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        container.pause();
        
        ConsumerSeekCallback callback = (consumer, topicPartition) -> 
            consumer.seekToBeginning(Collections.singleton(topicPartition));
        
        container.getContainerProperties().setConsumerStartTimestamp(callback);
        container.resume();
    }
}
```

### 9.2. Seek to Specific Offset

```java
public void seekToOffset(String listenerId, long offset) {
    MessageListenerContainer container = registry.getListenerContainer(listenerId);
    
    container.pause();
    container.seekToOffset("order-events", 0, offset);
    container.resume();
}
```

### 9.3. Seek to Timestamp

```java
public void seekToTimestamp(String listenerId, long timestamp) {
    MessageListenerContainer container = registry.getListenerContainer(listenerId);
    
    ConsumerSeekCallback callback = (consumer, topicPartition) -> {
        Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();
        timestampsToSearch.put(topicPartition, timestamp);
        
        Map<TopicPartition, OffsetAndTimestamp> offsets = 
            consumer.offsetsForTimes(timestampsToSearch);
        
        if (offsets.get(topicPartition) != null) {
            consumer.seek(topicPartition, offsets.get(topicPartition).offset());
        }
    };
    
    container.getContainerProperties().setConsumerRebalanceListener(callback);
}
```

---

## 10. Filtering Messages

### 10.1. Record Filter Strategy

```java
@Bean
public ConcurrentKafkaListenerContainerFactory<String, Order> filterFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
        new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    
    factory.setRecordFilterStrategy(record -> {
        Order order = record.value();
        // Filter out cancelled orders
        return order.getStatus().equals("CANCELLED");
    });
    
    return factory;
}

@KafkaListener(topics = "order-events", containerFactory = "filterFactory")
public void listenFiltered(Order order) {
    log.info("Active order: {}", order);
}
```

### 10.2. Batch Filter Strategy

```java
factory.setBatchFilterStrategy(records -> {
    // Filter entire batch if condition met
    return records.stream()
        .anyMatch(record -> record.value().getStatus().equals("INVALID"));
});
```

---

## 11. Reply and Request-Reply

### 11.1. Sending Reply

```java
@KafkaListener(topics = "request-topic")
@SendTo("response-topic")
public OrderResponse listen(OrderRequest request) {
    log.info("Processing request: {}", request);
    return processRequest(request);
}
```

### 11.2. Dynamic Reply Topic

```java
@KafkaListener(topics = "request-topic")
@SendTo
public Message<?> listen(OrderRequest request, 
                        @Header(KafkaHeaders.REPLY_TOPIC) String replyTopic) {
    OrderResponse response = processRequest(request);
    
    return MessageBuilder
        .withPayload(response)
        .setHeader(KafkaHeaders.TOPIC, replyTopic)
        .build();
}
```

### 11.3. Request-Reply Template

```java
@Service
public class RequestReplyService {
    
    @Autowired
    private ReplyingKafkaTemplate<String, OrderRequest, OrderResponse> replyTemplate;
    
    public OrderResponse sendAndReceive(OrderRequest request) throws Exception {
        ProducerRecord<String, OrderRequest> record = 
            new ProducerRecord<>("request-topic", request);
        
        RequestReplyFuture<String, OrderRequest, OrderResponse> future = 
            replyTemplate.sendAndReceive(record);
        
        ConsumerRecord<String, OrderResponse> response = future.get(10, TimeUnit.SECONDS);
        return response.value();
    }
}
```

---

## 12. Error Handling in Listeners

### 12.1. Error Handler

```java
@Bean
public DefaultErrorHandler errorHandler() {
    BackOff fixedBackOff = new FixedBackOff(1000L, 3L); // 1s interval, 3 retries
    DefaultErrorHandler handler = new DefaultErrorHandler(fixedBackOff);
    
    // Don't retry for certain exceptions
    handler.addNotRetryableExceptions(IllegalArgumentException.class);
    
    return handler;
}

@Bean
public ConcurrentKafkaListenerContainerFactory<String, Order> kafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
        new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    factory.setCommonErrorHandler(errorHandler());
    return factory;
}
```

### 12.2. Dead Letter Topic (DLT)

```java
@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<String, Object> template) {
    DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
        template,
        (record, ex) -> new TopicPartition(record.topic() + ".DLT", record.partition())
    );
    
    return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}

// DLT Listener
@KafkaListener(topics = "order-events.DLT")
public void listenDLT(ConsumerRecord<String, Order> record) {
    log.error("Message in DLT: {}", record.value());
    // Handle poison messages
}
```

### 12.3. Retry with Exponential Backoff

```java
@Bean
public DefaultErrorHandler errorHandler() {
    ExponentialBackOff backOff = new ExponentialBackOff(1000L, 2.0);
    backOff.setMaxInterval(10000L); // Max 10 seconds
    backOff.setMaxElapsedTime(60000L); // Max 1 minute total
    
    return new DefaultErrorHandler(backOff);
}
```

---

## 13. Container Lifecycle

### 13.1. Lifecycle Events

```java
@Component
public class KafkaLifecycleListener {
    
    @EventListener
    public void onContainerStarted(ConsumerStartedEvent event) {
        log.info("Container started: {}", event.getContainer());
    }
    
    @EventListener
    public void onContainerStopped(ConsumerStoppedEvent event) {
        log.info("Container stopped: {}", event.getContainer());
    }
    
    @EventListener
    public void onPartitionsAssigned(ConsumerPartitionsAssignedEvent event) {
        log.info("Partitions assigned: {}", event.getPartitions());
    }
    
    @EventListener
    public void onPartitionsRevoked(ConsumerPartitionsRevokedEvent event) {
        log.info("Partitions revoked: {}", event.getPartitions());
    }
}
```

### 13.2. Pause and Resume

```java
@Component
public class ListenerController {
    
    @Autowired
    private KafkaListenerEndpointRegistry registry;
    
    public void pauseListener(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        if (container != null) {
            container.pause();
            log.info("Listener {} paused", listenerId);
        }
    }
    
    public void resumeListener(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        if (container != null) {
            container.resume();
            log.info("Listener {} resumed", listenerId);
        }
    }
    
    public boolean isRunning(String listenerId) {
        MessageListenerContainer container = registry.getListenerContainer(listenerId);
        return container != null && container.isRunning();
    }
}
```

---

## 14. Custom Container Factory

```java
@Configuration
public class CustomListenerConfig {
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Order> customFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Order> factory = 
            new ConcurrentKafkaListenerContainerFactory<>();
        
        factory.setConsumerFactory(consumerFactory());
        factory.setConcurrency(3);
        factory.setBatchListener(false);
        factory.getContainerProperties().setAckMode(AckMode.MANUAL);
        factory.getContainerProperties().setPollTimeout(3000);
        factory.getContainerProperties().setIdleEventInterval(60000L);
        factory.setCommonErrorHandler(errorHandler());
        
        return factory;
    }
    
    @KafkaListener(topics = "orders", containerFactory = "customFactory")
    public void listen(Order order, Acknowledgment ack) {
        processOrder(order);
        ack.acknowledge();
    }
}
```

---

## 15. Testing Listeners

### 15.1. Embedded Kafka Test

```java
@SpringBootTest
@EmbeddedKafka(partitions = 1, topics = {"test-topic"})
public class ListenerTest {
    
    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;
    
    @Autowired
    private TestListener testListener;
    
    @Test
    public void testListener() throws Exception {
        kafkaTemplate.send("test-topic", "test-message");
        
        // Wait for consumption
        assertTrue(testListener.getLatch().await(10, TimeUnit.SECONDS));
        assertEquals("test-message", testListener.getReceivedMessage());
    }
}

@Component
class TestListener {
    private CountDownLatch latch = new CountDownLatch(1);
    private String receivedMessage;
    
    @KafkaListener(topics = "test-topic")
    public void listen(String message) {
        this.receivedMessage = message;
        latch.countDown();
    }
    
    public CountDownLatch getLatch() { return latch; }
    public String getReceivedMessage() { return receivedMessage; }
}
```

---

## 16. Best Practices

1. **Use manual acknowledgment** for critical processing
2. **Implement idempotent consumers** (handle duplicates)
3. **Configure appropriate concurrency** based on partitions
4. **Use batch processing** for high-throughput scenarios
5. **Implement proper error handling** with DLT
6. **Monitor consumer lag** regularly
7. **Handle rebalancing** gracefully
8. **Use filtering** to skip irrelevant messages
9. **Set proper timeouts** (poll, session, heartbeat)
10. **Test with embedded Kafka** for reliability

---

## 17. Interview Questions

**Q1: What is @KafkaListener?**
**A:** Annotation for declarative Kafka message consumption in Spring applications.

**Q2: How to access message metadata?**
**A:** Use ConsumerRecord or @Header annotations to access topic, partition, offset, headers.

**Q3: What is concurrency in listeners?**
**A:** Number of consumer threads processing messages in parallel from different partitions.

**Q4: Batch vs record listener?**
**A:** Record: Processes one message at a time. Batch: Processes multiple messages together.

**Q5: What are acknowledgment modes?**
**A:** RECORD, BATCH, TIME, COUNT, COUNT_TIME, MANUAL, MANUAL_IMMEDIATE - control offset commit timing.

**Q6: How to filter messages?**
**A:** Use RecordFilterStrategy in container factory to skip unwanted messages.

**Q7: What is consumer seek?**
**A:** Moving consumer position to specific offset, timestamp, or beginning/end of partition.

**Q8: How to handle errors in listeners?**
**A:** Use DefaultErrorHandler with retry backoff and DeadLetterPublishingRecoverer for DLT.

**Q9: What is Dead Letter Topic?**
**A:** Topic for messages that failed processing after all retries.

**Q10: How to pause/resume listeners?**
**A:** Use KafkaListenerEndpointRegistry to get container and call pause()/resume().

**Q11: What is @SendTo?**
**A:** Annotation to send listener method return value to specified topic.

**Q12: How to consume from specific partitions?**
**A:** Use @TopicPartition with partitions or partitionOffsets parameters.

**Q13: What is topicPattern?**
**A:** Regex pattern to subscribe to multiple topics matching the pattern.

**Q14: How to implement request-reply?**
**A:** Use @SendTo or ReplyingKafkaTemplate for synchronous request-response pattern.

**Q15: What is ConsumerRebalanceListener?**
**A:** Callback for partition assignment/revocation events during rebalancing.

**Q16: How to seek to timestamp?**
**A:** Use consumer.offsetsForTimes() to find offset for timestamp, then seek.

**Q17: What is idle event?**
**A:** Event fired when no messages received for configured idle interval.

**Q18: How to handle poison messages?**
**A:** Use error handler with DLT to isolate messages that always fail.

**Q19: What is manual acknowledgment?**
**A:** Application explicitly commits offset after successful processing via Acknowledgment.acknowledge().

**Q20: How to test Kafka listeners?**
**A:** Use @EmbeddedKafka for integration tests with in-memory broker.

---

## 18. Summary

Kafka listeners in Spring provide powerful, flexible message consumption with declarative configuration, concurrent processing, manual offset control, comprehensive error handling, and lifecycle management. Understanding acknowledgment modes, batch processing, and error recovery patterns is essential for building robust event-driven applications.

---

**Next:** Error Handling â†’
