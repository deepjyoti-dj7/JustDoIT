# 04. Replication & Leaders

## What is Replication in Kafka?

**Replication** is the process of copying partition data across multiple brokers to ensure fault tolerance and high availability.

**Why Replication?**
- Prevents data loss if broker fails
- Ensures high availability
- Enables disaster recovery
- Maintains service during maintenance

```
Topic: orders (Partition 0, Replication Factor 3)

Broker 1: Partition 0 (Leader)      ← Handles reads/writes
Broker 2: Partition 0 (Follower)    ← Replicates data
Broker 3: Partition 0 (Follower)    ← Replicates data
```

---

## Replication Factor

### What is Replication Factor?

Number of copies of each partition across brokers.

```bash
# Create topic with replication factor 3
kafka-topics.sh --create \
  --topic orders \
  --partitions 3 \
  --replication-factor 3 \
  --bootstrap-server localhost:9092
```

### Replication Factor Examples

**Replication Factor = 1:**
```
Broker 1: P0 (Leader)

If Broker 1 fails → Data lost ❌
```

**Replication Factor = 2:**
```
Broker 1: P0 (Leader)
Broker 2: P0 (Follower)

Can survive 1 broker failure ✅
```

**Replication Factor = 3 (Recommended):**
```
Broker 1: P0 (Leader)
Broker 2: P0 (Follower)
Broker 3: P0 (Follower)

Can survive 2 broker failures ✅
```

### Best Practices

✅ **Production**: Use replication factor ≥ 3
✅ **Development**: Can use 1 or 2
✅ **Critical data**: Use 3 or more
❌ **Never use**: Replication factor > number of brokers

---

## Leader and Follower

### Leader Replica

The **leader** handles all client requests for a partition.

**Responsibilities:**
- All producer writes
- All consumer reads (by default)
- Maintaining replica consistency
- Tracking follower progress

```
Producer → Leader (writes here)
Consumer → Leader (reads from here)
```

### Follower Replica

**Followers** replicate data from the leader.

**Responsibilities:**
- Fetch data from leader
- Stay in sync with leader
- Ready to become leader if needed
- Do NOT serve client requests

```
Leader → Follower 1 (replicates)
      → Follower 2 (replicates)
```

### Example Architecture

```
Topic: orders (3 partitions, RF=3)

Partition 0:
  Broker 1: Leader
  Broker 2: Follower
  Broker 3: Follower

Partition 1:
  Broker 1: Follower
  Broker 2: Leader      ← Different leader
  Broker 3: Follower

Partition 2:
  Broker 1: Follower
  Broker 2: Follower
  Broker 3: Leader      ← Different leader

Leaders distributed across brokers ✅
```

---

## In-Sync Replicas (ISR)

### What is ISR?

**ISR** = Set of replicas that are fully caught up with the leader.

**Criteria:**
- Replica has fetched latest messages
- Replica lag is within acceptable limits
- Replica sends heartbeats regularly

```
Leader: offset 100
Follower 1: offset 100 ✅ (In ISR)
Follower 2: offset 98  ❌ (Out of sync, not in ISR)
```

### ISR List

```bash
kafka-topics.sh --describe --topic orders --bootstrap-server localhost:9092
```

**Output:**
```
Topic: orders   Partition: 0    Leader: 1    Replicas: 1,2,3    Isr: 1,2,3
                                                                      ↑
                                                          All replicas in sync
```

### Replica Falls Out of ISR

**Conditions:**
- Replica.lag.time.max.ms exceeded (default 10 seconds)
- Follower stops fetching
- Follower crashes
- Network partition

```
Time 0: ISR = [1, 2, 3]
Time 10s: Broker 3 stops responding
Time 20s: ISR = [1, 2] (Broker 3 removed)
```

### Replica Rejoins ISR

```
1. Follower catches up with leader
2. Fetches all missing messages
3. Lag < threshold
4. Added back to ISR automatically

ISR = [1, 2] → Broker 3 catches up → ISR = [1, 2, 3]
```

---

## Leader Election

### When Does Leader Election Happen?

1. Broker (current leader) crashes
2. Broker becomes unresponsive
3. Manual partition reassignment
4. Broker shutdown for maintenance

### How Leader Election Works

```
Step 1: Kafka Controller detects leader failure
        (via heartbeats or ZooKeeper/KRaft)

Step 2: Controller picks new leader from ISR
        (First available replica in ISR list)

Step 3: Updates metadata
        - All brokers notified
        - Producers/consumers get new leader info

Step 4: New leader starts serving requests
```

**Typical time: 1-5 seconds**

### Leader Election Example

**Before:**
```
Partition 0:
  Broker 1: Leader (ISR)
  Broker 2: Follower (ISR)
  Broker 3: Follower (ISR)
```

**Broker 1 crashes:**
```
Partition 0:
  Broker 1: Down ❌
  Broker 2: New Leader (promoted from ISR) ✅
  Broker 3: Follower (ISR)
```

**Broker 1 recovers:**
```
Partition 0:
  Broker 1: Follower (rejoins as follower)
  Broker 2: Leader (stays leader)
  Broker 3: Follower (ISR)
```

---

## Unclean Leader Election

### What is Unclean Leader Election?

Allowing a **non-ISR** replica to become leader when no ISR is available.

### Risk

```
Leader (offset 100) crashes
ISR replicas all down
Only non-ISR replica available (offset 95)

If unclean election enabled:
  → Non-ISR becomes leader
  → Messages 96-100 lost ❌
```

### Configuration

```properties
# server.properties
unclean.leader.election.enable=false  # Recommended for data safety
```

**false (default):** Wait for ISR to come back (no data loss, but unavailability)
**true:** Promote non-ISR (availability, but data loss risk)

### When to Use

✅ **Disable (false):**
- Financial data
- Critical business data
- Data integrity > availability

✅ **Enable (true):**
- Logs, metrics
- Availability > data integrity
- Some data loss acceptable

---

## Replication Process

### How Replication Works

```
1. Producer sends message to leader
      ↓
2. Leader writes to local log
      ↓
3. Leader responds to producer (if acks=1)
      ↓
4. Followers fetch from leader
      ↓
5. Followers write to local log
      ↓
6. Followers send ack to leader
      ↓
7. Leader updates high watermark
      ↓
8. Message visible to consumers
```

### High Watermark

**High Watermark (HW)** = Offset of last message replicated to all ISR.

```
Leader log:     [M0][M1][M2][M3][M4]
                           ↑
                     High Watermark (offset 2)

Follower 1 log: [M0][M1][M2]
Follower 2 log: [M0][M1][M2]

Consumers can read up to offset 2 only
Messages 3-4 not yet fully replicated
```

### Log End Offset (LEO)

**LEO** = Offset of last message in the log (may not be replicated yet).

```
Leader LEO: 5 (latest message)
High Watermark: 2 (last replicated message)

Gap = 3 messages not yet replicated
```

---

## Acknowledgment Modes

### acks=0 (No Acknowledgment)

```
Producer → Leader (fire and forget)
           (no wait for response)

Fastest, but may lose data ❌
```

### acks=1 (Leader Acknowledgment)

```
Producer → Leader writes
        ← Ack (before followers replicate)

If leader crashes before replication → Data lost ❌
```

### acks=all (All ISR Acknowledgment)

```
Producer → Leader writes
        → Followers replicate
        ← Ack (after all ISR replicate)

Safest, no data loss ✅
Slowest performance
```

**Configuration:**

```javascript
const producer = kafka.producer({
  acks: 'all',              // Wait for all ISR
  retries: 5,
  idempotence: true
});
```

---

## min.insync.replicas

### What is min.insync.replicas?

Minimum number of ISR required for writes to succeed.

**Purpose:** Ensure data is replicated to enough replicas.

### Configuration

```properties
# Topic-level config
min.insync.replicas=2
```

### How It Works

```
Replication Factor = 3
min.insync.replicas = 2

Scenario 1: ISR = [1, 2, 3]
  → Write succeeds ✅ (3 ≥ 2)

Scenario 2: ISR = [1, 2]
  → Write succeeds ✅ (2 ≥ 2)

Scenario 3: ISR = [1]
  → Write fails ❌ (1 < 2)
  → Error: NOT_ENOUGH_REPLICAS
```

### Best Practice

```
Replication Factor = 3
min.insync.replicas = 2
acks = 'all'

Guarantees:
  ✅ Data on at least 2 brokers
  ✅ Can survive 1 broker failure
  ✅ Balance between safety and availability
```

---

## Monitoring Replication

### Check Topic Details

```bash
kafka-topics.sh --describe \
  --topic orders \
  --bootstrap-server localhost:9092
```

**Output:**
```
Topic: orders
  Partition: 0
    Leader: 1
    Replicas: 1,2,3
    Isr: 1,2,3              ← All in sync ✅
    
  Partition: 1
    Leader: 2
    Replicas: 2,3,1
    Isr: 2,3                ← Replica 1 out of sync ❌
```

### Check Under-Replicated Partitions

```bash
kafka-topics.sh --describe \
  --under-replicated-partitions \
  --bootstrap-server localhost:9092
```

**Shows partitions where ISR < Replicas**

### Monitor Metrics

**Key metrics:**
- Under-replicated partitions (should be 0)
- ISR shrink rate (should be low)
- ISR expand rate
- Leader election rate
- Replica lag (time and messages)

---

## Preferred Leader Election

### What is Preferred Leader?

The **first replica** in the replica list is the "preferred leader."

**Why?**
- Distribute load evenly
- Return to balanced state after failures

### Automatic Preferred Leader Election

```properties
# server.properties
auto.leader.rebalance.enable=true
leader.imbalance.check.interval.seconds=300
leader.imbalance.per.broker.percentage=10
```

### Manual Preferred Leader Election

```bash
kafka-preferred-replica-election.sh \
  --bootstrap-server localhost:9092 \
  --path-to-json-file election.json
```

**election.json:**
```json
{
  "partitions": [
    {"topic": "orders", "partition": 0},
    {"topic": "orders", "partition": 1}
  ]
}
```

---

## Rack Awareness

### What is Rack Awareness?

Distributing replicas across different physical racks or availability zones.

**Benefits:**
- Survive rack/zone failures
- Better fault tolerance
- Geographic distribution

### Configuration

```properties
# server.properties
broker.rack=us-east-1a
```

**Brokers:**
```
Broker 1: rack = us-east-1a
Broker 2: rack = us-east-1b
Broker 3: rack = us-east-1c
```

**Partition replicas distributed:**
```
Partition 0:
  Leader: Broker 1 (us-east-1a)
  Follower: Broker 2 (us-east-1b)
  Follower: Broker 3 (us-east-1c)

If rack us-east-1a fails → Failover to other racks ✅
```

---

## Best Practices

### Replication Factor

✅ Use RF ≥ 3 in production
✅ Critical data: RF = 3 or 5
✅ RF should be ≤ number of brokers
✅ Odd numbers preferred (3, 5, 7)

### ISR Management

✅ Monitor under-replicated partitions
✅ Set `replica.lag.time.max.ms` appropriately (default 10s)
✅ Alert on ISR shrinkage
✅ Investigate replicas frequently out of ISR

### Leader Election

✅ Disable unclean leader election for critical data
✅ Enable auto preferred leader election
✅ Distribute partitions evenly across brokers
✅ Monitor leader election rate

### Configuration

✅ Use `acks='all'` for producers
✅ Set `min.insync.replicas=2` minimum
✅ Ensure `min.insync.replicas < replication.factor`
✅ Use rack awareness in multi-zone deployments

---

## Summary

**Replication:**
- Copies partition data to multiple brokers
- Replication factor defines number of copies
- Ensures fault tolerance and high availability

**Leader:**
- Handles all client reads/writes for partition
- Maintains replica consistency
- One leader per partition

**Follower:**
- Replicates data from leader
- Can be promoted to leader
- Does not serve client requests

**ISR:**
- Set of in-sync replicas
- Only ISRs eligible for leader election
- Ensures no data loss during failover

**Key Configs:**
```
replication.factor = 3
min.insync.replicas = 2
acks = all
unclean.leader.election.enable = false
```

---

## Interview Questions & Answers

### Q1: What is replication in Kafka?
**A:** Replication is the process of copying partition data from the leader to follower replicas across multiple brokers. It ensures fault tolerance, high availability, and prevents data loss if brokers fail.

### Q2: What is the difference between leader and follower?
**A:**
- **Leader**: Handles all client read/write requests for a partition, maintains the authoritative copy
- **Follower**: Replicates data from leader, does not serve clients, can be promoted to leader if needed

### Q3: What is an ISR (In-Sync Replica)?
**A:** ISR is the set of replicas that are fully caught up with the leader (within acceptable lag time). Only ISRs are eligible to become leader during failover, ensuring no data loss.

### Q4: What happens when a leader broker fails?
**A:** Kafka controller detects the failure and automatically elects a new leader from the ISR set. The new leader starts serving client requests. Typical failover time is 1-5 seconds.

### Q5: What is unclean leader election?
**A:** Unclean leader election allows a non-ISR (out-of-sync) replica to become leader when no ISR is available. This can cause data loss but maintains availability. Recommended to disable (`unclean.leader.election.enable=false`) for critical data.

### Q6: What is min.insync.replicas?
**A:** It's the minimum number of ISRs that must acknowledge a write for it to succeed. If ISR count falls below this, writes fail with `NOT_ENOUGH_REPLICAS` error.

Example: `min.insync.replicas=2` with `acks=all` ensures data is on at least 2 replicas.

### Q7: What are the different acknowledgment modes (acks)?
**A:**
- **acks=0**: No acknowledgment (fire-and-forget, fastest, may lose data)
- **acks=1**: Leader acknowledges (medium safety, may lose if leader fails before replication)
- **acks=all**: All ISRs acknowledge (safest, no data loss, slowest)

### Q8: What is the recommended replication factor for production?
**A:** Replication factor of **3** is recommended for production. This allows:
- Survival of up to 2 broker failures
- With `min.insync.replicas=2`, system can tolerate 1 failure while still accepting writes
- Good balance between safety, availability, and resource usage

### Q9: How do you check if replicas are in sync?
**A:** Use `kafka-topics.sh --describe`:
```bash
kafka-topics.sh --describe --topic orders --bootstrap-server localhost:9092
```
Compare the `Replicas` and `Isr` lists. If they match, all replicas are in sync. Under-replicated partitions have fewer ISRs than replicas.

### Q10: What is a preferred leader?
**A:** The preferred leader is the first replica in the replica list. Kafka tries to make this replica the leader to ensure even load distribution across brokers. Auto preferred leader election can be enabled to automatically rebalance leaders after failures.

### Q11: What is high watermark?
**A:** High watermark (HW) is the offset of the last message that has been replicated to all ISRs. Consumers can only read messages up to the high watermark, ensuring they don't read data that might be lost if the leader fails.

### Q12: Can you increase replication factor after topic creation?
**A:** Yes, using `kafka-reassign-partitions.sh`. You can increase the replication factor by adding more replicas to partitions. However, you cannot decrease it directly.

### Q13: What is rack awareness in Kafka?
**A:** Rack awareness distributes partition replicas across different physical racks or availability zones. This ensures that if an entire rack/zone fails, replicas are available in other racks, improving fault tolerance.

Config: `broker.rack=us-east-1a`

### Q14: What happens if all ISRs are unavailable?
**A:**
- If `unclean.leader.election.enable=false`: Partition becomes unavailable until an ISR comes back (no data loss)
- If `unclean.leader.election.enable=true`: A non-ISR can become leader (availability maintained, but data loss possible)

### Q15: How does replication affect producer performance?
**A:**
- **acks=0**: Fastest (no wait)
- **acks=1**: Medium (wait for leader only)
- **acks=all**: Slowest (wait for all ISRs)

Higher replication factor and `acks=all` reduce throughput but ensure durability. Use batching, compression, and appropriate `linger.ms` to mitigate performance impact.

**Next:** Serialization (JSON, Avro, Protobuf)!
