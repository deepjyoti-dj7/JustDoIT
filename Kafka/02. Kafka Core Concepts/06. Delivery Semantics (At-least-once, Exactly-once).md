# 06. Delivery Semantics (At-least-once, Exactly-once)

## What are Delivery Semantics?

**Delivery semantics** define the guarantee of message delivery between producer and consumer.

Three types:
1. **At-most-once**: Message delivered 0 or 1 time (may lose)
2. **At-least-once**: Message delivered 1 or more times (may duplicate)
3. **Exactly-once**: Message delivered exactly 1 time (no loss, no duplicates)

---

## 1. At-Most-Once Delivery

### Guarantee

**Messages may be lost, but never duplicated.**

```
Producer sends message
  ↓
Broker may or may not receive
  ↓
No retry on failure
  ↓
Message lost if failed ❌
```

### Configuration (Producer)

```javascript
const producer = kafka.producer({
  acks: 0,           // No acknowledgment
  retries: 0         // No retries
});

await producer.send({
  topic: 'logs',
  messages: [{ value: 'Log message' }]
});
```

### Use Cases

✅ Logging (some loss acceptable)
✅ Metrics/monitoring
✅ Non-critical notifications
✅ High-throughput scenarios

❌ Financial transactions
❌ Critical business data

---

## 2. At-Least-Once Delivery

### Guarantee

**Messages never lost, but may be duplicated.**

```
Producer sends message
  ↓
Broker receives and acks
  ↓
If ack lost, producer retries
  ↓
Message may be written twice ⚠️
```

### Configuration (Producer)

```javascript
const producer = kafka.producer({
  acks: 'all',       // Wait for all replicas
  retries: 5,        // Retry on failure
  idempotence: false // Allow duplicates
});

await producer.send({
  topic: 'orders',
  messages: [{ value: 'Order data' }]
});
```

### Configuration (Consumer)

```javascript
const consumer = kafka.consumer({
  groupId: 'my-group',
  autoCommit: false  // Manual commit after processing
});

await consumer.run({
  eachMessage: async ({ topic, partition, message }) => {
    // Process message
    await processMessage(message);
    
    // Commit offset after processing
    await consumer.commitOffsets([
      {
        topic,
        partition,
        offset: (parseInt(message.offset) + 1).toString()
      }
    ]);
  }
});
```

### Duplicate Handling

**Idempotent Processing:**

```javascript
const processedIds = new Set();

await consumer.run({
  eachMessage: async ({ message }) => {
    const order = JSON.parse(message.value.toString());
    
    // Check if already processed
    if (processedIds.has(order.id)) {
      console.log('Duplicate, skipping');
      return;
    }
    
    // Process
    await saveToDatabase(order);
    processedIds.add(order.id);
  }
});
```

### Use Cases

✅ **Default choice for most applications**
✅ Financial transactions (with deduplication)
✅ Order processing
✅ Critical event streams

---

## 3. Exactly-Once Semantics (EOS)

### Guarantee

**Messages delivered exactly once - no loss, no duplicates.**

```
Producer sends message (idempotent)
  ↓
Broker deduplicates (same producer ID + sequence)
  ↓
Consumer processes once
  ↓
Transactional commit
  ↓
Message processed exactly once ✅
```

### How It Works

**Producer side:**
- Idempotent producer (prevents duplicate writes)
- Producer ID and sequence numbers
- Automatic deduplication

**Consumer side:**
- Transactional reads
- Atomic offset commits
- Read committed messages only

### Configuration (Producer - Idempotent)

```javascript
const producer = kafka.producer({
  idempotence: true,    // Enable exactly-once for producer
  acks: 'all',          // Required for idempotence
  maxInFlightRequests: 5,
  retries: Number.MAX_SAFE_INTEGER
});

await producer.send({
  topic: 'orders',
  messages: [{ value: 'Order data' }]
});
```

**What happens:**
```
Producer ID: 123
Sequence: 0, 1, 2, 3...

If retry sends same message:
  Broker sees: ID=123, Seq=2 (already written)
  Broker: "Already have this, ignoring"
  No duplicate! ✅
```

### Configuration (Transactional Producer)

```javascript
const producer = kafka.producer({
  transactionalId: 'my-transactional-producer',
  idempotence: true,
  acks: 'all'
});

await producer.connect();

// Start transaction
const transaction = await producer.transaction();

try {
  // Send messages in transaction
  await transaction.send({
    topic: 'orders',
    messages: [{ value: 'Order 1' }]
  });
  
  await transaction.send({
    topic: 'payments',
    messages: [{ value: 'Payment 1' }]
  });
  
  // Commit transaction
  await transaction.commit();
} catch (error) {
  // Rollback on error
  await transaction.abort();
}
```

### Configuration (Consumer - Read Committed)

```javascript
const consumer = kafka.consumer({
  groupId: 'my-group',
  isolation: 'read_committed'  // Only read committed transactions
});

await consumer.subscribe({ topic: 'orders' });

await consumer.run({
  eachMessage: async ({ message }) => {
    // Process only committed messages
    await processMessage(message);
  }
});
```

### Exactly-Once End-to-End

**Stream Processing Example:**

```javascript
const producer = kafka.producer({
  transactionalId: 'processor-1',
  idempotence: true
});

const consumer = kafka.consumer({
  groupId: 'processor-group',
  isolation: 'read_committed'
});

await consumer.subscribe({ topic: 'input' });

await consumer.run({
  eachMessage: async ({ topic, partition, message }) => {
    const transaction = await producer.transaction();
    
    try {
      // Process input message
      const result = await processMessage(message);
      
      // Send result
      await transaction.send({
        topic: 'output',
        messages: [{ value: result }]
      });
      
      // Commit consumer offset in same transaction
      await transaction.sendOffsets({
        consumerGroupId: 'processor-group',
        topics: [{
          topic,
          partitions: [{
            partition,
            offset: (parseInt(message.offset) + 1).toString()
          }]
        }]
      });
      
      // Atomic commit (both output and offset)
      await transaction.commit();
    } catch (error) {
      await transaction.abort();
    }
  }
});
```

**What this achieves:**
```
Read input → Process → Write output → Commit offset
          ALL IN ONE TRANSACTION
          
If any step fails → Rollback all
No duplicate processing ✅
```

---

## Comparison Table

| Semantic | Duplicates | Loss | Performance | Complexity | Use Case |
|----------|------------|------|-------------|------------|----------|
| **At-most-once** | ❌ No | ✅ Possible | Fastest | Simple | Logs, metrics |
| **At-least-once** | ✅ Possible | ❌ No | Fast | Medium | Most apps |
| **Exactly-once** | ❌ No | ❌ No | Slowest | Complex | Critical data |

---

## Producer Delivery Guarantees

### Scenario 1: No Acks (At-Most-Once)

```javascript
acks: 0
retries: 0

Producer → Broker
           (doesn't wait for response)
           
Fastest, may lose data ❌
```

### Scenario 2: Leader Acks (At-Least-Once)

```javascript
acks: 1
retries: 5

Producer → Broker (Leader)
        ← Ack
        
If ack lost → retry → possible duplicate ⚠️
```

### Scenario 3: All Replicas (At-Least-Once)

```javascript
acks: 'all'
retries: 5
idempotence: false

Producer → Broker (Leader + Followers)
        ← Ack after all ISR write
        
Safer, but duplicates possible ⚠️
```

### Scenario 4: Idempotent (Exactly-Once)

```javascript
acks: 'all'
idempotence: true

Producer (ID=123, Seq=0) → Broker
                         ← Ack
                         
Retry → Broker sees duplicate sequence → Ignores ✅
```

---

## Consumer Delivery Guarantees

### At-Most-Once (Auto-Commit Before Processing)

```javascript
const consumer = kafka.consumer({
  autoCommit: true,
  autoCommitInterval: 5000
});

// Offset committed BEFORE processing
await consumer.run({
  eachMessage: async ({ message }) => {
    // If crash here, message lost ❌
    await processMessage(message);
  }
});
```

### At-Least-Once (Manual Commit After Processing)

```javascript
const consumer = kafka.consumer({
  autoCommit: false
});

await consumer.run({
  eachMessage: async ({ topic, partition, message }) => {
    await processMessage(message);
    
    // Commit AFTER processing
    await consumer.commitOffsets([{
      topic,
      partition,
      offset: (parseInt(message.offset) + 1).toString()
    }]);
    
    // If crash before commit → reprocess (duplicate) ⚠️
  }
});
```

### Exactly-Once (Transactional)

```javascript
// Process and commit in transaction
const transaction = await producer.transaction();

try {
  await processMessage(message);
  
  await transaction.send({
    topic: 'output',
    messages: [{ value: result }]
  });
  
  await transaction.sendOffsets({
    consumerGroupId: 'my-group',
    topics: [{ topic, partitions: [{ partition, offset }] }]
  });
  
  await transaction.commit();  // Atomic ✅
} catch (error) {
  await transaction.abort();
}
```

---

## Idempotent Producer

### How It Works

```
Producer assigns:
- Producer ID (PID): Unique per producer instance
- Sequence Number: Increments per message

Message: PID=123, Seq=5

Broker tracks:
{
  PID: 123,
  lastSeq: 5
}

Retry same message (PID=123, Seq=5):
Broker: "Already have Seq=5, duplicate!"
Deduplicates automatically ✅
```

### Enable Idempotence

```javascript
const producer = kafka.producer({
  idempotence: true  // Requires acks='all'
});
```

**Benefits:**
- No duplicates even with retries
- No application-level deduplication needed
- Safe retries

**Limitations:**
- Only within a single producer session
- Doesn't survive producer restart (new PID)

---

## Transactions

### What are Kafka Transactions?

Atomic multi-partition/multi-topic writes.

**Use cases:**
- Write to multiple topics atomically
- Consume-process-produce exactly-once
- Offset commits with message sends

### Transaction Example

```javascript
const transaction = await producer.transaction();

try {
  // All or nothing
  await transaction.send({ topic: 'topic1', messages: [...] });
  await transaction.send({ topic: 'topic2', messages: [...] });
  await transaction.send({ topic: 'topic3', messages: [...] });
  
  await transaction.commit();  // All visible together
} catch (error) {
  await transaction.abort();   // None visible
}
```

**Consumer sees:**
```
Before commit: No messages visible
After commit: All 3 messages visible at once ✅
After abort: No messages visible ✅
```

---

## Best Practices

### For At-Least-Once

✅ Enable retries on producer
✅ Use `acks='all'` for durability
✅ Implement idempotent processing on consumer
✅ Manual offset commit after successful processing
✅ Store processed IDs to detect duplicates

### For Exactly-Once

✅ Enable idempotent producer (`idempotence: true`)
✅ Use transactional producer for multi-topic writes
✅ Consumer reads with `isolation='read_committed'`
✅ Commit offsets within transactions
✅ Monitor transaction coordinator health

### General

✅ Choose semantic based on business requirements
✅ Test failure scenarios (network issues, crashes)
✅ Monitor duplicate rates
✅ Document delivery guarantees for your system

---

## Performance Impact

```
Throughput:
At-most-once:   ████████████ (Highest)
At-least-once:  ████████     (Medium)
Exactly-once:   █████        (Lowest)

Latency:
At-most-once:   Lowest
At-least-once:  Medium
Exactly-once:   Highest
```

**Tradeoff:** Stronger guarantees = Lower performance

---

## Summary

**At-Most-Once:**
- Fast, may lose messages
- `acks=0`, no retries
- Use for non-critical data

**At-Least-Once:**
- Never lose, may duplicate
- `acks='all'`, retries enabled
- Default choice, handle duplicates in app

**Exactly-Once:**
- No loss, no duplicates
- Idempotent + transactions
- Use for critical data (banking, payments)

**Key Concepts:**
```
Idempotent Producer → No duplicate writes
Transactions → Atomic multi-message operations
Read Committed → Only see committed transactions
```

---

## Interview Questions & Answers

### Q1: What are delivery semantics in Kafka?
**A:** Delivery semantics define the guarantee of message delivery:
- **At-most-once**: Delivered 0 or 1 time (may lose, no duplicates)
- **At-least-once**: Delivered 1+ times (no loss, may duplicate)
- **Exactly-once**: Delivered exactly 1 time (no loss, no duplicates)

### Q2: What is at-least-once delivery?
**A:** At-least-once ensures no message is lost, but duplicates are possible. Achieved by enabling retries and acknowledging after all replicas write. If ack is lost, producer retries, potentially writing duplicate.

Config: `acks='all'`, `retries > 0`

### Q3: What is exactly-once semantics (EOS)?
**A:** Exactly-once guarantees each message is processed exactly once - no loss, no duplicates. Achieved through:
- Idempotent producer (prevents duplicate writes)
- Transactions (atomic operations)
- Read committed isolation (consumers see only committed data)

### Q4: How does idempotent producer work?
**A:** Idempotent producer assigns:
- Producer ID (PID): Unique identifier
- Sequence number: Increments per message

Broker tracks last sequence per PID. If retry sends duplicate sequence, broker ignores it. Prevents duplicates even with retries.

Enable: `idempotence: true`

### Q5: What are Kafka transactions?
**A:** Transactions enable atomic writes across multiple partitions/topics. All messages in a transaction are visible together or not at all. Used for:
- Multi-topic writes
- Exactly-once stream processing
- Atomic offset commits with message sends

### Q6: What is the difference between idempotent producer and transactions?
**A:**
- **Idempotent producer**: Prevents duplicates within single topic/partition from same producer
- **Transactions**: Atomic writes across multiple topics/partitions, survives consumer-process-producer flow

Transactions provide stronger exactly-once guarantees.

### Q7: How do you configure at-least-once delivery?
**A:**
**Producer:**
```javascript
acks: 'all'
retries: 5
```

**Consumer:**
```javascript
autoCommit: false
// Manual commit after processing
```

### Q8: How do you configure exactly-once delivery?
**A:**
**Producer:**
```javascript
idempotence: true
transactionalId: 'my-app'
acks: 'all'
```

**Consumer:**
```javascript
isolation: 'read_committed'
```

Use transactions for atomic processing.

### Q9: What is `read_committed` isolation level?
**A:** Consumers with `isolation='read_committed'` only see messages from committed transactions. Aborted transactions are filtered out. Required for exactly-once semantics on consumer side.

### Q10: How do you handle duplicates in at-least-once?
**A:** Implement idempotent processing:
1. Track processed message IDs (database/cache)
2. Check if already processed before processing
3. Use unique message IDs or offsets as deduplication keys
4. Design operations to be naturally idempotent (e.g., upsert vs insert)

### Q11: What is the performance impact of exactly-once?
**A:**
- **Throughput**: 30-50% lower than at-least-once
- **Latency**: Higher due to transaction coordination
- **Complexity**: More configuration and monitoring

Tradeoff: Stronger guarantees for lower performance.

### Q12: Can you achieve exactly-once without transactions?
**A:** Partial exactly-once:
- Idempotent producer prevents duplicate writes
- But can't achieve end-to-end exactly-once for consume-process-produce without transactions
- Transactions needed for atomic offset commits with message sends

### Q13: What happens if a transaction is aborted?
**A:** All messages in the aborted transaction:
- Are not visible to `read_committed` consumers
- Are marked as aborted in Kafka logs
- Don't affect processing

It's as if the transaction never happened.

### Q14: What is `transactionalId` in Kafka?
**A:** `transactionalId` uniquely identifies a transactional producer. It enables:
- Fencing out zombie producers (old instances)
- Exactly-once across producer restarts
- Transaction recovery

Must be unique per producer instance doing transactions.

### Q15: When should you use each delivery semantic?
**A:**
- **At-most-once**: Logs, metrics, non-critical monitoring (performance critical)
- **At-least-once**: Most applications (default choice), handle duplicates in app
- **Exactly-once**: Financial transactions, billing, inventory, critical state changes (zero tolerance for duplicates/loss)

**End of Kafka Core Concepts!**