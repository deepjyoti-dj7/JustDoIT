# 05. Serialization (JSON, Avro, Protobuf)

## What is Serialization?

**Serialization** is the process of converting data structures or objects into a byte format for transmission over the network.

**Deserialization** is the reverse process - converting bytes back to data structures.

```
Producer:
Object → Serializer → Bytes → Kafka

Consumer:
Kafka → Bytes → Deserializer → Object
```

---

## Why Serialization in Kafka?

Kafka only stores and transmits **bytes**. Applications must serialize/deserialize data:

✅ Efficient network transmission
✅ Language-agnostic data exchange
✅ Schema evolution support
✅ Smaller message sizes (compression)

---

## Common Serialization Formats

| Format | Schema | Size | Speed | Human Readable |
|--------|--------|------|-------|----------------|
| **JSON** | No | Large | Slow | ✅ Yes |
| **Avro** | Yes | Small | Fast | ❌ No |
| **Protobuf** | Yes | Small | Fast | ❌ No |
| **String** | No | Medium | Fast | ✅ Yes |

---

## 1. JSON Serialization

### Characteristics

✅ Human-readable
✅ Easy to debug
✅ No schema required
✅ Widely supported

❌ Larger message size
❌ No schema validation
❌ Slower performance

### Producer Example (Node.js)

```javascript
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092']
});

const producer = kafka.producer();

await producer.connect();

// JSON object
const order = {
  orderId: '12345',
  userId: 'user123',
  amount: 99.99,
  timestamp: new Date().toISOString()
};

// Serialize to JSON string
await producer.send({
  topic: 'orders',
  messages: [
    {
      key: order.userId,
      value: JSON.stringify(order)  // Serialize
    }
  ]
});

await producer.disconnect();
```

### Consumer Example (Node.js)

```javascript
const consumer = kafka.consumer({ groupId: 'order-processor' });

await consumer.connect();
await consumer.subscribe({ topic: 'orders' });

await consumer.run({
  eachMessage: async ({ message }) => {
    // Deserialize from JSON string
    const order = JSON.parse(message.value.toString());
    
    console.log('Order:', order);
    console.log('Order ID:', order.orderId);
    console.log('Amount:', order.amount);
  }
});
```

### JSON Message Example

```json
{
  "orderId": "12345",
  "userId": "user123",
  "amount": 99.99,
  "items": [
    { "productId": "A1", "quantity": 2 },
    { "productId": "B2", "quantity": 1 }
  ],
  "timestamp": "2024-12-30T10:30:00Z"
}
```

**Size:** ~200 bytes (human-readable but verbose)

---

## 2. Avro Serialization

### Characteristics

✅ Compact binary format
✅ Schema-based (enforces structure)
✅ Schema evolution support
✅ Faster than JSON

❌ Not human-readable
❌ Requires schema registry
❌ More setup complexity

### Avro Schema Definition

```json
{
  "type": "record",
  "name": "Order",
  "namespace": "com.example",
  "fields": [
    { "name": "orderId", "type": "string" },
    { "name": "userId", "type": "string" },
    { "name": "amount", "type": "double" },
    { "name": "timestamp", "type": "long", "logicalType": "timestamp-millis" }
  ]
}
```

### Producer Example (Node.js with Avro)

```javascript
const { Kafka } = require('kafkajs');
const { SchemaRegistry } = require('@kafkajs/confluent-schema-registry');

const registry = new SchemaRegistry({ host: 'http://localhost:8081' });

const schema = {
  type: 'record',
  name: 'Order',
  fields: [
    { name: 'orderId', type: 'string' },
    { name: 'userId', type: 'string' },
    { name: 'amount', type: 'double' },
    { name: 'timestamp', type: 'long' }
  ]
};

const producer = kafka.producer();
await producer.connect();

const order = {
  orderId: '12345',
  userId: 'user123',
  amount: 99.99,
  timestamp: Date.now()
};

// Serialize with Avro
const encodedMessage = await registry.encode(1, order);  // 1 = schema ID

await producer.send({
  topic: 'orders',
  messages: [
    {
      key: order.userId,
      value: encodedMessage  // Binary Avro data
    }
  ]
});
```

### Consumer Example (Node.js with Avro)

```javascript
const consumer = kafka.consumer({ groupId: 'order-processor' });

await consumer.connect();
await consumer.subscribe({ topic: 'orders' });

await consumer.run({
  eachMessage: async ({ message }) => {
    // Deserialize from Avro
    const order = await registry.decode(message.value);
    
    console.log('Order:', order);
    console.log('Order ID:', order.orderId);
  }
});
```

### Avro Benefits

**Schema Evolution:**
```
Version 1:
{ orderId, userId, amount }

Version 2 (add field with default):
{ orderId, userId, amount, status: "pending" }

✅ Old consumers can still read new messages
✅ New consumers can read old messages
```

**Size:** ~50 bytes (much smaller than JSON)

---

## 3. Protobuf Serialization

### Characteristics

✅ Compact binary format
✅ Schema-based (`.proto` files)
✅ Fast serialization/deserialization
✅ Multi-language support
✅ Schema evolution

❌ Not human-readable
❌ Requires protobuf compiler

### Protobuf Schema Definition

```protobuf
// order.proto
syntax = "proto3";

message Order {
  string order_id = 1;
  string user_id = 2;
  double amount = 3;
  int64 timestamp = 4;
}
```

### Producer Example (Node.js with Protobuf)

```javascript
const protobuf = require('protobufjs');
const { Kafka } = require('kafkajs');

// Load schema
const root = await protobuf.load('order.proto');
const Order = root.lookupType('Order');

const producer = kafka.producer();
await producer.connect();

const order = {
  orderId: '12345',
  userId: 'user123',
  amount: 99.99,
  timestamp: Date.now()
};

// Validate and serialize
const errMsg = Order.verify(order);
if (errMsg) throw Error(errMsg);

const message = Order.create(order);
const buffer = Order.encode(message).finish();

await producer.send({
  topic: 'orders',
  messages: [
    {
      key: order.userId,
      value: buffer  // Binary Protobuf data
    }
  ]
});
```

### Consumer Example (Node.js with Protobuf)

```javascript
const consumer = kafka.consumer({ groupId: 'order-processor' });

await consumer.connect();
await consumer.subscribe({ topic: 'orders' });

await consumer.run({
  eachMessage: async ({ message }) => {
    // Deserialize from Protobuf
    const order = Order.decode(message.value);
    
    console.log('Order:', order);
    console.log('Order ID:', order.orderId);
  }
});
```

**Size:** ~40 bytes (smallest format)

---

## Schema Registry

### What is Schema Registry?

A centralized repository for storing and managing schemas (Avro, Protobuf, JSON Schema).

**Benefits:**
- Schema versioning
- Compatibility checking
- Centralized schema management
- Prevents incompatible changes

### How it Works

```
1. Producer registers schema → Schema Registry
2. Registry returns schema ID (e.g., 1)
3. Producer sends: [schema_id][avro_data]
4. Consumer reads schema_id
5. Consumer fetches schema from registry
6. Consumer deserializes data
```

### Schema Registry Setup

```bash
# Start Schema Registry (Docker)
docker run -d \
  --name schema-registry \
  -p 8081:8081 \
  -e SCHEMA_REGISTRY_HOST_NAME=schema-registry \
  -e SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=localhost:9092 \
  confluentinc/cp-schema-registry
```

### Schema Registry API

```bash
# Register schema
curl -X POST http://localhost:8081/subjects/orders-value/versions \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{"schema": "{...}"}'

# Get schema by ID
curl http://localhost:8081/schemas/ids/1

# List all subjects
curl http://localhost:8081/subjects
```

---

## Comparison

### Message Size (Same Data)

```
JSON:      ~200 bytes
Avro:      ~50 bytes  (75% smaller)
Protobuf:  ~40 bytes  (80% smaller)
```

### Performance

```
Serialization Speed:
Protobuf > Avro > JSON

Deserialization Speed:
Protobuf > Avro > JSON
```

### When to Use

**JSON:**
- Quick prototyping
- Human readability needed
- Simple applications
- Debugging/logging

**Avro:**
- Large-scale production
- Schema evolution required
- Kafka ecosystem (Confluent)
- Data lakes/analytics

**Protobuf:**
- Microservices communication
- High performance needed
- Multi-language support
- Google stack (gRPC)

---

## Custom Serializers (KafkaJS)

### String Serializer

```javascript
const producer = kafka.producer();

await producer.send({
  topic: 'logs',
  messages: [
    { value: 'Simple string message' }  // Auto-converted to bytes
  ]
});
```

### Buffer Serializer

```javascript
const message = Buffer.from('Hello Kafka', 'utf-8');

await producer.send({
  topic: 'messages',
  messages: [
    { value: message }
  ]
});
```

---

## Schema Evolution

### Backward Compatibility

New schema can read old data.

```
Old: { name, age }
New: { name, age, email: "default@example.com" }

✅ New code reads old messages (uses default for email)
```

### Forward Compatibility

Old schema can read new data.

```
Old: { name, age }
New: { name, age, email }

✅ Old code reads new messages (ignores email field)
```

### Full Compatibility

Both backward and forward compatible.

---

## Best Practices

### JSON

✅ Use consistent field naming (camelCase or snake_case)
✅ Include version field for manual versioning
✅ Validate data before producing
✅ Handle parsing errors gracefully

### Avro

✅ Use Schema Registry for centralized management
✅ Define default values for new fields
✅ Test schema compatibility before deployment
✅ Use logical types (timestamp-millis, decimal)

### Protobuf

✅ Never change field numbers
✅ Use `reserved` for removed fields
✅ Add new fields with default values
✅ Use `optional` for nullable fields

---

## Summary

**Serialization Formats:**
- **JSON**: Human-readable, large, slow (good for development)
- **Avro**: Binary, compact, schema-based (good for Kafka ecosystem)
- **Protobuf**: Binary, fastest, schema-based (good for microservices)

**Schema Registry:**
- Centralized schema storage
- Version management
- Compatibility enforcement

**Choose based on:**
- Performance requirements
- Schema evolution needs
- Team expertise
- Ecosystem compatibility

---

## Interview Questions & Answers

### Q1: What is serialization in Kafka?
**A:** Serialization is the process of converting data objects into bytes for transmission. Kafka stores and transmits only bytes, so producers serialize data before sending and consumers deserialize after receiving.

### Q2: What are the common serialization formats used with Kafka?
**A:**
- **JSON**: Human-readable, text-based, no schema required
- **Avro**: Binary, compact, schema-based, schema evolution support
- **Protobuf**: Binary, fast, schema-based (.proto files)
- **String/Bytes**: Simple formats for basic use cases

### Q3: What are the advantages of Avro over JSON?
**A:**
- **Smaller size**: 70-80% smaller than JSON
- **Faster**: Binary format is quicker to parse
- **Schema validation**: Ensures data structure consistency
- **Schema evolution**: Backward/forward compatibility built-in
- **No field names**: Schema stored separately, reducing message size

### Q4: What is Schema Registry and why use it?
**A:** Schema Registry is a centralized service for storing and managing schemas. Benefits:
- Centralized schema management
- Version control for schemas
- Compatibility checking (prevent breaking changes)
- Reduces message size (schema stored once, not in every message)
- Enables safe schema evolution

### Q5: How does Schema Registry work with Kafka?
**A:**
1. Producer registers schema with Schema Registry
2. Registry assigns unique schema ID
3. Producer sends: `[schema_id][serialized_data]`
4. Consumer reads schema_id from message
5. Consumer fetches schema from Registry (cached)
6. Consumer deserializes data using schema

### Q6: What is schema evolution?
**A:** Schema evolution is the ability to modify schemas over time while maintaining compatibility with old/new versions. Types:
- **Backward**: New schema reads old data
- **Forward**: Old schema reads new data
- **Full**: Both backward and forward compatible

### Q7: How do you ensure backward compatibility in Avro?
**A:** Add new fields with default values:
```json
{
  "name": "email",
  "type": "string",
  "default": "unknown@example.com"
}
```
Old consumers without this field will use the default value.

### Q8: What is the difference between Avro and Protobuf?
**A:**
- **Avro**: JSON-based schema, dynamic typing, better Kafka ecosystem integration
- **Protobuf**: .proto files, faster serialization, better for microservices/gRPC
- Both are binary, compact, and support schema evolution

### Q9: When should you use JSON serialization?
**A:** Use JSON when:
- Rapid prototyping or development
- Human readability is important (debugging, logging)
- Small message volume (performance not critical)
- No schema evolution needed
- Simple data structures

### Q10: What happens if producer and consumer use different schemas?
**A:** With Schema Registry:
- Registry checks compatibility
- If compatible, evolution allows different versions to coexist
- If incompatible, registry rejects new schema

Without Schema Registry:
- Deserialization may fail
- Data corruption or errors
- Manual coordination required

### Q11: How do you handle schema changes in production?
**A:**
1. Test compatibility using Schema Registry API
2. Deploy consumers first (can read old and new data)
3. Deploy producers with new schema
4. Monitor for errors
5. Use backward-compatible changes (add fields with defaults)

### Q12: What is the typical message size comparison?
**A:** For the same data:
- JSON: ~200 bytes
- Avro: ~50 bytes (75% smaller)
- Protobuf: ~40 bytes (80% smaller)

Binary formats save network bandwidth and storage.

### Q13: Can you use multiple serialization formats in the same Kafka cluster?
**A:** Yes, different topics can use different formats. You can even use different formats for keys vs values in the same topic. However, all producers/consumers for a specific topic should agree on the format.

### Q14: What are Avro logical types?
**A:** Logical types extend Avro primitive types with semantic meaning:
- `timestamp-millis`: long representing timestamp
- `decimal`: bytes representing decimal number
- `date`: int representing days since epoch
- `uuid`: string in UUID format

### Q15: How do you debug Avro or Protobuf messages?
**A:**
- Use Schema Registry UI or API to view schemas
- Use Kafka tools with deserializers (kafka-avro-console-consumer)
- Convert to JSON for inspection
- Use schema-aware tools (Confluent Control Center, Conduktor)
- Log deserialized objects in consumer code

**Next:** Delivery Semantics (At-least-once, Exactly-once)!