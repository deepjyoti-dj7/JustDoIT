# 01. Kafka with Node.js (KafkaJS)

## What is KafkaJS?

**KafkaJS** is a modern Apache Kafka client for Node.js applications.

**Features:**
✅ Native JavaScript (no native dependencies)
✅ Promise-based API
✅ Producer & Consumer support
✅ Admin client for cluster management
✅ SASL/SSL authentication
✅ TypeScript support

**GitHub:** https://github.com/tulios/kafkajs

---

## Installation

```bash
npm install kafkajs
```

**With TypeScript:**
```bash
npm install kafkajs
npm install --save-dev @types/kafkajs
```

---

## Basic Setup

### Create Kafka Instance

```javascript
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092']
});
```

### With Multiple Brokers

```javascript
const kafka = new Kafka({
  clientId: 'my-app',
  brokers: [
    'broker1:9092',
    'broker2:9092',
    'broker3:9092'
  ],
  connectionTimeout: 3000,
  requestTimeout: 30000
});
```

---

## Producer

### Simple Producer

```javascript
const producer = kafka.producer();

// Connect
await producer.connect();

// Send message
await producer.send({
  topic: 'orders',
  messages: [
    { value: 'Hello Kafka!' }
  ]
});

// Disconnect
await producer.disconnect();
```

### Producer with Key

```javascript
await producer.send({
  topic: 'orders',
  messages: [
    {
      key: 'user123',
      value: JSON.stringify({
        orderId: '12345',
        amount: 99.99
      })
    }
  ]
});
```

### Batch Messages

```javascript
await producer.send({
  topic: 'orders',
  messages: [
    { key: 'user1', value: 'Order 1' },
    { key: 'user2', value: 'Order 2' },
    { key: 'user3', value: 'Order 3' }
  ]
});
```

### Producer with Partitions

```javascript
await producer.send({
  topic: 'orders',
  messages: [
    {
      partition: 0,  // Specific partition
      key: 'user123',
      value: 'Order data'
    }
  ]
});
```

### Producer with Headers

```javascript
await producer.send({
  topic: 'orders',
  messages: [
    {
      key: 'user123',
      value: 'Order data',
      headers: {
        'correlation-id': '12345',
        'content-type': 'application/json'
      }
    }
  ]
});
```

---

## Producer Configuration

### Basic Configuration

```javascript
const producer = kafka.producer({
  // Acknowledgment level
  acks: -1,  // -1 or 'all' for all ISRs
  
  // Compression
  compression: CompressionTypes.GZIP,
  
  // Retries
  retry: {
    retries: 5,
    initialRetryTime: 100,
    maxRetryTime: 30000
  },
  
  // Idempotence
  idempotent: true,
  
  // Max in-flight requests
  maxInFlightRequests: 5,
  
  // Timeout
  timeout: 30000
});
```

### Compression Types

```javascript
const { CompressionTypes } = require('kafkajs');

const producer = kafka.producer({
  compression: CompressionTypes.GZIP
  // CompressionTypes.None
  // CompressionTypes.GZIP
  // CompressionTypes.Snappy
  // CompressionTypes.LZ4
  // CompressionTypes.ZSTD
});
```

---

## Consumer

### Simple Consumer

```javascript
const consumer = kafka.consumer({ groupId: 'my-group' });

// Connect
await consumer.connect();

// Subscribe to topic
await consumer.subscribe({ topic: 'orders' });

// Process messages
await consumer.run({
  eachMessage: async ({ topic, partition, message }) => {
    console.log({
      topic,
      partition,
      offset: message.offset,
      key: message.key?.toString(),
      value: message.value.toString()
    });
  }
});
```

### Consumer from Beginning

```javascript
await consumer.subscribe({ 
  topic: 'orders',
  fromBeginning: true  // Read from offset 0
});
```

### Multiple Topics

```javascript
await consumer.subscribe({ 
  topics: ['orders', 'users', 'products']
});
```

### Topic Pattern

```javascript
await consumer.subscribe({ 
  topics: /^logs\..*/  // Subscribe to logs.app, logs.error, etc.
});
```

### Batch Processing

```javascript
await consumer.run({
  eachBatch: async ({ batch, resolveOffset, heartbeat }) => {
    for (let message of batch.messages) {
      console.log({
        offset: message.offset,
        value: message.value.toString()
      });
      
      // Manually commit offset
      resolveOffset(message.offset);
      
      // Send heartbeat
      await heartbeat();
    }
  }
});
```

---

## Consumer Configuration

```javascript
const consumer = kafka.consumer({
  groupId: 'my-group',
  
  // Session timeout
  sessionTimeout: 30000,
  heartbeatInterval: 3000,
  
  // Offset reset
  fromBeginning: false,
  
  // Max bytes per partition
  maxBytesPerPartition: 1048576,
  
  // Min bytes to fetch
  minBytes: 1,
  maxBytes: 10485760,
  
  // Max wait time
  maxWaitTimeInMs: 5000,
  
  // Retry
  retry: {
    retries: 10
  }
});
```

---

## Offset Management

### Auto Commit (Default)

```javascript
const consumer = kafka.consumer({
  groupId: 'my-group'
  // Auto-commit enabled by default
});

await consumer.run({
  eachMessage: async ({ message }) => {
    await processMessage(message);
    // Offset committed automatically
  }
});
```

### Manual Commit

```javascript
const consumer = kafka.consumer({
  groupId: 'my-group'
});

await consumer.run({
  autoCommit: false,  // Disable auto-commit
  eachMessage: async ({ topic, partition, message }) => {
    await processMessage(message);
    
    // Manual commit
    await consumer.commitOffsets([
      {
        topic,
        partition,
        offset: (parseInt(message.offset) + 1).toString()
      }
    ]);
  }
});
```

---

## Admin Operations

### Create Admin Client

```javascript
const admin = kafka.admin();
await admin.connect();
```

### Create Topic

```javascript
await admin.createTopics({
  topics: [
    {
      topic: 'orders',
      numPartitions: 3,
      replicationFactor: 2
    }
  ]
});
```

### List Topics

```javascript
const topics = await admin.listTopics();
console.log(topics);
```

### Delete Topic

```javascript
await admin.deleteTopics({
  topics: ['orders']
});
```

### Describe Topic

```javascript
const metadata = await admin.fetchTopicMetadata({
  topics: ['orders']
});

console.log(metadata.topics);
```

### Create Partitions

```javascript
await admin.createPartitions({
  topicPartitions: [
    {
      topic: 'orders',
      count: 5  // Increase to 5 partitions
    }
  ]
});
```

### Describe Consumer Groups

```javascript
const groups = await admin.describeGroups(['my-group']);
console.log(groups);
```

### Delete Consumer Group

```javascript
await admin.deleteGroups(['my-group']);
```

---

## Error Handling

### Producer Errors

```javascript
try {
  await producer.send({
    topic: 'orders',
    messages: [{ value: 'data' }]
  });
} catch (error) {
  if (error.type === 'REQUEST_TIMED_OUT') {
    console.error('Request timeout');
  } else if (error.type === 'UNKNOWN_TOPIC_OR_PARTITION') {
    console.error('Topic does not exist');
  } else {
    console.error('Producer error:', error);
  }
}
```

### Consumer Errors

```javascript
const { consumer } = require('kafkajs');

consumer.on(consumer.events.CRASH, ({ payload }) => {
  console.error('Consumer crashed:', payload.error);
});

await consumer.run({
  eachMessage: async ({ message }) => {
    try {
      await processMessage(message);
    } catch (error) {
      console.error('Processing error:', error);
      // Handle or skip message
    }
  }
});
```

---

## Complete Example: Order Processing

### producer.js

```javascript
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'order-service',
  brokers: ['localhost:9092']
});

const producer = kafka.producer();

const createOrder = async (order) => {
  await producer.connect();
  
  try {
    await producer.send({
      topic: 'orders',
      messages: [
        {
          key: order.userId,
          value: JSON.stringify(order),
          headers: {
            'timestamp': Date.now().toString()
          }
        }
      ]
    });
    
    console.log('Order created:', order.orderId);
  } catch (error) {
    console.error('Failed to create order:', error);
  } finally {
    await producer.disconnect();
  }
};

// Create sample order
const order = {
  orderId: '12345',
  userId: 'user123',
  items: [
    { productId: 'A1', quantity: 2, price: 29.99 },
    { productId: 'B2', quantity: 1, price: 49.99 }
  ],
  total: 109.97,
  timestamp: new Date().toISOString()
};

createOrder(order);
```

### consumer.js

```javascript
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'order-processor',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({ groupId: 'order-processing-group' });

const processOrder = async (order) => {
  console.log('Processing order:', order.orderId);
  
  // Business logic
  // 1. Validate order
  // 2. Check inventory
  // 3. Process payment
  // 4. Update database
  
  console.log('Order processed successfully');
};

const run = async () => {
  await consumer.connect();
  await consumer.subscribe({ topic: 'orders', fromBeginning: true });
  
  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      try {
        const order = JSON.parse(message.value.toString());
        await processOrder(order);
      } catch (error) {
        console.error('Failed to process order:', error);
      }
    }
  });
};

run().catch(console.error);
```

---

## TypeScript Example

```typescript
import { Kafka, Producer, Consumer, Message } from 'kafkajs';

interface Order {
  orderId: string;
  userId: string;
  total: number;
}

const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092']
});

const producer: Producer = kafka.producer();
const consumer: Consumer = kafka.consumer({ groupId: 'my-group' });

const sendOrder = async (order: Order): Promise<void> => {
  await producer.connect();
  
  await producer.send({
    topic: 'orders',
    messages: [
      {
        key: order.userId,
        value: JSON.stringify(order)
      }
    ]
  });
  
  await producer.disconnect();
};

const consumeOrders = async (): Promise<void> => {
  await consumer.connect();
  await consumer.subscribe({ topic: 'orders' });
  
  await consumer.run({
    eachMessage: async ({ message }) => {
      const order: Order = JSON.parse(message.value!.toString());
      console.log('Received order:', order);
    }
  });
};
```

---

## Best Practices

### Connection Management

✅ Reuse producer/consumer instances
✅ Connect once, send multiple messages
✅ Handle disconnection gracefully

```javascript
// Good: Reuse producer
const producer = kafka.producer();
await producer.connect();

for (let i = 0; i < 100; i++) {
  await producer.send({ topic: 'test', messages: [{ value: `msg${i}` }] });
}

await producer.disconnect();
```

### Error Handling

✅ Wrap send/run in try-catch
✅ Listen to error events
✅ Implement retry logic
✅ Use dead letter queues

### Performance

✅ Enable compression
✅ Batch messages
✅ Use async/await properly
✅ Tune consumer fetch settings

```javascript
const producer = kafka.producer({
  compression: CompressionTypes.GZIP,
  idempotent: true
});
```

### Graceful Shutdown

```javascript
const signals = ['SIGINT', 'SIGTERM'];

signals.forEach(signal => {
  process.on(signal, async () => {
    console.log('Shutting down...');
    await consumer.disconnect();
    await producer.disconnect();
    process.exit(0);
  });
});
```

---

## Summary

**KafkaJS:**
- Modern Kafka client for Node.js
- Promise-based, TypeScript support
- Producer, Consumer, Admin APIs

**Producer:**
```javascript
await producer.send({ topic, messages });
```

**Consumer:**
```javascript
await consumer.run({ eachMessage: async ({ message }) => {} });
```

**Key Features:**
- Idempotent producer
- Manual/auto offset commit
- Batch processing
- Compression support

---

## Interview Questions & Answers

### Q1: What is KafkaJS?
**A:** KafkaJS is a modern Apache Kafka client library for Node.js. It's written in pure JavaScript (no native dependencies), provides a Promise-based API, and supports producers, consumers, and admin operations with TypeScript support.

### Q2: How do you create a Kafka producer in KafkaJS?
**A:**
```javascript
const { Kafka } = require('kafkajs');
const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092']
});
const producer = kafka.producer();
await producer.connect();
```

### Q3: How do you send a message with a key in KafkaJS?
**A:**
```javascript
await producer.send({
  topic: 'orders',
  messages: [
    {
      key: 'user123',
      value: JSON.stringify({ orderId: '12345' })
    }
  ]
});
```
Messages with the same key go to the same partition, maintaining order.

### Q4: What are the different ways to consume messages in KafkaJS?
**A:**
1. **eachMessage**: Process one message at a time
2. **eachBatch**: Process messages in batches for better performance

```javascript
// eachMessage
await consumer.run({
  eachMessage: async ({ message }) => {}
});

// eachBatch
await consumer.run({
  eachBatch: async ({ batch }) => {}
});
```

### Q5: How do you manually commit offsets in KafkaJS?
**A:**
```javascript
await consumer.run({
  autoCommit: false,
  eachMessage: async ({ topic, partition, message }) => {
    await processMessage(message);
    
    await consumer.commitOffsets([{
      topic,
      partition,
      offset: (parseInt(message.offset) + 1).toString()
    }]);
  }
});
```

### Q6: How do you enable compression in KafkaJS producer?
**A:**
```javascript
const { CompressionTypes } = require('kafkajs');
const producer = kafka.producer({
  compression: CompressionTypes.GZIP
  // Options: None, GZIP, Snappy, LZ4, ZSTD
});
```

### Q7: How do you subscribe to multiple topics in KafkaJS?
**A:**
```javascript
// Array of topics
await consumer.subscribe({ topics: ['orders', 'users'] });

// Pattern matching
await consumer.subscribe({ topics: /^logs\..*/ });
```

### Q8: How do you handle errors in KafkaJS consumer?
**A:**
1. Listen to crash events
2. Wrap processing in try-catch
3. Implement retry logic

```javascript
consumer.on(consumer.events.CRASH, ({ payload }) => {
  console.error('Consumer crashed:', payload.error);
});

await consumer.run({
  eachMessage: async ({ message }) => {
    try {
      await processMessage(message);
    } catch (error) {
      console.error('Processing error:', error);
    }
  }
});
```

### Q9: How do you create a topic using KafkaJS admin client?
**A:**
```javascript
const admin = kafka.admin();
await admin.connect();

await admin.createTopics({
  topics: [{
    topic: 'orders',
    numPartitions: 3,
    replicationFactor: 2
  }]
});

await admin.disconnect();
```

### Q10: What is the difference between `fromBeginning: true` and `false`?
**A:**
- `fromBeginning: true`: Consumer reads from offset 0 (earliest message)
- `fromBeginning: false` (default): Consumer reads from latest offset (only new messages)

Only applies when there's no committed offset for the consumer group.

### Q11: How do you implement graceful shutdown in KafkaJS?
**A:**
```javascript
const signals = ['SIGINT', 'SIGTERM'];

signals.forEach(signal => {
  process.on(signal, async () => {
    await consumer.disconnect();
    await producer.disconnect();
    process.exit(0);
  });
});
```

### Q12: How do you enable idempotent producer in KafkaJS?
**A:**
```javascript
const producer = kafka.producer({
  idempotent: true,
  acks: -1  // Required for idempotence
});
```
Prevents duplicate messages even with retries.

### Q13: How do you send messages to a specific partition in KafkaJS?
**A:**
```javascript
await producer.send({
  topic: 'orders',
  messages: [{
    partition: 0,  // Specific partition number
    value: 'message data'
  }]
});
```

### Q14: What are message headers in KafkaJS and how do you use them?
**A:** Headers are key-value metadata attached to messages:
```javascript
await producer.send({
  topic: 'orders',
  messages: [{
    value: 'data',
    headers: {
      'correlation-id': '12345',
      'content-type': 'application/json'
    }
  }]
});

// Read headers
await consumer.run({
  eachMessage: async ({ message }) => {
    const correlationId = message.headers['correlation-id'].toString();
  }
});
```

### Q15: How do you batch process messages in KafkaJS?
**A:**
```javascript
await consumer.run({
  eachBatch: async ({ batch, resolveOffset, heartbeat }) => {
    for (let message of batch.messages) {
      await processMessage(message);
      resolveOffset(message.offset);
      await heartbeat();
    }
  }
});
```
Batch processing improves throughput for high-volume scenarios.

**Next:** Kafka with Spring Boot!