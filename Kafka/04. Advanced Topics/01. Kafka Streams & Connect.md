# 01. Kafka Streams & Connect

## Kafka Streams

### What is Kafka Streams?

**Kafka Streams** is a client library for building stream processing applications that consume and produce data from/to Kafka.

**Key Features:**
✅ Real-time stream processing
✅ No external dependencies (no separate cluster needed)
✅ Exactly-once semantics
✅ Stateful and stateless operations
✅ Windowing and aggregations
✅ Fault-tolerant and scalable

**Use Cases:**
- Real-time analytics
- Data transformation pipelines
- Aggregations and windowing
- Join multiple streams
- Filtering and routing

---

## Kafka Streams Architecture

```
Input Topic(s) → Kafka Streams App → Output Topic(s)
                     │
                State Store (RocksDB)
```

**Components:**
- **Stream**: Unbounded sequence of records
- **Table**: Changelog stream (latest state)
- **State Store**: Local storage for stateful operations
- **Topology**: Processing graph (sources, processors, sinks)

---

## Basic Kafka Streams Example

### Word Count (Node.js - KafkaStreams)

```javascript
const { KafkaStreams } = require('kafka-streams');

const config = {
  noptions: {
    "metadata.broker.list": "localhost:9092",
    "group.id": "word-count-group"
  }
};

const kafkaStreams = new KafkaStreams(config);

const stream = kafkaStreams.getKStream("input-topic");

stream
  .mapStringToKV(" ")  // Split by space
  .countByKey("word-count-store")
  .forEach(kv => {
    console.log(`Word: ${kv.key}, Count: ${kv.value}`);
  });

stream.start();
```

### Word Count (Java)

```java
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.*;

import java.util.Arrays;
import java.util.Properties;

public class WordCountApp {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("application.id", "word-count-app");
        props.put("bootstrap.servers", "localhost:9092");
        
        StreamsBuilder builder = new StreamsBuilder();
        
        KStream<String, String> source = builder.stream("input-topic");
        
        source
            .flatMapValues(value -> Arrays.asList(value.toLowerCase().split(" ")))
            .groupBy((key, word) -> word)
            .count()
            .toStream()
            .to("output-topic");
        
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
    }
}
```

---

## Kafka Streams Operations

### Stateless Operations

**map**: Transform each record
```java
stream.map((key, value) -> KeyValue.pair(key.toUpperCase(), value.toUpperCase()));
```

**filter**: Filter records
```java
stream.filter((key, value) -> value.length() > 5);
```

**flatMap**: One-to-many transformation
```java
stream.flatMap((key, value) -> 
    Arrays.stream(value.split(" "))
        .map(word -> KeyValue.pair(key, word))
        .collect(Collectors.toList())
);
```

**branch**: Split stream into multiple branches
```java
KStream<String, Integer>[] branches = stream.branch(
    (key, value) -> value > 100,  // Branch 0: high values
    (key, value) -> value > 50,   // Branch 1: medium values
    (key, value) -> true          // Branch 2: low values
);
```

### Stateful Operations

**groupByKey**: Group records by key
```java
KGroupedStream<String, String> grouped = stream.groupByKey();
```

**count**: Count records per key
```java
KTable<String, Long> counts = grouped.count();
```

**aggregate**: Custom aggregation
```java
KTable<String, Integer> aggregated = grouped.aggregate(
    () -> 0,  // Initializer
    (key, value, aggregate) -> aggregate + value.length()  // Aggregator
);
```

**reduce**: Combine values
```java
KTable<String, String> reduced = grouped.reduce(
    (value1, value2) -> value1 + "," + value2
);
```

---

## Windowing

### Tumbling Window

Fixed-size, non-overlapping windows.

```java
KTable<Windowed<String>, Long> windowed = grouped
    .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
    .count();
```

```
Window 1: [00:00 - 05:00]
Window 2: [05:00 - 10:00]
Window 3: [10:00 - 15:00]
```

### Hopping Window

Fixed-size, overlapping windows.

```java
KTable<Windowed<String>, Long> hopping = grouped
    .windowedBy(TimeWindows.of(Duration.ofMinutes(5))
                           .advanceBy(Duration.ofMinutes(1)))
    .count();
```

```
Window 1: [00:00 - 05:00]
Window 2: [01:00 - 06:00]
Window 3: [02:00 - 07:00]
```

### Sliding Window

Window based on record timestamps.

```java
KTable<Windowed<String>, Long> sliding = grouped
    .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(
        Duration.ofMinutes(5),
        Duration.ofMinutes(1)
    ))
    .count();
```

---

## Kafka Connect

### What is Kafka Connect?

**Kafka Connect** is a framework for streaming data between Kafka and external systems (databases, files, cloud services).

**Architecture:**
```
Source Systems → Source Connectors → Kafka Topics
Kafka Topics → Sink Connectors → Target Systems
```

**Benefits:**
✅ No code required (configuration-based)
✅ Scalable and fault-tolerant
✅ Automatic offset management
✅ Built-in converters (JSON, Avro, etc.)
✅ Large connector ecosystem

---

## Connector Types

### Source Connector

Imports data FROM external system TO Kafka.

**Examples:**
- Database CDC (Change Data Capture)
- File system (tailing logs)
- REST APIs
- Message queues

### Sink Connector

Exports data FROM Kafka TO external system.

**Examples:**
- Databases (PostgreSQL, MongoDB)
- Data warehouses (S3, HDFS)
- Search engines (Elasticsearch)
- Cache (Redis)

---

## Kafka Connect Setup

### Standalone Mode

```bash
connect-standalone.sh config/connect-standalone.properties connector.properties
```

### Distributed Mode (Production)

```bash
connect-distributed.sh config/connect-distributed.properties
```

**config/connect-distributed.properties:**
```properties
bootstrap.servers=localhost:9092
group.id=connect-cluster
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false
```

---

## Source Connector Example (JDBC)

### Configuration

```json
{
  "name": "jdbc-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "tasks.max": "1",
    "connection.url": "jdbc:postgresql://localhost:5432/mydb",
    "connection.user": "postgres",
    "connection.password": "password",
    "table.whitelist": "users,orders",
    "mode": "incrementing",
    "incrementing.column.name": "id",
    "topic.prefix": "db-"
  }
}
```

### Create Connector via REST API

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @jdbc-source.json
```

---

## Sink Connector Example (Elasticsearch)

### Configuration

```json
{
  "name": "elasticsearch-sink",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "orders",
    "connection.url": "http://localhost:9200",
    "type.name": "_doc",
    "key.ignore": "false",
    "schema.ignore": "true"
  }
}
```

---

## Kafka Connect REST API

### List Connectors

```bash
curl http://localhost:8083/connectors
```

### Get Connector Status

```bash
curl http://localhost:8083/connectors/jdbc-source-connector/status
```

### Delete Connector

```bash
curl -X DELETE http://localhost:8083/connectors/jdbc-source-connector
```

### Pause/Resume Connector

```bash
# Pause
curl -X PUT http://localhost:8083/connectors/jdbc-source-connector/pause

# Resume
curl -X PUT http://localhost:8083/connectors/jdbc-source-connector/resume
```

---

## Popular Connectors

### Source Connectors

| Connector | Purpose |
|-----------|----------|
| **JDBC** | Database tables to Kafka |
| **Debezium** | CDC from MySQL, PostgreSQL, MongoDB |
| **File** | Read files from filesystem |
| **HTTP** | Poll REST APIs |
| **MQTT** | IoT devices |

### Sink Connectors

| Connector | Purpose |
|-----------|----------|
| **JDBC** | Kafka to database tables |
| **Elasticsearch** | Kafka to Elasticsearch |
| **S3** | Kafka to AWS S3 |
| **HDFS** | Kafka to Hadoop |
| **Redis** | Kafka to Redis cache |

---

## Summary

**Kafka Streams:**
- Stream processing library
- Real-time transformations
- Stateful operations
- Windowing support

**Kafka Connect:**
- Integration framework
- No code required
- Source and sink connectors
- REST API management

**Use Cases:**
- Streams: Real-time analytics, ETL
- Connect: Data integration, CDC

---

## Interview Questions & Answers

### Q1: What is Kafka Streams?
**A:** Kafka Streams is a client library for building real-time stream processing applications. It processes data from Kafka topics, performs transformations, and produces results back to Kafka. No separate cluster needed - runs as part of your application.

### Q2: What is the difference between KStream and KTable?
**A:**
- **KStream**: Represents an unbounded stream of records (insert-only)
- **KTable**: Represents a changelog stream (updates/deletes), stores latest value per key

Example: User clicks = KStream, User profile = KTable

### Q3: What are stateful vs stateless operations in Kafka Streams?
**A:**
- **Stateless**: Operations that don't require state (map, filter, flatMap)
- **Stateful**: Operations that maintain state (count, aggregate, join, windowing)

Stateful operations use state stores (local RocksDB) to maintain data.

### Q4: What is windowing in Kafka Streams?
**A:** Windowing groups records into time-based windows for aggregations:
- **Tumbling**: Fixed-size, non-overlapping (e.g., every 5 minutes)
- **Hopping**: Fixed-size, overlapping (e.g., 5-minute window every 1 minute)
- **Sliding**: Dynamic windows based on record timestamps

### Q5: What is Kafka Connect?
**A:** Kafka Connect is a framework for integrating Kafka with external systems (databases, filesystems, cloud services) using pre-built connectors. It handles data import/export without writing code.

### Q6: What is the difference between source and sink connectors?
**A:**
- **Source Connector**: Imports data FROM external system TO Kafka (e.g., database → Kafka)
- **Sink Connector**: Exports data FROM Kafka TO external system (e.g., Kafka → Elasticsearch)

### Q7: How do you deploy Kafka Connect?
**A:** Two modes:
- **Standalone**: Single process, good for development/testing
- **Distributed**: Multiple workers, fault-tolerant, scalable, production-ready

Distributed mode recommended for production.

### Q8: How do you manage Kafka Connect connectors?
**A:** Use REST API:
```bash
# Create
POST /connectors

# List
GET /connectors

# Status
GET /connectors/{name}/status

# Delete
DELETE /connectors/{name}
```

### Q9: What is exactly-once semantics in Kafka Streams?
**A:** Kafka Streams ensures each record is processed exactly once using:
- Transactions for producing results
- Idempotent producers
- Atomic read-process-write

Enable with: `processing.guarantee=exactly_once_v2`

### Q10: What are state stores in Kafka Streams?
**A:** State stores are local storage (typically RocksDB) used by stateful operations to maintain state. They are:
- Fault-tolerant (backed by changelog topics)
- Queryable via Interactive Queries API
- Automatically restored on failure

### Q11: What is CDC (Change Data Capture) in Kafka?
**A:** CDC captures database changes (inserts, updates, deletes) and streams them to Kafka in real-time. Popular CDC connector: Debezium (supports MySQL, PostgreSQL, MongoDB, etc.).

### Q12: How does Kafka Streams handle failures?
**A:**
- State stores backed by changelog topics
- Automatic task reassignment on failure
- Reprocessing from last committed offset
- Standby replicas for fast recovery

### Q13: Can you join streams in Kafka Streams?
**A:** Yes, three types of joins:
- **KStream-KStream**: Windowed join of two streams
- **KTable-KTable**: Join two tables (latest values)
- **KStream-KTable**: Enrich stream with table data

### Q14: What is the difference between Kafka Streams and Kafka Connect?
**A:**
- **Kafka Streams**: Stream processing (transformations, aggregations, joins)
- **Kafka Connect**: Data integration (import/export between Kafka and external systems)

Streams = processing, Connect = integration.

### Q15: How do you configure a JDBC source connector?
**A:**
```json
{
  "connector.class": "JdbcSourceConnector",
  "connection.url": "jdbc:postgresql://localhost/mydb",
  "table.whitelist": "users",
  "mode": "incrementing",
  "incrementing.column.name": "id",
  "topic.prefix": "db-"
}
```
Reads database table incrementally and publishes to Kafka topic.

**Next:** Schema Registry!