# 02. Schema Registry

## What is Schema Registry?

**Schema Registry** is a centralized repository for managing and validating schemas for Kafka messages.

**Maintained by:** Confluent (part of Confluent Platform)

**Supported Formats:**
✅ Avro (most popular)
✅ Protobuf
✅ JSON Schema

**Why Use Schema Registry?**
- Centralized schema management
- Schema versioning
- Compatibility enforcement
- Reduces message size (schema stored separately)
- Prevents breaking changes

---

## Architecture

```
Producer:
  1. Get schema ID from Schema Registry
  2. Serialize data with schema
  3. Send [schema_id][data] to Kafka

Consumer:
  1. Read message from Kafka
  2. Extract schema_id
  3. Fetch schema from Schema Registry (cached)
  4. Deserialize data
```

---

## Setup Schema Registry

### Docker Compose

```yaml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
```

### Start Services

```bash
docker-compose up -d
```

---

## Schema Registry REST API

### Register Schema

```bash
curl -X POST http://localhost:8081/subjects/orders-value/versions \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{
    "schema": "{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"orderId\",\"type\":\"string\"},{\"name\":\"amount\",\"type\":\"double\"}]}"
  }'
```

**Response:**
```json
{"id": 1}
```

### Get Schema by ID

```bash
curl http://localhost:8081/schemas/ids/1
```

### List All Subjects

```bash
curl http://localhost:8081/subjects
```

### Get All Versions of a Subject

```bash
curl http://localhost:8081/subjects/orders-value/versions
```

### Get Latest Schema

```bash
curl http://localhost:8081/subjects/orders-value/versions/latest
```

### Delete Subject

```bash
curl -X DELETE http://localhost:8081/subjects/orders-value
```

---

## Using Schema Registry with Avro

### Node.js Producer (Avro)

```javascript
const { Kafka } = require('kafkajs');
const { SchemaRegistry } = require('@kafkajs/confluent-schema-registry');

const registry = new SchemaRegistry({ 
  host: 'http://localhost:8081' 
});

const schema = {
  type: 'record',
  name: 'Order',
  fields: [
    { name: 'orderId', type: 'string' },
    { name: 'userId', type: 'string' },
    { name: 'amount', type: 'double' }
  ]
};

const kafka = new Kafka({
  brokers: ['localhost:9092']
});

const producer = kafka.producer();
await producer.connect();

const order = {
  orderId: '12345',
  userId: 'user123',
  amount: 99.99
};

// Register schema and encode
const { id } = await registry.register({ 
  type: 'AVRO', 
  schema: JSON.stringify(schema) 
});

const encodedMessage = await registry.encode(id, order);

await producer.send({
  topic: 'orders',
  messages: [{ value: encodedMessage }]
});
```

### Node.js Consumer (Avro)

```javascript
const consumer = kafka.consumer({ groupId: 'order-group' });
await consumer.connect();
await consumer.subscribe({ topic: 'orders' });

await consumer.run({
  eachMessage: async ({ message }) => {
    // Decode using Schema Registry
    const order = await registry.decode(message.value);
    console.log(order);
  }
});
```

---

## Spring Boot with Schema Registry

### Dependencies

```xml
<dependency>
    <groupId>io.confluent</groupId>
    <artifactId>kafka-avro-serializer</artifactId>
    <version>7.0.0</version>
</dependency>
```

### application.properties

```properties
spring.kafka.bootstrap-servers=localhost:9092

# Producer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
spring.kafka.properties.schema.registry.url=http://localhost:8081

# Consumer
spring.kafka.consumer.group-id=order-group
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
spring.kafka.consumer.properties.specific.avro.reader=true
```

### Avro Schema (order.avsc)

```json
{
  "type": "record",
  "name": "Order",
  "namespace": "com.example",
  "fields": [
    {"name": "orderId", "type": "string"},
    {"name": "userId", "type": "string"},
    {"name": "amount", "type": "double"}
  ]
}
```

### Generate Java Classes

Add Avro Maven plugin:

```xml
<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.11.0</version>
    <executions>
        <execution>
            <goals>
                <goal>schema</goal>
            </goals>
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/resources/avro</sourceDirectory>
                <outputDirectory>${project.basedir}/target/generated-sources</outputDirectory>
            </configuration>
        </execution>
    </executions>
</plugin>
```

### Producer

```java
@Service
public class OrderProducer {
    
    @Autowired
    private KafkaTemplate<String, Order> kafkaTemplate;
    
    public void sendOrder(Order order) {
        kafkaTemplate.send("orders", order.getUserId(), order);
    }
}
```

### Consumer

```java
@Service
public class OrderConsumer {
    
    @KafkaListener(topics = "orders", groupId = "order-group")
    public void consume(Order order) {
        System.out.println("Order ID: " + order.getOrderId());
        System.out.println("Amount: " + order.getAmount());
    }
}
```

---

## Schema Evolution

### Compatibility Types

| Type | Description |
|------|-------------|
| **BACKWARD** | New schema can read old data (default) |
| **FORWARD** | Old schema can read new data |
| **FULL** | Both backward and forward compatible |
| **NONE** | No compatibility checks |

### Set Compatibility Level

```bash
# Global
curl -X PUT http://localhost:8081/config \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{"compatibility": "BACKWARD"}'

# Subject-level
curl -X PUT http://localhost:8081/config/orders-value \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{"compatibility": "FULL"}'
```

---

## Schema Evolution Examples

### Backward Compatible (Add Field with Default)

**Version 1:**
```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "name", "type": "string"}
  ]
}
```

**Version 2:**
```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "name", "type": "string"},
    {"name": "email", "type": "string", "default": ""}
  ]
}
```

✅ **New consumer can read old data** (uses default for missing email)

### Forward Compatible (Remove Field)

**Version 1:**
```json
{
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"}
  ]
}
```

**Version 2:**
```json
{
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "name", "type": "string"}
  ]
}
```

✅ **Old consumer can read new data** (ignores missing age field)

---

## Best Practices

### Schema Design

✅ Use meaningful field names
✅ Provide default values for new fields
✅ Never change field types
✅ Use unions for optional fields
✅ Document schemas with `doc` field

```json
{
  "type": "record",
  "name": "Order",
  "doc": "Order information",
  "fields": [
    {
      "name": "orderId",
      "type": "string",
      "doc": "Unique order identifier"
    },
    {
      "name": "email",
      "type": ["null", "string"],
      "default": null,
      "doc": "Customer email (optional)"
    }
  ]
}
```

### Compatibility

✅ Use BACKWARD for most cases
✅ Use FULL for critical data
✅ Test schema changes before production
✅ Version schemas properly

### Performance

✅ Cache schemas in consumers
✅ Reuse schema registry clients
✅ Monitor registry latency

---

## Summary

**Schema Registry:**
- Centralized schema storage
- Version management
- Compatibility enforcement
- Supports Avro, Protobuf, JSON Schema

**Benefits:**
- Prevents breaking changes
- Reduces message size
- Schema evolution
- Type safety

**Compatibility Modes:**
- BACKWARD: Add fields with defaults
- FORWARD: Remove optional fields
- FULL: Both backward and forward

---

## Interview Questions & Answers

### Q1: What is Schema Registry?
**A:** Schema Registry is a centralized service for storing and managing schemas (Avro, Protobuf, JSON Schema) for Kafka messages. It provides schema versioning, compatibility checking, and prevents breaking changes.

### Q2: Why use Schema Registry instead of embedding schemas in messages?
**A:**
- **Smaller messages**: Schema stored once, not in every message
- **Version management**: Track schema evolution
- **Compatibility enforcement**: Prevent breaking changes
- **Centralized**: Single source of truth for schemas

### Q3: How does Schema Registry work with Kafka?
**A:**
1. Producer registers schema, gets schema ID
2. Producer sends `[schema_id][serialized_data]` to Kafka
3. Consumer reads message, extracts schema_id
4. Consumer fetches schema from registry (cached)
5. Consumer deserializes data using schema

### Q4: What are the compatibility modes in Schema Registry?
**A:**
- **BACKWARD** (default): New schema reads old data
- **FORWARD**: Old schema reads new data
- **FULL**: Both backward and forward compatible
- **NONE**: No compatibility checks

### Q5: How do you add a new field in a backward-compatible way?
**A:** Add field with a default value:
```json
{"name": "email", "type": "string", "default": ""}
```
Old consumers without this field will use the default.

### Q6: What is the difference between subject and schema in Schema Registry?
**A:**
- **Subject**: Named collection of schema versions (e.g., `orders-value`)
- **Schema**: Actual schema definition (Avro, Protobuf, JSON)

One subject can have multiple schema versions.

### Q7: How do you register a schema via REST API?
**A:**
```bash
curl -X POST http://localhost:8081/subjects/orders-value/versions \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{"schema": "...avro schema..."}'
```

### Q8: What happens if you try to register an incompatible schema?
**A:** Schema Registry rejects it with an error if compatibility mode is enforced. Example: Adding a required field without default in BACKWARD mode fails.

### Q9: How do you configure Spring Boot to use Schema Registry?
**A:**
```properties
spring.kafka.producer.value-serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
spring.kafka.consumer.value-deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
spring.kafka.properties.schema.registry.url=http://localhost:8081
```

### Q10: What are schema IDs and how are they used?
**A:** Schema IDs are unique integers assigned by Schema Registry to each schema version. Messages contain the schema ID (first 4 bytes), allowing consumers to fetch and cache the correct schema for deserialization.

### Q11: Can you delete a schema from Schema Registry?
**A:** Yes, but it's a two-step process:
1. Soft delete: `DELETE /subjects/{subject}`
2. Hard delete: `DELETE /subjects/{subject}?permanent=true`

Soft delete allows recovery; hard delete is permanent.

### Q12: What is the default port for Schema Registry?
**A:** Port **8081**

### Q13: How do you handle optional fields in Avro?
**A:** Use union types with null:
```json
{"name": "email", "type": ["null", "string"], "default": null}
```
This makes the field optional.

### Q14: What is the difference between Avro and Protobuf in Schema Registry?
**A:**
- **Avro**: JSON-based schema, dynamic typing, better Kafka ecosystem integration
- **Protobuf**: Binary schema (.proto files), faster serialization, better for microservices

Both supported by Schema Registry.

### Q15: How do you monitor Schema Registry?
**A:**
- REST API: `GET /subjects`, `GET /config`
- Metrics: JMX metrics for latency, errors
- Logs: Check registry logs for errors
- Health check: `GET /` returns version

**Next:** Event-Driven Architecture & Microservices!