# 05. Docker & Deployment

## Docker Setup

### Single Broker (Development)

#### docker-compose.yml

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
```

#### Start Services

```bash
docker-compose up -d
```

---

### Multi-Broker Cluster (Production-like)

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka1:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3

  kafka2:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3

  kafka3:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
```

---

### With Schema Registry

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
```

---

## Kubernetes Deployment

### Using Strimzi Operator

#### Install Strimzi

```bash
kubectl create namespace kafka
kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka
```

#### Kafka Cluster (kafka-cluster.yaml)

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.5.0
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      default.replication.factor: 3
      min.insync.replicas: 2
    storage:
      type: persistent-claim
      size: 100Gi
      class: standard
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 10Gi
      class: standard
  entityOperator:
    topicOperator: {}
    userOperator: {}
```

#### Apply Configuration

```bash
kubectl apply -f kafka-cluster.yaml -n kafka
```

#### Check Status

```bash
kubectl get kafka -n kafka
kubectl get pods -n kafka
```

---

### Create Kafka Topic (Kubernetes)

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: orders
  namespace: kafka
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 6
  replicas: 3
  config:
    retention.ms: 604800000  # 7 days
    segment.bytes: 1073741824  # 1 GB
```

```bash
kubectl apply -f kafka-topic.yaml -n kafka
```

---

## Helm Deployment

### Install Kafka with Helm

```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

helm install kafka bitnami/kafka \
  --set replicaCount=3 \
  --set defaultReplicationFactor=3 \
  --set offsetsTopicReplicationFactor=3 \
  --set persistence.enabled=true \
  --set persistence.size=100Gi
```

### Custom values.yaml

```yaml
replicaCount: 3

defaultReplicationFactor: 3
offsetsTopicReplicationFactor: 3
transactionStateLogReplicationFactor: 3
transactionStateLogMinIsr: 2

persistence:
  enabled: true
  size: 100Gi
  storageClass: "standard"

resources:
  limits:
    cpu: 2
    memory: 4Gi
  requests:
    cpu: 1
    memory: 2Gi

zookeeper:
  enabled: true
  replicaCount: 3
  persistence:
    enabled: true
    size: 10Gi
```

```bash
helm install kafka bitnami/kafka -f values.yaml
```

---

## AWS MSK (Managed Kafka)

### Create MSK Cluster (AWS CLI)

```bash
aws kafka create-cluster \
  --cluster-name my-kafka-cluster \
  --broker-node-group-info file://broker-info.json \
  --kafka-version 3.5.0 \
  --number-of-broker-nodes 3
```

### broker-info.json

```json
{
  "InstanceType": "kafka.m5.large",
  "ClientSubnets": [
    "subnet-12345678",
    "subnet-87654321",
    "subnet-11223344"
  ],
  "SecurityGroups": ["sg-12345678"],
  "StorageInfo": {
    "EbsStorageInfo": {
      "VolumeSize": 100
    }
  }
}
```

### Connect to MSK

```java
props.put("bootstrap.servers", "b-1.mycluster.xxx.kafka.us-east-1.amazonaws.com:9092");
```

---

## Confluent Cloud (Fully Managed)

### Create Cluster (CLI)

```bash
confluent kafka cluster create my-cluster \
  --cloud aws \
  --region us-east-1 \
  --type basic
```

### Connection Config

```properties
bootstrap.servers=pkc-xxx.us-east-1.aws.confluent.cloud:9092
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='API_KEY' password='API_SECRET';
```

---

## Production Deployment Checklist

### Infrastructure

✅ **Cluster size**: Minimum 3 brokers
✅ **Replication factor**: 3 for critical data
✅ **Storage**: SSD with adequate IOPS
✅ **Network**: Dedicated network, low latency
✅ **Resources**: Adequate CPU, RAM, disk

### Configuration

✅ **Unclean leader election**: Disabled
✅ **min.insync.replicas**: 2
✅ **Replication factor**: 3
✅ **Log retention**: Based on requirements
✅ **JVM heap**: 4-8 GB for brokers

### Security

✅ **Authentication**: SASL enabled
✅ **Encryption**: SSL/TLS for transit
✅ **Authorization**: ACLs configured
✅ **Network**: VPC/security groups
✅ **Secrets**: Vault/AWS Secrets Manager

### Monitoring

✅ **Metrics**: Prometheus/Datadog
✅ **Dashboards**: Grafana/Confluent Control Center
✅ **Alerts**: Consumer lag, under-replicated partitions
✅ **Logs**: Centralized logging (ELK/Splunk)

### Backup & Recovery

✅ **Mirror Maker**: Cross-region replication
✅ **Snapshots**: Periodic backups
✅ **DR plan**: Disaster recovery documented
✅ **Testing**: Regular failover drills

---

## High Availability Setup

### Multi-Zone Deployment

```
AWS Region: us-east-1

Zone A:
  - Broker 1
  - ZooKeeper 1

Zone B:
  - Broker 2
  - ZooKeeper 2

Zone C:
  - Broker 3
  - ZooKeeper 3
```

### Rack Awareness

```properties
broker.rack=us-east-1a
```

Ensures replicas distributed across zones.

---

## Scaling

### Horizontal Scaling (Add Brokers)

```bash
# Add new broker to cluster
docker-compose scale kafka=5

# Reassign partitions
kafka-reassign-partitions.sh --bootstrap-server localhost:9092 \
  --reassignment-json-file reassign.json --execute
```

### Vertical Scaling (Increase Resources)

```yaml
resources:
  limits:
    cpu: 4      # Increased from 2
    memory: 8Gi # Increased from 4Gi
```

---

## Summary

**Docker:**
- Easy local development
- docker-compose for multi-broker
- Include Schema Registry, Kafka UI

**Kubernetes:**
- Strimzi Operator for easy management
- Helm charts for quick deployment
- StatefulSets for persistent storage

**Cloud:**
- AWS MSK: Managed Kafka on AWS
- Confluent Cloud: Fully managed
- Azure Event Hubs, Google Pub/Sub

**Production Checklist:**
- 3+ brokers, RF=3
- Security enabled
- Monitoring configured
- HA setup with rack awareness

---

## Interview Questions & Answers

### Q1: How do you deploy Kafka with Docker?
**A:** Use docker-compose with ZooKeeper and Kafka services:
```yaml
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
  kafka:
    image: confluentinc/cp-kafka
    depends_on:
      - zookeeper
```
Run with `docker-compose up -d`.

### Q2: What is Strimzi?
**A:** Strimzi is a Kubernetes operator for running Apache Kafka on Kubernetes. It provides:
- Custom resource definitions (CRDs) for Kafka clusters
- Automated deployment and management
- Easy scaling and upgrades
- Built-in monitoring

### Q3: How do you create a Kafka cluster on Kubernetes?
**A:** Using Strimzi:
1. Install Strimzi operator
2. Define Kafka custom resource
3. Apply with `kubectl apply -f kafka-cluster.yaml`

Strimzi handles broker pods, ZooKeeper, storage, and networking.

### Q4: What is AWS MSK?
**A:** Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed Kafka service on AWS. AWS handles:
- Provisioning and scaling
- Patching and upgrades
- High availability
- Monitoring

You connect using standard Kafka clients.

### Q5: How many brokers should a production cluster have?
**A:** Minimum **3 brokers** for production:
- Enables RF=3
- Can survive 1 broker failure with min.insync.replicas=2
- Allows rolling upgrades

Large deployments: 10+ brokers.

### Q6: How do you ensure high availability in Kafka?
**A:**
1. **Multi-zone deployment**: Distribute brokers across availability zones
2. **Replication factor**: RF=3
3. **min.insync.replicas**: 2
4. **Rack awareness**: Ensure replicas in different zones
5. **Monitoring**: Alert on under-replicated partitions

### Q7: What is rack awareness in Kafka?
**A:** Rack awareness (`broker.rack`) ensures replicas are distributed across different racks/zones:
```properties
broker.rack=us-east-1a
```
Prevents all replicas from being in the same failure domain.

### Q8: How do you scale Kafka horizontally?
**A:**
1. Add new brokers to cluster
2. Reassign partitions to new brokers using `kafka-reassign-partitions.sh`
3. Monitor reassignment progress
4. Verify balanced distribution

Note: Kafka doesn't automatically redistribute existing partitions.

### Q9: What is the recommended JVM heap size for Kafka brokers?
**A:** 4-8 GB heap is recommended:
```bash
export KAFKA_HEAP_OPTS="-Xmx6g -Xms6g"
```
Kafka uses OS page cache heavily, so leave memory for OS (e.g., 6 GB heap on 32 GB machine).

### Q10: How do you deploy Kafka with Helm?
**A:**
```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install kafka bitnami/kafka --set replicaCount=3
```
Customize with values.yaml for production settings.

### Q11: What is the difference between StatefulSet and Deployment in Kubernetes?
**A:**
- **StatefulSet**: For stateful apps like Kafka (stable network IDs, persistent storage)
- **Deployment**: For stateless apps (no persistent identity)

Kafka requires StatefulSet for broker IDs and data persistence.

### Q12: How do you monitor Kafka in production?
**A:**
- **JMX metrics**: Expose via Prometheus exporter
- **Dashboards**: Grafana, Confluent Control Center
- **Alerting**: On lag, under-replicated partitions, disk usage
- **Logging**: Centralized logs (ELK, Splunk)

### Q13: What is Confluent Cloud?
**A:** Confluent Cloud is a fully managed Kafka service by Confluent. Features:
- No infrastructure management
- Auto-scaling
- Global availability
- Built-in security
- Pay-as-you-go pricing

### Q14: How do you upgrade Kafka with zero downtime?
**A:** Rolling upgrade:
1. Upgrade one broker at a time
2. Wait for replicas to sync (ISR)
3. Move to next broker
4. Ensure RF ≥ 2 so cluster stays available

Total time: ~15-30 minutes for 3-broker cluster.

### Q15: What storage type should you use for Kafka?
**A:** **SSD** (not HDD):
- High IOPS required for log writes
- Low latency for consumer reads
- AWS: gp3 or io2
- Size: 1-2 TB per broker typical

**Next:** Security & Monitoring!